{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Welcome to BEDbase documentation  <p>This website hosts user and developer documentation for BEDbase, it's components, and related tools, notably including:</p> <ul> <li> - Genomic interval machine learning toolkit.</li> <li> - Rust package with Python bindings for genomic interval analysis.</li> <li> - BEDbase companion package for processing and managing interval datasets. </li> </ul> <p>You can access the main BEDbase interface at https://bedbase.org.</p>"},{"location":"changelog/","title":"Changelogs for BEDbase","text":"<p>BEDbase is not versioned as a whole; instead, you will need to look at the changelogs for the individual components.</p>"},{"location":"citations/","title":"Manuscripts describing BEDbase and geniml","text":"<p>Components of BEDbase and geniml have been published independently. Here, we showcase the tutorials organized by the manuscript that published them, so if you're reading a paper you can more easily find the relevant features. Find the citation of interest on the left to find links to tutorials published as part of each manuscript.</p>"},{"location":"citations/#how-to-cite-bedbase-and-geniml","title":"How to cite BEDbase and geniml","text":"<p>Thanks for citing us! If you use BEDbase, geniml, or their components in your research, here are papers you can cite.</p> If you use... Please cite ... <code>region2vec</code> embeddings Gharavi et al. (2021) Bioinformatics <code>bedspace</code> search and embeddings Gharavi et al. (2024) Bioengineering <code>scEmbed</code> single-cell embedding framework LeRoy et al. (2024) NAR Genomics and Bioinformatics <code>geniml</code> region set evaluations Zheng et al. (2024) NAR Genomics and Bioinformatics <code>geniml hmm</code> module Rymuza et al. (2024) Nucleic Acis Research <code>bedbase</code> database Unpublished <code>tokenizers</code> module LeRoy et al. (2025) arXiv <code>atacformer</code> foundation model LeRoy et al. (2025) bioRxiv"},{"location":"citations/#full-citation-information-for-manuscripts","title":"Full citation information for manuscripts","text":"<ul> <li> <p>LeRoy et al. (2025). Atacformer: A transformer-based foundation model for analysis and interpretation of ATAC-seq data.   bioRxiv. 10.1101/2025.11.03.685753</p> </li> <li> <p>LeRoy et al. (2025). Fast, memory-efficient genomic interval tokenizers for modern machine learning.   arXiv. 10.48550/arXiv.2511.01555 </p> </li> <li> <p>Gharavi et al. (2024). Joint representation learning for retrieval and annotation of genomic interval sets.   Bioengineering. 10.3390/bioengineering11030263</p> </li> <li> <p>Zheng et al. (2024). Methods for evaluating unsupervised vector representations of genomic regions.   Nucleic Acids Research Genomics and Bioinformatics. 10.1093/nargab/lqae086</p> </li> <li> <p>Xue et al. (2023). Opportunities and challenges in sharing and reusing genomic interval data.   Frontiers in Genetics. 10.3389/fgene.2023.1155809</p> </li> <li> <p>Rymuza et al. (2024). Methods for constructing and evaluating consensus genomic interval sets.   Nucleic Acids Research. 10.1093/nar/gkae685</p> </li> <li> <p>LeRoy et al. (2024). Fast clustering and cell-type annotation of scATAC data with pre-trained embeddings.   Nucleic Acids Research Genomics and Bioinformatics. 10.1093/nargab/lqae073</p> </li> <li> <p>Gu et al. (2021). Bedshift: perturbation of genomic interval sets.   Genome Biology. 10.1186/s13059-021-02440-w</p> </li> <li> <p>Gharavi et al. (2021). Embeddings of genomic region sets capture rich biological associations in low dimensions.   Bioinformatics. 10.1093/bioinformatics/btab439</p> </li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>Pull requests or issues are welcome.</p> <ul> <li>After adding tests in <code>tests</code> for a new feature or a bug fix, please run the test suite.</li> <li>To do so, the only additional dependencies needed beyond those for the package can be  installed with:</li> </ul> <p><code>pip install -r requirements/requirements-all.txt</code></p> <ul> <li>Once those are installed, the tests can be run with <code>pytest</code>. Alternatively,  <code>python setup.py test</code> can be used.</li> </ul>"},{"location":"atacformer/","title":"Atacformer","text":"<p>Atacformer is a general-purpose transformer-based foundation model for genomic interval data, specifically designed for ATAC-seq data. It is part of the Geniml toolkit, which provides a suite of tools for genomic interval machine learning. Trained on a large corpus of ATAC-seq data, Atacformer can be fine-tuned for various downstream tasks such as classification, regression, and sequence generation.</p> <p>All pre-trained models are available on the Hugging Face Hub, and you can easily load them using the <code>geniml</code> package:</p> <pre><code>from geniml.atacformer import AtacformerForCellClustering\n\nmodel = AtacformerForCellClustering.from_pretrained(\"databio/atacformer-base-hg38\")\n</code></pre>"},{"location":"atacformer/core-concepts/","title":"Atacformer core concepts","text":"<p>Using an Atacformer model requires two steps: 1) tokenization and 2) model inference. Step one (tokenization) converts your data into a format that can be processed by the model. Step two actually performs the inference, using the processed data to make predictions and generate embeddings.</p> <p>All tutorials and documentation related to Atacformer will follow this two-step process:</p> <p>Tokenize first, then infer.</p> <p>Whether you are working with single-cell data, bulk data, ATAC-seq, ChIP-seq, or fragments files -- they all follow the same two-step process: tokenize first, then infer.</p> <p> </p>"},{"location":"atacformer/core-concepts/#infrastructure","title":"Infrastructure","text":"<p>The Atacformer models and training infrastructure are built on top of the <code>transformers</code> library, which provides a robust framework for building and training transformer-based models. The Atacformer models are designed to be compatible with the Hugging Face ecosystem, allowing users to easily leverage pre-trained models, while customizing them for specific tasks.</p>"},{"location":"atacformer/core-concepts/#tokenization","title":"Tokenization","text":"<p>Atacformer is designed to handle genomic interval data. It can process anything that can be represented by a chrom, start, and end. To represent this data, each Atacformer model has a specified vocabulary of genomic intervals that it can process represented as a <code>.bed</code> file. Oftentimes, this is referred to as a consensus peak file. We call it a universe. When new data is to be processed, it is first \"tokenized\" into the model's universe, which is a process of mapping the genomic intervals in the data to the intervals in the model's universe.</p> <p> </p> <p>Once tokenized, each token is mapped to an integer ID, which is subsequently converted to a dense embedding vector. This vector is then used as input to the Atacformer model.</p>"},{"location":"atacformer/core-concepts/#pre-trained-models-and-their-tokenizers","title":"Pre-trained models and their tokenizers","text":"<p>Atacformer models are pre-trained on large datasets of genomic interval data. Each model has a corresponding tokenizer that is used to preprocess the data before it is fed into the model. The tokenizer is responsible for converting genomic intervals into a format that the model can understand. It also has knowledge of special tokens like <code>&lt;pad&gt;</code>, <code>&lt;mask&gt;</code>, and <code>&lt;cls&gt;</code> that are used in the model's architecture.</p> <p>When you want to use a pre-trained Atacformer model, you should take care to use the corresponding tokenizer. For example, if you want to use the <code>atacformer-base-hg38</code> model, you should also use the <code>atacformer-base-hg38</code> tokenizer:</p> <pre><code>from gtars.tokenizers import Tokenizer\nfrom geniml.atacformer import AtacformerForCellClustering\n\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-base-hg38\")\nmodel = AtacformerForCellClustering.from_pretrained(\"databio/atacformer-base-hg38\")\n</code></pre>"},{"location":"atacformer/core-concepts/#tokenizer-api","title":"Tokenizer API","text":"<p>We designed the tokenizer API to be as similar as possible to the Hugging Face tokenizer API, so you can use it in a similar way. If you're familiar with Hugging Face tokenizers, you can use the Atacformer tokenizer in a similar way:</p> <pre><code>from gtars.models import RegionSet\nfrom gtars.tokenizers import Tokenizer\n\nrs = RegionSet.from_bed(\"path/to/your/regions.bed\")\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-base-hg38\")\n\n# get the input IDs for the regions\ntokens = tokenizer(rs)\ninput_ids = tokens[\"input_ids\"]\n\n# decode the input IDs back to regions\ndecoded_regions = tokenizer.decode(input_ids)\n\n# get properties of the tokenizer\nvocab_size = tokenizer.vocab_size\nspecial_tokens = tokenizer.special_tokens_map\n</code></pre>"},{"location":"atacformer/quickstart/","title":"Quickstart - generate single-cell embeddings with Atacformer","text":"<p>This quickstart guide will help you generate single-cell embeddings using Atacformer.</p>"},{"location":"atacformer/quickstart/#installation","title":"Installation","text":"<p>To install Atacformer, you need to have the <code>geniml</code> package installed. You can do this using pip:</p> <pre><code>pip install geniml[ml] # install with machine learning dependencies\n</code></pre> <p>Test the installation by importing Atacformer in Python:</p> <pre><code>python -c \"from geniml import __version__; print(__version__)\"\n</code></pre>"},{"location":"atacformer/quickstart/#loading-a-pre-trained-model","title":"Loading a Pre-trained Model","text":"<p>You can easily load a pre-trained Atacformer model from the Hugging Face Hub. For example, you can load the base model described in our paper:</p> <pre><code>from geniml.atacformer import AtacformerForCellClustering\n\nmodel = AtacformerForCellClustering.from_pretrained(\"databio/atacformer-base-hg38\")\nmodel = model.to(\"cuda\")  # move the model to GPU (...if available)\n</code></pre>"},{"location":"atacformer/quickstart/#tokenize-your-data","title":"Tokenize your data","text":"<p>To generate embeddings, you need to tokenize your genomic interval data. Frequently, this data is stored in AnnData format. You can use the <code>geniml</code> package to tokenize your AnnData object.</p> <pre><code>import scanpy as sc\n\nfrom gtars.tokenizers import Tokenizer\nfrom geniml.tokenization.utils import tokenize_anndata\n\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-base-hg38\")\nadata = sc.read_h5ad(\"path/to/your/anndata.h5ad\")\n\ntokens = tokenize_anndata(adata, tokenizer)\n\ninput_ids = [t[\"input_ids\"] for t in tokens]\n</code></pre> <p>Alternatively, you can tokenize <code>.fragments.tsv.gz</code> files directly:</p> <pre><code>from tqdm import tqdm\nfrom gtars.tokenizers import tokenize_fragment_file\n\ntokenized_data = tokenize_fragment_file(\"path/to/your/fragments.tsv.gz\", tokenizer)\n\n# qc \nMIN_COUNT = 500\nMAX_COUNT = 100_000\n\n# filter down low and high count cells\ntokens_filtered = [(barcode, ids) for barcode, ids in tqdm(tokens, desc=\"Filtering tokens\", total=len(tokens)) if len(ids) &gt;= MIN_COUNT and len(ids) &lt;= MAX_COUNT]\ntokens_filtered = [(barcode, list(set(ids))) for barcode, ids in tqdm(tokens_filtered, desc=\"Removing duplicates\", total=len(tokens_filtered))]\n\ninput_ids = [ids for _, ids in tokens_filtered]\n</code></pre>"},{"location":"atacformer/quickstart/#generate-embeddings","title":"Generate Embeddings","text":"<p>Once you have your tokenized data, you can generate embeddings using the model:</p> <pre><code>cell_embeddings = model.encode_tokenized_cells(\n    input_ids=input_ids,\n    batch_size=32,  # adjust based on your memory capacity\n)\n</code></pre>"},{"location":"atacformer/quickstart/#downstream-tasks","title":"Downstream Tasks","text":"<p>You can use the generated embeddings for various downstream tasks such as clustering, classification, or visualization.</p>"},{"location":"atacformer/local_compute/apple_silicon/","title":"Using atacformer with Apple Silicon","text":"<p>Apple's M-series chips have become increasingly popular for local-first machine learning tasks. Conveniently, PyTorch has first-class support for Apple Silicon, making it a great choice for running Atacformer models on these devices.</p> <p>In most cases, it could be as simple as moving your model to the <code>mps</code> device:</p> <pre><code>from geniml.atacformer import AtacformerForCellClustering\n\nmodel = AtacformerForCellClustering.from_pretrained(\"databio/atacformer-base-hg38\")\nmodel = model.to(\"mps\")\n</code></pre> <p>However, at the moment, not all tensor operations are supported on the <code>mps</code> device. One of these is the <code>aten::_nested_tensor_from_mask_left_aligned</code> operation inside the <code>TransformerEncoder</code> class, which is used in the Atacformer model.</p> <p>To work around this, we've provided a patch that modifies the <code>TransformerEncoder</code> to use a different operation that is supported on the <code>mps</code> device. You can use it like so:</p> <pre><code>from geniml.atacformer import AtacformerForCellClustering, patch_atacformer_model_for_mps\n\nmodel = AtacformerForCellClustering.from_pretrained(\"databio/atacformer-base-hg38\")\nmodel = model.to(\"mps\")\n\npatch_atacformer_model_for_mps(model)\n\n# use the model as usual\n# ...\n</code></pre>"},{"location":"atacformer/local_compute/compute_requirements/","title":"Compute requirements","text":"<p>Atacformer is a transformer model that leverages a large context-window, due to the nature of scATAC-seq data. A typical Atacformer context-window size is 8,192 tokens. To that end, Atacformer requires a significant amount of GPU memory to use in its full capacity.</p> <p>Atacformer processes data in chunks (batches). This <code>batch_size</code> is configurable and can be adjusted based on the available hardware resources and the specific requirements of the task at hand. You can use the following table as a general rule of thumb for how much GPU VRAM you need:</p> Batch Size GPU VRAM Required Example GPUs 4 2GB NVIDIA RTX 2080 Ti 8 4GB NVIDIA RTX 3080 16 8GB NVIDIA RTX 3090 32 16GB NVIDIA RTX 4070 Ti 64 48GB NVIDIA A40/A6000"},{"location":"atacformer/local_compute/local_compute/","title":"Using atacformer locally","text":"<p>Because of the nature of scATAC-seq data, Atacformer is trained using a large context size (8,192 tokens). This allows the model to capture long-range dependencies in the genomic data, which is crucial for understanding regulatory elements and their interactions.</p> <p>However, this large context size also means that Atacformer requires significant computational resources for training and inference. While we recommend using a GPU when running Atacformer, it is possible to artificially limit the context size during inference to reduce memory usage.</p> <p>You can do this in two ways: 1. Manually truncating the input sequences to a smaller size before passing them to the model. 2. Augmenting the model's configuration to set a smaller maximum sequence length.</p>"},{"location":"atacformer/local_compute/local_compute/#manually-truncating-input-sequences","title":"Manually Truncating Input Sequences","text":"<p>If you want to limit the context size during inference, you can manually truncate your input sequences. For example, if you want to limit the context size to 1,024 tokens, you can do this: <pre><code>from geniml.atacformer import AtacformerForCellClustering\nfrom gtars.tokenizers import Tokenizer\n\nmodel = AtacformerForCellClustering.from_pretrained(\"databio/atacformer-craft100k-hg38\")\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-craft100k-hg38\")\n\n# Load your data and tokenize it\ninput_ids = ...  # your tokenized input ids\n\n# Truncate to 1024 tokens\ninput_ids = [ids[:1024] for ids in input_ids]\n</code></pre></p>"},{"location":"atacformer/local_compute/local_compute/#augmenting-the-models-configuration","title":"Augmenting the Model's Configuration","text":"<p>Manual truncation might not be desired as it biases the model towards tokens at the beginning of the sequence. A better approach would randomly sample a subset of the input ids to create a smaller context size. this is done automatically for you when your sequence exceeds the maximum sequence length defined in the model's configuration.</p> <p>To that end, you can augment the model's configuration to set a smaller maximum sequence length. This way, the model will automatically handle the truncation for you.</p> <pre><code>model.config.max_position_embeddings = 1024  # set to your desired context size\n</code></pre>"},{"location":"atacformer/tutorials/ev-projection/","title":"Visualizing new embeddings in the latent space of a reference","text":"<p>Reference mapping is a common technique in single-cell genomics. It involves representing both new data and a high-quality reference dataset in the same latent space, allowing for comparison and analysis of new embeddings against established ones.</p> <p>Atacformer, because it can generate embeddings for any genomic region, is well-suited for this task. You can use it to project new embeddings into the latent space of a reference dataset, enabling you to visualize and analyze how new data relates to existing references.</p>"},{"location":"atacformer/tutorials/ev-projection/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have <code>geniml</code> and <code>gtars</code> installed. <code>gtars</code> is our companion library for genomic interval data processing, and <code>geniml</code> provides the Atacformer model and tokenizer.</p> <p>The steps we will follow include:</p> <ol> <li>Load a pre-trained Atacformer model.</li> <li>Load a reference dataset along side a query dataset.</li> <li>Tokenize both datasets.</li> <li>Generate embeddings for both datasets.</li> <li>Create a UMAP projection of the reference dataset embeddings.</li> <li>Visualize the new embeddings in the latent space of the reference dataset.</li> <li>Interpret the results and draw conclusions about the relationship between the new and reference datasets.</li> </ol>"},{"location":"atacformer/tutorials/ev-projection/#loading-the-pre-trained-model","title":"Loading the Pre-trained Model","text":"<p>To start, we need to load a pre-trained Atacformer model. This model will be used to generate embeddings for both the reference and new datasets.</p> <pre><code>from gtars.tokenizers import Tokenizer\nfrom geniml.atacformer import AtacformerForCellClustering\n\n# craft performs well on blood datasets\nmodel = AtacformerForCellClustering.from_pretrained(\"databio/atacformer-craft100k-hg38\")\n# model = model.to(\"cuda\")  # or \"mps\" for Apple Silicon\n\n\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-craft100k-hg38\")\n</code></pre>"},{"location":"atacformer/tutorials/ev-projection/#loading-the-datasets","title":"Loading the Datasets","text":"<p>TODO: how do we let users quickly load the Luecken2021 and pbmc datasets?</p> <pre><code>import scanpy as sc\n\n# load the reference dataset (e.g., Luecken2021)\nreference_dataset = sc.read(\"path/to/reference_dataset.h5ad\")\n\n# load the query dataset (e.g., pbmc)\nquery_dataset = sc.read(\"path/to/query_dataset.h5ad\")\n</code></pre>"},{"location":"atacformer/tutorials/ev-projection/#tokenizing-the-datasets","title":"Tokenizing the Datasets","text":"<p>Next, we need to tokenize both the reference and new datasets. This step converts the genomic intervals into a format that the Atacformer model can understand, specifically input ids that represent genomic regions.</p> <pre><code>from geniml.atacformer import tokenize_anndata\n\nreference_inputs = tokenize_anndata(reference_dataset, tokenizer)\nquery_inputs = tokenize_anndata(query_dataset, tokenizer)\n\nreference_input_ids = [t[\"input_ids\"] for t in reference_inputs]\nquery_input_ids = [t[\"input_ids\"] for t in query_inputs]\n</code></pre>"},{"location":"atacformer/tutorials/ev-projection/#generating-embeddings","title":"Generating Embeddings","text":"<p>Now that we have tokenized both datasets, we can generate embeddings using the Atacformer model. These embeddings will represent the genomic regions in a high-dimensional space.</p> <pre><code>batch_size = 4  # adjust based on your memory\nreference_embeddings = model.encode_tokenized_cells(\n    reference_input_ids,\n    batch_size=batch_size,\n)\nquery_embeddings = model.encode_tokenized_cells(\n    query_input_ids,\n    batch_size=batch_size,\n)\n\n# attach embeddings to the AnnData objects\nreference_dataset.obsm[\"X_atacformer\"] = reference_embeddings.cpu().numpy()\nquery_dataset.obsm[\"X_atacformer\"] = query_embeddings.cpu().numpy()\n</code></pre>"},{"location":"atacformer/tutorials/ev-projection/#visualize-the-embeddings","title":"Visualize the embeddings","text":"<p>Before we project the new embeddings into the reference latent space, it can be helpful to visualize the embeddings of both datasets separately. This step allows us to see the distribution of the embeddings in their respective latent spaces.</p> <p>We can also perform leiden clustering on the embeddings to identify clusters in the query dataset.</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom umap import UMAP\n\n# perform Leiden clustering on the query dataset (since we dont have labels)\nsc.pp.neighbors(query_dataset, use_rep='X_atacformer', n_neighbors=10)\nsc.tl.leiden(query_dataset)\n\n# generate two UMAPS, one for the reference and one for the query dataset\numap_model = UMAP(n_neighbors=15, random_state=42)\nreference_umap = umap_model.fit_transform(reference_dataset.obsm[\"X_atacformer\"])\n\numap_model = UMAP(n_neighbors=15, random_state=42)\nquery_umap = umap_model.fit_transform(query_dataset.obsm[\"X_atacformer\"])\n\n# plot the UMAPs\nplt.figure(figsize=(12, 6))\nsns.scatterplot(\n    x=reference_umap[:, 0],\n    y=reference_umap[:, 1],\n    hue=reference_dataset.obs['cell_type'],  # assuming you have cell type labels\n    palette='tab20',\n    ax=plt.subplot(1, 2, 1)\n)\nsns.scatterplot(\n    x=query_umap[:, 0],\n    y=query_umap[:, 1],\n    hue=query_dataset.obs['leiden'],  # using leiden clusters\n    palette='tab20',\n    ax=plt.subplot(1, 2, 2)\n)\n\nplt.suptitle(\"UMAP of Reference and Query Datasets\")\nplt.show()\n</code></pre>"},{"location":"atacformer/tutorials/ev-projection/#projecting-new-embeddings-into-the-reference-latent-space","title":"Projecting New Embeddings into the Reference Latent Space","text":"<p>Now, we can project the new embeddings (from the query dataset) into the latent space of the reference dataset. This step allows us to visualize how the new data relates to the established reference.</p> <pre><code>umap_model = UMAP(random_state=42)\nreference_umap = umap_model.fit_transform(reference_dataset.obsm[\"X_atacformer\"])\nquery_umap = umap_model.transform(query_dataset.obsm[\"X_atacformer\"])\n\n# plot the projected query embeddings in the reference latent space\nplt.figure(figsize=(8, 6))\nsns.scatterplot(\n    x=reference_umap[:, 0],\n    y=reference_umap[:, 1],\n    color='gray',\n    label='Reference',\n    alpha=0.5\n)\nsns.scatterplot(\n    x=query_umap[:, 0],\n    y=query_umap[:, 1],\n    hue=query_dataset.obs['leiden'],  # using leiden clusters\n    palette='tab20',\n    label='Query',\n    alpha=0.8\n)\nplt.title(\"Projected Query Embeddings in Reference Latent Space\")\nplt.xlabel(\"UMAP 1\")\nplt.ylabel(\"UMAP 2\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"atacformer/tutorials/example-brain3k-processing/","title":"Example Brain3k Processing","text":"<p>This tutorial demonstrates how to process a small dataset from 10X Genomics' Brain3k multiome dataset using Atacformer. </p> <p>To start, we need to download the matrix files:</p> <pre><code>wget \"https://cf.10xgenomics.com/samples/cell-arc/2.0.0/human_brain_3k/human_brain_3k_filtered_feature_bc_matrix.tar.gz\" -O  \"brain3k.tar.gz\"\n\ntar -xzf brain3k.tar.gz\n</code></pre> <p>There are three files in the extracted directory: <code>barcodes.tsv</code>, <code>features.tsv</code>, and <code>matrix.mtx</code>. These files contain the cell barcodes, the feature names, and the matrix data, respectively. We will use these files to build an <code>AnnData</code> object, which is a common format for storing single-cell data. The <code>AnnData</code> object will contain the genomic regions of interest in the <code>.var</code> dataframe:</p> <p></p> <pre><code>import pandas as pd\nfrom anndata import AnnData\nfrom scipy.io import mmread\n\n# read in the data\nmtx = mmread(\"matrix.mtx.gz\").T.tocsr()\nbarcodes = pd.read_csv(\"barcodes.tsv.gz\", header=None, names=[\"barcode\"], sep=\"\\t\")\nfeatures = pd.read_csv(\"features.tsv.gz\", header=None, names=[\"feature_id\", \"feature_name\", \"type\", \"chr\", \"start\", \"end\"], sep=\"\\t\")\n\n# create the AnnData object, write it to disk\nadata = AnnData(\n    X=mtx,\n    obs=barcodes,\n    var=features.set_index(\"feature_id\"),\n)\nadata.write_h5ad(\"brain3k.h5ad\")\n</code></pre> <p>We need to subset the data to only include the ATAC-seq regions.</p> <pre><code>atac = adata[:, adata.var[\"type\"] == \"Peaks\"]\n</code></pre> <p>Next, we will pre-tokenize the data using the Atacformer tokenizer. This step converts the genomic regions into input IDs that the Atacformer model can understand.</p> <pre><code>from geniml.atacformer import AtacformerForCellClustering\nfrom geniml.tokenization import tokenize_anndata\nfrom gtars.tokenizers import Tokenizer\n\nmodel = AtacformerForCellClustering.from_pretrained(\"databio/atacformer-base-hg38\")\n\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-base-hg38\")\ntokens = tokenize_anndata(atac, tokenizer)\ntokens = [t[\"input_ids\"] for t in tokens]\n</code></pre> <p>Now we can generate single-cell embedding and attach them back to the <code>AnnData</code> object. This will allow us to use the embeddings for downstream tasks such as clustering or visualization.</p> <pre><code>embeddings = model.encode_tokenized_cells(tokens)\natac.obsm[\"X_atacformer\"] = embeddings.detach().cpu().numpy()\n</code></pre> <p>Finally, we can generate clusters and visualize the results using UMAP. </p> <pre><code>import scanpy as sc\n\nsc.pp.neighbors(atac, use_rep=\"X_atacformer\")\nsc.tl.leiden(atac)\n\nsc.pl.umap(atac, color=[\"leiden\", \"chr\", \"start\", \"end\"], frameon=False, wspace=0.4, hspace=0.4)\n</code></pre>"},{"location":"atacformer/tutorials/umap_from_fragments/","title":"Generating a UMAP from fragments files","text":"<p>One of the most common single-cell ATAC-seq data formats is the fragments file format from 10X Genomics. These files contain information about the genomic regions that were accessible in individual cells during the ATAC-seq experiment.</p>"},{"location":"atacformer/tutorials/umap_from_fragments/#getting-the-data","title":"Getting the data","text":"<p>To start, lets grab an example fragments file: <pre><code>wget \"https://cf.10xgenomics.com/samples/cell-arc/2.0.0/human_brain_3k/human_brain_3k_atac_fragments.tsv.gz\" -O human_brain_3k_atac_fragments.tsv.gz\n</code></pre></p>"},{"location":"atacformer/tutorials/umap_from_fragments/#tokenize-first","title":"Tokenize first","text":"<p>Remember that we always tokenize first, then infer after. We will start by tokenizing the fragments file: <pre><code>from gtars.tokenizers import tokenize_fragments_file, Tokenizer\n\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-base-hg38\")\ntokens = tokenize_fragments_file(\"human_brain_3k_atac_fragments.tsv.gz\", tokenizer)\n</code></pre></p> <p><code>tokens</code> is now a list of dictionaries, where each key is a unique cell barcode, and then each value is a list of <code>input_ids</code> for the corresponding cell.</p>"},{"location":"atacformer/tutorials/umap_from_fragments/#basic-qc","title":"Basic QC","text":"<p>Before inferring, lets remove cells with very low and very high fragment counts: <pre><code>min_fragments = 200\nmax_fragments = 10_000\n\nfiltered_tokens = {k: v for k, v in tokens.items() if min_fragments &lt;= len(v) &lt;= max_fragments}\n</code></pre></p>"},{"location":"atacformer/tutorials/umap_from_fragments/#infer-embeddings","title":"Infer embeddings","text":"<p>Now that we have tokenized the fragments file and performed basic QC, we can infer embeddings for the filtered tokens: <pre><code>from gtars.models import Atacformer\n\nmodel = Atacformer.from_pretrained(\"databio/atacformer-base-hg38\")\nembeddings = model.encode_tokenized_cells(\n    input_ids=filtered_tokens.values(),\n    batch_size=32\n)\n\n# detach embeddings from the computation graph and convert to numpy\nembeddings = embeddings.detach().cpu().numpy()\n</code></pre></p>"},{"location":"atacformer/tutorials/umap_from_fragments/#generate-umap","title":"Generate UMAP","text":"<p>Now that we have the embeddings, we can generate a UMAP: <pre><code>from umap import UMAP\nimport matplotlib.pyplot as plt\n\nreducer = UMAP(n_components=2, random_state=42)\numap_embeddings = reducer.fit_transform(embeddings)\n\nplt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1])\nplt.show()\n</code></pre></p>"},{"location":"atacformer/tutorials/training/fine-tune/","title":"Fine-tune Atacformer on your data","text":"<p>Often, you will want to fine-tune a pre-trained Atacformer model on your own dataset. This is a common practice in transfer learning, where you take a model that has been pre-trained on a large dataset and adapt it to your specific dataset.</p>"},{"location":"atacformer/tutorials/training/fine-tune/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>A pre-trained Atacformer model (e.g., <code>databio/atacformer-base-hg38</code>)</li> <li>Pre-tokenized your dataset. If not, see the pre-tokenize for training guide.</li> </ul>"},{"location":"atacformer/tutorials/training/fine-tune/#setup-training","title":"Setup training","text":"<p>We use a mixture of <code>geniml</code> and the <code>transformers</code> library to run training.</p> <pre><code>import torch\n\nfrom datasets import Dataset\nfrom transformers import TrainingArguments, Trainer\nfrom atacformer import (\n    AtacformerForReplacedTokenDetection,\n    DataCollatorForReplacedTokenDetection,\n    TrainingTokenizer,\n)\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.set_float32_matmul_precision(\"medium\")\n# optional for experiment tracking\n# os.environ[\"WANDB_PROJECT\"] = \"atacformer-pretraining\"\n\nMLM_PROBABILITY = 0.45\nBATCH_SIZE = 32\nMAX_LEARNING_RATE = 1.5e-4\nRUN_NAME = \"atacformer-fine-tuning\"\n\nMODEL_TO_FINE_TUNE = \"databio/atacformer-base-hg38\"\n</code></pre>"},{"location":"atacformer/tutorials/training/fine-tune/#load-your-dataset","title":"Load your dataset","text":"<p>Load your pre-tokenized dataset. The training expects a pre-tokenized dataset in Parquet format with the following columns: - <code>input_ids</code>: Tokenized genomic regions</p> <pre><code>dataset_path = \"path/to/dataset.parquet\"\ntokenized_dataset = Dataset.from_parquet(dataset_path)\ntokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n</code></pre>"},{"location":"atacformer/tutorials/training/fine-tune/#create-the-tokenizer-and-data-collator","title":"Create the tokenizer and data collator","text":"<p>Next, set up the tokenizer. The tokenizer is created from a universe file that defines the genomic regions:</p> <pre><code>tokenizer = TrainingTokenizer.from_pretrained(MODEL_TO_FINE_TUNE)\ndata_collator = DataCollatorForReplacedTokenDetection(\n    tokenizer=tokenizer,\n    mlm_probability=MLM_PROBABILITY,\n)\n</code></pre>"},{"location":"atacformer/tutorials/training/fine-tune/#create-the-model","title":"Create the model","text":"<p>Grab the pre-trained model weights from the Hugging Face Hub:</p> <pre><code>model = AtacformerForReplacedTokenDetection.from_pretrained(MODEL_TO_FINE_TUNE) \nmodel = model.to(torch.bfloat16)  # use bfloat16 for training (its faster on ampere GPUs)\n</code></pre>"},{"location":"atacformer/tutorials/training/fine-tune/#training-arguments","title":"Training arguments","text":"<p>Set up the training arguments. You can adjust these based on your hardware and dataset size:</p> <pre><code>training_args = TrainingArguments(\n    output_dir=\"atacformer-fine-tuning-output\",\n    overwrite_output_dir=True,\n    num_train_epochs=10,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    eval_strategy=\"no\",\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    run_name=RUN_NAME,\n    warmup_steps=500,\n    lr_scheduler_type=\"cosine_with_restarts\",\n    learning_rate=MAX_LEARNING_RATE,\n    bf16=True,\n    max_grad_norm=1.0\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=data_collator,\n)\n</code></pre>"},{"location":"atacformer/tutorials/training/fine-tune/#train-your-model","title":"Train your model","text":"<p>Finally, start the training process:</p> <pre><code>trainer.train()\nmodel.save_pretrained(\"output/atacformer-fine-tuned\")\n</code></pre>"},{"location":"atacformer/tutorials/training/fine-tune/#evaluate-your-model","title":"Evaluate your model","text":"<p>Your model can now be used like any other Atacformer model. You can evaluate it on your test set or use it for downstream tasks such as cell clustering, classification, or regression.</p>"},{"location":"atacformer/tutorials/training/pretokenize-for-training/","title":"Pre-tokenize data for training","text":"<p>Before doing any training with Atacformer, you need to pre-tokenize your genomic interval data. This process converts your data into a format that the Atacformer model can understand, specifically input ids that represent genomic regions.</p> <p>For easy reading, writing, and manipulation of pre-tokenized data, we use the Parquet format. This format is efficient and works well with the huggingface ecosystem; namely, the <code>datasets</code> library.</p>"},{"location":"atacformer/tutorials/training/pretokenize-for-training/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have <code>geniml</code> and <code>gtars</code> installed. <code>gtars</code> is our companion library for genomic interval data processing -- it contains the tokenizers. <code>geniml</code> provides the Atacformer model. <pre><code>pip install geniml[ml] datasets gtars\n</code></pre></p> <p>Next, ensure you have a universe file that defines the genomic regions you want to tokenize. This file is typically in BED format and contains the regions of interest.</p> <p>Finally, you need your data in the <code>AnnData</code> format or as <code>.fragments.tsv.gz</code> files. If you have your data in a different format, you may need to convert it first. If using <code>AnnData</code>, please ensure that you have a <code>chr</code>, <code>start</code>, and <code>end</code> column in the <code>.var</code> dataframe.</p>"},{"location":"atacformer/tutorials/training/pretokenize-for-training/#tokenization-process","title":"Tokenization Process","text":"<p>To pre-tokenize your data, you can use the <code>gtars</code> tokenizer. This tokenizer will read your genomic interval data and convert it into the Atacformer input format.</p> <pre><code>import scanpy as sc\nimport polars as pl\n\nfrom geniml.tokenization import tokenize_anndata\nfrom gtars.tokenizers import Tokenizer\n\n# read in data; create a tokenizer\nadata = sc.read(\"path/to/your/anndata.h5ad\")\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-base-hg38\")\n\n# tokenize the data\ntokens = tokenize_anndata(adata, tokenizer)\ninput_ids = [t[\"input_ids\"] for t in tokens]\n\n# optional: cutoff at context size\nCONTEXT_SIZE = 8192\ninput_ids = [ids[:CONTEXT_SIZE] for ids in input_ids]\n\n# convert to a DataFrame\ndf = pl.DataFrame({\n    \"input_ids\": input_ids,\n})\n\ndf.write_parquet(\"path/to/tokenized_data.parquet\")\n</code></pre>"},{"location":"atacformer/tutorials/training/train-from-scratch/","title":"Train Atacformer from scratch","text":"<p>This tutorial will guide you through the process of training an Atacformer model from scratch using genomic interval data. We will cover the necessary steps, including data preparation, model configuration, and training.</p>"},{"location":"atacformer/tutorials/training/train-from-scratch/#overview","title":"Overview","text":"<p>The pre-training process uses a replaced token detection objective (similar to ELECTRA) to learn representations of genomic regions and their accessibility patterns.</p>"},{"location":"atacformer/tutorials/training/train-from-scratch/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>Access to pre-tokenized single-cell ATAC-seq data</li> <li>A universe file (<code>.bed.gz</code>) defining the genomic regions of interest</li> <li>GPU resources for training</li> </ul>"},{"location":"atacformer/tutorials/training/train-from-scratch/#model-configuration","title":"Model Configuration","text":"<p>You can configure the Atacformer model however you like. This is what was used in the original paper:</p> <pre><code>config = AtacformerConfig(\n    vocab_size=tokenizer.vocab_size,    # Based on universe file\n    hidden_size=192,                    # Hidden dimension size\n    num_hidden_layers=6,                # Number of transformer layers\n    num_attention_heads=8,              # Number of attention heads\n    intermediate_size=768,              # Feed-forward network size\n    max_position_embeddings=8192,       # Maximum sequence length\n    pad_token_id=tokenizer.pad_token_id,\n)\n</code></pre>"},{"location":"atacformer/tutorials/training/train-from-scratch/#module-imports","title":"Module imports","text":"<p>First, import the necessary modules:</p> <pre><code>import torch\n\nfrom datasets import Dataset\nfrom transformers import TrainingArguments, Trainer\nfrom atacformer import (\n    AtacformerConfig,\n    AtacformerForReplacedTokenDetection,\n    DataCollatorForReplacedTokenDetection,\n    TrainingTokenizer,\n    get_decaying_cosine_with_hard_restarts_schedule_with_warmup,\n)\n</code></pre>"},{"location":"atacformer/tutorials/training/train-from-scratch/#training-setup","title":"Training setup","text":"<p>Set up <code>torch</code></p> <pre><code>torch.backends.cuda.matmul.allow_tf32 = True\ntorch.set_float32_matmul_precision(\"medium\")\n</code></pre> <p>Make sure your hyperparameters are set up correctly:</p> <pre><code>MLM_PROBABILITY = 0.45\nBATCH_SIZE = 16\nMAX_LEARNING_RATE = 1.5e-4\n\nRUN_NAME = \"atacformer-pretraining\"\n</code></pre>"},{"location":"atacformer/tutorials/training/train-from-scratch/#data-preparation","title":"Data Preparation","text":"<p>Then load your dataset. The training expects a pre-tokenized dataset in Parquet format with the following columns:</p> <ul> <li><code>input_ids</code>: Tokenized genomic regions</li> <li>Additional metadata columns (removed during training)</li> </ul> <pre><code>dataset_path = \"path/to/dataset.parquet\"\ntokenized_dataset = Dataset.from_parquet(dataset_path)\ntokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n</code></pre>"},{"location":"atacformer/tutorials/training/train-from-scratch/#tokenizer-setup","title":"Tokenizer Setup","text":"<p>Next, set up the tokenizer:</p> <pre><code>tokenizer = TrainingTokenizer(\"path/to/universe.bed.gz\")\n</code></pre> <p>The tokenizer is created from a universe file that defines the genomic regions:</p> <pre><code>tokenizer = TrainingTokenizer(universe_bed_file)\n</code></pre>"},{"location":"atacformer/tutorials/training/train-from-scratch/#model-and-training-arguments","title":"Model and Training Arguments","text":"<p>Instantiate a new Atacformer model with the replaced token detection (RTD) objective. We move the model to <code>bfloat16</code> because its much faster when using \"ampere\" GPUs:</p> <pre><code>config = AtacformerConfig(\n    use_pos_embeddings=False,\n    vocab_size=tokenizer.vocab_size,\n    hidden_size=192,\n    num_hidden_layers=6,\n    num_attention_heads=8,\n    intermediate_size=768,\n    max_position_embeddings=8192,\n    pad_token_id=tokenizer.pad_token_id,\n)\nmodel = AtacformerForReplacedTokenDetection(config)\nmodel = model.to(torch.bfloat16)\n\nprint(f\"Model size: {model.num_parameters()} parameters\")\n</code></pre>"},{"location":"atacformer/tutorials/training/train-from-scratch/#data-collation","title":"Data Collation","text":"<p>The <code>DataCollatorForReplacedTokenDetection</code> handles: - Random token replacement based on <code>mlm_probability</code> - Proper masking and attention handling - Batch preparation for training</p> <p>Create the data collator:</p> <pre><code>data_collator = DataCollatorForReplacedTokenDetection(\n    tokenizer=tokenizer,\n    mlm_probability=MLM_PROBABILITY,\n)\n</code></pre>"},{"location":"atacformer/tutorials/training/train-from-scratch/#setup-the-trainer","title":"Setup the Trainer","text":"<p>Now, set up the <code>Trainer</code> with the model, training arguments, and data collator:</p> <pre><code>training_args = TrainingArguments(\n    output_dir=\"output\",\n    num_train_epochs=25,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    run_name=RUN_NAME,\n    optim=\"adamw_torch\",\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=500,\n    learning_rate=MAX_LEARNING_RATE,\n    bf16=True,\n    max_grad_norm=1.0,\n)\n\n# trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=data_collator,\n)\n</code></pre>"},{"location":"atacformer/tutorials/training/train-from-scratch/#training","title":"Training","text":"<p>Finally, start the training process:</p> <pre><code>trainer.train()\nmodel.save_pretrained(\"output/atacformer-pretrained\")\n</code></pre>"},{"location":"bbconf/","title":"BBConf","text":"bbconf <p>BEDbase project configuration package (agent)</p>"},{"location":"bbconf/#what-is-this","title":"What is this?","text":"<p><code>bbconf</code> is a configuration and management tool for BEDbase, facilitating the reading of configuration files,  setting up connections to PostgreSQL, PEPhub, S3, and Qdrant databases, managing file paths, and storing transformer models.  It formalizes communication pathways for pipelines and downstream tools, ensuring seamless interaction.\"</p>"},{"location":"bbconf/#installation","title":"Installation","text":"<p>To install <code>bbconf</code> use this command: </p> <pre><code>pip install bbconf\n</code></pre> <p>or, install the latest version from the GitHub repository:</p> <pre><code>pip install git+https://github.com/databio/bbconf.git\n</code></pre>"},{"location":"bbconf/bbc_api/","title":"Guide","text":""},{"location":"bbconf/bbc_api/#docs-in-progress-stay-tuned-for-updates-were-working-hard-to-bring-you-valuable-content-soon","title":"\ud83d\udea7 Docs in progress! Stay tuned for updates. We're working hard to bring you valuable content soon!","text":""},{"location":"bbconf/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format.</p>"},{"location":"bbconf/changelog/#0130-2025-12-01","title":"[0.13.0] - 2025-12-01","text":""},{"location":"bbconf/changelog/#added","title":"Added:","text":"<ul> <li>UMAP calculation</li> </ul>"},{"location":"bbconf/changelog/#0120-2025-09-11","title":"[0.12.0] - 2025-09-11","text":""},{"location":"bbconf/changelog/#added_1","title":"Added:","text":"<ul> <li>New qdrant semantic search</li> <li>Added more plots to bedbase summary page</li> <li>New reference genome compatibility that supports new refgenie</li> <li>Genomes table, with ability of automatic updates from refgenie</li> <li>Added filter in search (Assay and genome)</li> </ul>"},{"location":"bbconf/changelog/#changed","title":"Changed:","text":"<ul> <li>Improved reindexing methods</li> </ul>"},{"location":"bbconf/changelog/#fixed","title":"Fixed:","text":"<ul> <li>Issues in bedfile update method</li> </ul>"},{"location":"bbconf/changelog/#0114-2025-06-01","title":"[0.11.4] - 2025-06-01","text":""},{"location":"bbconf/changelog/#fixed_1","title":"Fixed:","text":"<ul> <li>SQL search</li> </ul>"},{"location":"bbconf/changelog/#0113-2025-05-27","title":"[0.11.3] - 2025-05-27","text":""},{"location":"bbconf/changelog/#fixed_2","title":"Fixed:","text":"<ul> <li>Usage tracker</li> <li>Order of comprehensive stats</li> </ul>"},{"location":"bbconf/changelog/#added_2","title":"Added:","text":"<ul> <li>Platform usage stats method</li> <li>Concise option in stats method</li> </ul>"},{"location":"bbconf/changelog/#0112-2025-06-22","title":"[0.11.2] - 2025-06-22","text":""},{"location":"bbconf/changelog/#added_3","title":"Added:","text":"<ul> <li>Statistics about bed files grouped by organism</li> </ul>"},{"location":"bbconf/changelog/#0111-2025-05-22","title":"[0.11.1] - 2025-05-22","text":""},{"location":"bbconf/changelog/#fixed_3","title":"Fixed:","text":"<ul> <li>Bedbuncher bug</li> </ul>"},{"location":"bbconf/changelog/#0110-2025-04-21","title":"[0.11.0] - 2025-04-21","text":""},{"location":"bbconf/changelog/#added_4","title":"Added:","text":"<ul> <li>Added usage tracking and statistics</li> <li>Added platform comprehensive statistics</li> <li>Updated database to a new database schema</li> <li>Added new items for bed compliance</li> <li>Added head of bed file as a new column in bed table</li> <li>Added file digest column to the file table </li> <li>Changed global sample and project source to be an array instead of string</li> </ul>"},{"location":"bbconf/changelog/#0104-2025-01-21","title":"[0.10.4] - 2025-01-21","text":""},{"location":"bbconf/changelog/#fixed_4","title":"Fixed:","text":"<ul> <li>Changes boto3 #78</li> </ul>"},{"location":"bbconf/changelog/#0103-2025-01-16","title":"[0.10.3] - 2025-01-16","text":""},{"location":"bbconf/changelog/#added_5","title":"Added:","text":"<ul> <li>Added config analyzer</li> <li>Added new methods <code>get_missing_stats</code> and <code>get_missing_files</code> to retrieve lists of bed files that are missing statistics and files, respectively.</li> </ul>"},{"location":"bbconf/changelog/#fixed_5","title":"Fixed:","text":"<ul> <li>Fixed bugs in updating bed files</li> </ul>"},{"location":"bbconf/changelog/#0102-2025-01-09","title":"[0.10.2] - 2025-01-09","text":""},{"location":"bbconf/changelog/#changed_1","title":"Changed:","text":"<ul> <li>Updated version of zarr</li> </ul>"},{"location":"bbconf/changelog/#0101-2025-01-07","title":"[0.10.1] - 2025-01-07","text":""},{"location":"bbconf/changelog/#changed_2","title":"Changed:","text":"<ul> <li>Updated <code>geo_gsm_status</code> and <code>geo_gse_status</code> tables to include additional information.</li> </ul>"},{"location":"bbconf/changelog/#0100-2025-01-03","title":"[0.10.0] - 2025-01-03","text":""},{"location":"bbconf/changelog/#added_6","title":"Added:","text":"<ul> <li>Added a new method <code>get_neighbours</code> in <code>bbconf/modules/bedfiles.py</code> to retrieve the nearest neighbors of a bed file from Qdrant.</li> <li>Added a new method <code>sql_search</code> in <code>bbconf/modules/bedfiles.py</code> for performing SQL exact searches on bed files.</li> <li>Added a new method <code>get_track_hub_file</code> in <code>bbconf/modules/bedsets.py</code> to generate track hub files for bedsets.</li> <li>Added <code>processed</code> for uploading bed files and bedsets</li> <li>Added <code>update</code> bedfile method</li> </ul>"},{"location":"bbconf/changelog/#changed_3","title":"Changed:","text":"<ul> <li>Updated exception handling in the <code>create</code> method of <code>bbconf/modules/bedsets.py</code> to provide more specific error messages.</li> </ul>"},{"location":"bbconf/changelog/#090-2024-11-07","title":"[0.9.0] - 2024-11-07","text":""},{"location":"bbconf/changelog/#changed_4","title":"Changed","text":"<ul> <li>Fixed bug with uploading tss dist plot</li> </ul>"},{"location":"bbconf/changelog/#added_7","title":"Added","text":"<ul> <li>Added annotations to bedsets (author, source)</li> <li>get_genome_list method to bedfiles, that lists all available genomes</li> <li>Added method that lists all missing plots for bedfiles (get_missing_plots)</li> </ul>"},{"location":"bbconf/changelog/#080-2024-10-23","title":"[0.8.0] - 2024-10-23","text":""},{"location":"bbconf/changelog/#added_8","title":"Added:","text":"<ul> <li>New text2vec search (bivec search)</li> <li>Added get_pep to bedset methods</li> </ul>"},{"location":"bbconf/changelog/#071-2024-10-15","title":"[0.7.1] - 2024-10-15","text":""},{"location":"bbconf/changelog/#added_9","title":"Added:","text":"<ul> <li>Added table with standardized bed annotation</li> <li>Added table with bed reference genome prediction values.</li> </ul>"},{"location":"bbconf/changelog/#changed_5","title":"Changed:","text":"<ul> <li>Updated requirements</li> </ul>"},{"location":"bbconf/changelog/#070-2024-09-20","title":"[0.7.0] - 2024-09-20","text":""},{"location":"bbconf/changelog/#added_10","title":"Added","text":"<ul> <li>Table and methods for reference genome validator</li> <li>Table with standard metadata schema</li> <li>Bed file opening improvements</li> </ul>"},{"location":"bbconf/changelog/#061-2024-08-21","title":"[0.6.1] - 2024-08-21","text":""},{"location":"bbconf/changelog/#added_11","title":"Added","text":"<ul> <li>DB tables for GEO uploader status</li> </ul>"},{"location":"bbconf/changelog/#060-2024-05-01","title":"[0.6.0] - 2024-05-01","text":""},{"location":"bbconf/changelog/#added_12","title":"Added","text":"<ul> <li>Added tokenized files and universes.</li> <li>Added bed embedding get endpoint to the API #50</li> <li>Fixed test speed #48</li> <li>Added license for the bed files #51</li> <li>Added payloads to qdrant </li> <li>Fixed search, and query results info</li> <li>Many other small bug fixes</li> </ul>"},{"location":"bbconf/changelog/#051-2024-04-09","title":"[0.5.1] - 2024-04-09","text":""},{"location":"bbconf/changelog/#changed_6","title":"Changed","text":"<ul> <li>updated qdrant uploader</li> <li>bedset bedfile list query improvement</li> <li>other minor fixes in uploading</li> </ul>"},{"location":"bbconf/changelog/#050-2024-04-08","title":"[0.5.0] - 2024-04-08","text":""},{"location":"bbconf/changelog/#changed_7","title":"Changed","text":"<ul> <li>Rebuild bbconf</li> <li>Introduced new DB schema</li> <li>Added bbagent that will be used to interact with the database</li> <li>Updated config schema</li> <li>Added new functionality to the bbagent</li> <li>New tests</li> </ul>"},{"location":"bbconf/changelog/#042-2024-03-12","title":"[0.4.2] - 2024-03-12","text":""},{"location":"bbconf/changelog/#change","title":"Change","text":"<ul> <li>Updated logger</li> <li>Updated requirements</li> <li>Added <code>upload_status</code> column to the <code>bedfile</code> table</li> </ul>"},{"location":"bbconf/changelog/#041-2024-01-01","title":"[0.4.1] - 2024-01-01","text":""},{"location":"bbconf/changelog/#fix","title":"Fix","text":"<ul> <li>Requirements</li> </ul>"},{"location":"bbconf/changelog/#040-2023-12-18","title":"[0.4.0] - 2023-12-18","text":""},{"location":"bbconf/changelog/#change_1","title":"Change","text":"<ul> <li>bbconf to use pipestat v0.6.0 and SQLModel</li> <li>Fixed tests</li> </ul>"},{"location":"bbconf/changelog/#added_13","title":"Added","text":"<ul> <li><code>qdrant</code> search, insert and update functionality</li> <li>functions that return results in the DRS format for both bed and bedhost. DRS</li> </ul>"},{"location":"bbconf/changelog/#030-2022-08-18","title":"[0.3.0] - 2022-08-18","text":""},{"location":"bbconf/changelog/#change_2","title":"Change","text":"<ul> <li>update select_bedfiles_for_distance</li> <li>update database table schema</li> </ul>"},{"location":"bbconf/changelog/#021-2021-11-11","title":"[0.2.1] - 2021-11-11","text":""},{"location":"bbconf/changelog/#fix_1","title":"Fix","text":"<ul> <li>attempt to fix database connection error</li> </ul>"},{"location":"bbconf/changelog/#020-2021-10-25","title":"[0.2.0] - 2021-10-25","text":"<p>This release introduces backwards incompatible changes </p>"},{"location":"bbconf/changelog/#changed_8","title":"Changed","text":"<ul> <li>switched to object-relational mapping approach (ORM) for database interface</li> </ul>"},{"location":"bbconf/changelog/#011-2021-04-15","title":"[0.1.1] - 2021-04-15","text":""},{"location":"bbconf/changelog/#added_14","title":"Added","text":"<ul> <li>added new fields in the bedfiles and bedsets schema</li> </ul>"},{"location":"bbconf/changelog/#010-2021-02-22","title":"[0.1.0] - 2021-02-22","text":"<p>This release introduces backwards incompatible changes</p>"},{"location":"bbconf/changelog/#changed_9","title":"Changed","text":"<ul> <li><code>BedBaseConf</code> backend (database) to PostgreSQL</li> <li>complete <code>BedBaseConf</code> class redesign</li> </ul>"},{"location":"bbconf/changelog/#002-2020-05-28","title":"[0.0.2] - 2020-05-28","text":""},{"location":"bbconf/changelog/#added_15","title":"Added","text":"<ul> <li>index deleting methods:<ul> <li><code>delete_bedsets_index</code></li> <li><code>delete_bedfiles_index</code></li> </ul> </li> <li>multiple new keys constants</li> </ul>"},{"location":"bbconf/changelog/#changed_10","title":"Changed","text":"<ul> <li>make <code>search_bedfiles</code> and <code>search_bedsets</code> methods return all hits by default instead of just 10. Parametrize it.</li> <li>added more arguments to <code>insert_bedfiles_data</code> and <code>insert_bedsets_data</code> method interfaces: <code>doc_id</code> and <code>force_update</code></li> <li>Elasticsearch documents are inserted into the indices more securily, <code>insert_*</code> methods prevent documents duplication</li> </ul>"},{"location":"bbconf/changelog/#001-2020-02-05","title":"[0.0.1] - 2020-02-05","text":""},{"location":"bbconf/changelog/#added_16","title":"Added","text":"<ul> <li>initial project release</li> </ul>"},{"location":"bbconf/overview/","title":"DEMO of the bbconf module","text":"<p><code>bbconf</code> is a configuration and management tool for BEDbase, facilitating the reading of configuration file, setting up connections to PostgreSQL, PEPhub, S3, and Qdrant databases, managing file paths, and storing transformer models.</p>"},{"location":"bbconf/overview/#introduction","title":"Introduction","text":"<p><code>bbconf</code> is divided into 2 main modules: - <code>bbconf.config_parser</code> - reads the configuration file and sets up connections to databases.  <code>BedBaseConfig</code> class is used to store the passwords, configuration, connection objects, and other information.</p> <ul> <li><code>bbconf.modules</code> - contains modules for managing <code>bed_files</code>, <code>bedsets</code>, and other common functionalities. Users will mainly use this module because it provides classes with methods for managing the database.</li> </ul>"},{"location":"bbconf/overview/#example","title":"Example:","text":""},{"location":"bbconf/overview/#1-init-the-bedbaseagent-class","title":"1) Init the <code>BedBaseAgent</code> class","text":"<p><pre><code>from bbconf.bbagent import BedBaseAgent\n\nbbagent = BedBaseAgent(bbconf_file_path)\n</code></pre> Where <code>bbconf_file_path</code> is the path to the configuration file. How to create a configuration file is described in the configuration section.</p>"},{"location":"bbconf/overview/#upload-a-bedfile-to-the-database","title":"Upload a bedfile to the database","text":"<pre><code>    bbagent.bed.add(\n        identifier=bed_metadata.bed_digest,\n        stats=stats.model_dump(exclude_unset=True),\n        metadata=other_metadata,\n        plots=plots.model_dump(exclude_unset=True),\n        files=files.model_dump(exclude_unset=True),\n        classification=classification.model_dump(exclude_unset=True),\n        upload_qdrant=upload_qdrant,\n        upload_pephub=upload_pephub,\n        upload_s3=upload_s3,\n        local_path=outfolder,\n        overwrite=force_overwrite,\n        nofail=True,\n    )\n</code></pre>"},{"location":"bbconf/overview/#get-a-bedfile-from-the-database","title":"Get a bedfile from the database","text":"<pre><code>bed = bbagent.bed.get(identifier=bed_id, full=True,)\n</code></pre>"},{"location":"bbconf/overview/#get-a-bedset-from-the-database","title":"Get a bedset from the database","text":"<pre><code>bedset = bbagent.bedset.get(identifier=bedset_id, full=True,)\n</code></pre>"},{"location":"bbconf/overview/#user-can-access-credentials-and-other-configurations-from-the-bedbaseconfig-class","title":"User can access credentials and other configurations from the <code>BedBaseConfig</code> class","text":"<p>e.g. to get pephub namespace used in config you can use the following code:</p> <pre><code>bbagent.config._config[\"pephub\"][\"namespace\"]\n</code></pre> <p>Full API of bbconf can be found here</p>"},{"location":"bedbase/","title":"BEDbase","text":"<p>BEDbase is a unifying platform for aggregating, analyzing and serving genomic region data as BED files. Input files are processed by a series of Python pipelines. The output of these pipelines is displayed through a RESTful API where users can access BED files along with useful statistics and plots.</p>"},{"location":"bedbase/#services","title":"\ud83d\udef0\ufe0f Services","text":"<p>Deployed public instance: https://bedbase.org/</p> <p>API: https://api.bedbase.org/</p> <p>API dev: https://api-dev.bedbase.org/</p> <p>UI: https://bedbase.org/</p> <p>UI dev: https://dev.bedhost.pages.dev/</p> <p>Source Code: https://github.com/databio/bedhost/</p> <p>Object store, production https://data2.bedbase.org/ - base URL for cloudflare/backblaze</p>"},{"location":"bedbase/#components","title":"\ud83d\uddc3\ufe0f Components","text":"<ul> <li>bedboss: Main BEDbase processing pipeline and managing tool, combining bedmaker, bedstat, bedbuncher, and other pipelines</li> <li>bbconf: BEDbase configuration package (core of the BEDbase stack)</li> <li>bedhost: FastAPI application with API for accessing data</li> <li>bedhost-ui: Front-end user interface built with React</li> <li><code>all_geo_beds</code>: A subfolder containing scripts to download all bed files on GEO using geofetch and build a backend to host the metadata using bedstat</li> <li>geniml: Machine learning for genomic intervals</li> </ul>"},{"location":"bedbase/bedbase-loader/","title":"\ud83d\udd04 BEDbase loader","text":"<p>BEDbase Loader is an automated system with scheduled cron jobs that continuously fetches, processes, and integrates new BED files from public repositories into the BEDbase database. This ensures that BEDbase stays up to date with the latest available genomic data. BEDbase loader repository: https://github.com/databio/bedbase-loader</p>"},{"location":"bedbase/bedbase-loader/#key-features","title":"Key Features","text":"<ul> <li>Automated GEO Retrieval</li> <li>Automated BED heavy processing</li> <li>Automated Genomes Updater</li> <li>Umap creator</li> </ul>"},{"location":"bedbase/bedbase-loader/#automated-geo-retrieval","title":"\ud83d\udfe2 Automated GEO Retrieval","text":"<p>The main and most important part of the bedbase-loader is the automated retrieval of GEO data. Steps:</p> <ol> <li>Metadata is fetched from the PEPhub API (BEDbase repository: https://pephub.databio.org/bedbase). It selects GSE projects uploaded within a given time window (e.g., the last 2 days).</li> <li>BEDboss checks whether these projects have already been processed. If not, it proceeds to the next step.</li> <li>Metadata for all selected projects is retrieved from PEPhub, including file URLs.</li> <li>The files are downloaded and the metadata is inserted into the BEDbase database.</li> <li>Finally, the project status flag is updated to \"downloaded\", and the project is ready for the next step \u2014 heavy processing (see below).</li> </ol>"},{"location":"bedbase/bedbase-loader/#automated-bed-heavy-processing","title":"\ud83d\udfe2 Automated BED heavy processing","text":"<p>Many files are downloaded from GEO during automated retrieval. To speed up the initial download and insertion, heavy processing is skipped at this stage.</p> <p>Heavy processing is performed later in AWS using AWS Fargate and an automated cron job, after the files are downloaded and stored in the database.</p> <p>Docker image for heavy processing: https://github.com/databio/bedboss/blob/main/Dockerfile</p>"},{"location":"bedbase/bedbase-loader/#automated-genome-updates","title":"\ud83d\udfe2 Automated Genome Updates","text":"<p>The bedbase-loader includes an automated genome updater that fetches genomes from the Refgenie server. Information about all available genomes is stored in BEDbase, allowing each BED file to be linked to the exact reference genome used to create it.</p> <p>Genome updates are handled by a scheduled cron job: https://github.com/databio/bedbase-loader/blob/master/.github/workflows/update_genomes.yml</p>"},{"location":"bedbase/bedbase-loader/#umap-creator","title":"\ud83d\udfe2 UMAP Creator","text":"<p>An important part of BEDbase is the creation of embeddings for BED files. These embeddings enable visualization, providing insights into the data stored in BEDbase.</p> <p>The bedbase package automatically creates UMAP files, which are then visualized here: https://bedbase.org/umap To keep the UMAP visualization up-to-date, a scheduled cron job is used: https://github.com/databio/bedbase-loader/blob/master/.github/workflows/update_umap.yml</p>"},{"location":"bedbase/how-to-configure/","title":"How to create bedbase config file","text":"<p>BEDbase config file serves as a configuration file for the BEDbase server to provide credentials and paths to the required resources.</p>"},{"location":"bedbase/how-to-configure/#how-to-create-a-bedbase-config-file","title":"How to create a bedbase config file","text":"<p>There are two ways to create a bedbase config file:  1. Create a new file and copy the content from the example below.  2. Use BEDboss command: <pre><code>bedboss init-config --outfolder path/to/outfolder\n</code></pre></p>"},{"location":"bedbase/how-to-configure/#how-to-check-if-the-config-file-is-correct","title":"How to check if the config file is correct:","text":"<p>Use BEDboss command: <pre><code>bedboss verify-config --config path/to/config.yaml\n</code></pre></p>"},{"location":"bedbase/how-to-configure/#example-of-the-config-file","title":"Example of the config file:","text":"<pre><code>path:\n  remote_url_base: http://data.bedbase.org/\n  region2vec: databio/r2v-encode-hg38\n  vec2vec: databio/v2v-geo-hg38\n  text2vec: sentence-transformers/all-MiniLM-L6-v2\ndatabase:\n  host: $POSTGRES_HOST\n  port: 5432\n  password: $POSTGRES_PASSWORD\n  user: $POSTGRES_USER\n  database: bedbase2\nqdrant:\n  host: $QDRANT_HOST\n  port: 6333\n  api_key: $QDRANT_API_KEY\n  collection: bedbase2\nserver:\n  host: 0.0.0.0\n  port: 8000\ns3:\n  endpoint_url: $AWS_ENDPOINT_URL\n  aws_access_key_id: $AWS_ACCESS_KEY_ID\n  aws_secret_access_key: $AWS_SECRET_ACCESS_KEY\n  bucket: bedbase\nphc:\n  namespace: databio\n  name: bedbase_all\n  tag: default\naccess_methods:\n  http:\n    type: \"https\"\n    description: HTTP compatible path\n    prefix: https://data2.bedbase.org/\n  s3:\n    type: \"s3\"\n    description: S3 compatible path\n    prefix: s3://data2.bedbase.org/\n  local:\n    type: \"https\"\n    description: How to serve local files.\n    prefix: /static/\n</code></pre>"},{"location":"bedbase/user/bbclient/","title":"BEDbase Client","text":"<p>To interact with BEDbase, we provide a client that allows you to easily access and cache RegionSets (bed files) and  BedSets (Collection of bed files) from the BEDbase, and local or remote (from url) files. The client is designed to be user-friendly and efficient, making it easy to work with large datasets.</p> <p>BBclient is both a command line interface (CLI), a Python API and a Rust API.</p> <p>BBClient currently available in Geniml python package and gtars rust package.</p>"},{"location":"bedbase/user/bbclient/#installation","title":"\ud83d\udcbf Installation","text":"<p>Installation documentation is available Geniml installation.</p>"},{"location":"bedbase/user/bbclient/#examples","title":"\ud83e\uddea Examples","text":""},{"location":"bedbase/user/bbclient/#python","title":"Python","text":"<pre><code>from geniml.bbclient import BBClient\n\n# Create a BBClient instance\nbbclient = BBClient()\n\n# download, cache and return a RegionSet object*\nbedfile = bbclient.load_bed(\"233479aab145cffe46221475d5af5fae\")\n</code></pre>"},{"location":"bedbase/user/bbclient/#cli","title":"CLI","text":"<pre><code>geniml bbclient cache-bed 233479aab145cffe46221475d5af5fae\n\ngeniml bbclient seek 233479aab145cffe46221475d5af5fae\n</code></pre>"},{"location":"bedbase/user/bbclient/#rust","title":"Rust","text":"<p><pre><code>use gtars::bbclient::BBClient;\n\nlet mut bbc = BBClient::new(Some(cache_folder.clone()), None).expect(\"Failed to create BBClient\");\n\nlet bed_id: String = bbc\n            .add_local_bed_to_cache(PathBuf::from(_path/to.bed.gz), None)\n            .unwrap();\n</code></pre> Full usage documentation is available in the Usage documentation is available BBclient usage.</p>"},{"location":"bedbase/user/bbclient/#regionset","title":"\ud83e\uddf0 RegionSet","text":"<p>RegionSet is a Python/Rust/R representation of a BED file. It allows user to compute identifiers, save bed files,  iterate through regions, and perform other operations on the BED file.</p> <p>How to install and use RegionSet in Python is described in the RegionSet documentation. </p> <p>Quick example:</p> <pre><code>from gtars.models import RegionSet\nrs = RegionSet(\"https://api.bedbase.org/v1/files/files/d/c/dcc005e8761ad5599545cc538f6a2a4d.bed.gz\")\nrs.identifier\n</code></pre>"},{"location":"bedbase/user/bed-analyzer/","title":"\u2699\ufe0f BED analyzer","text":"<p>\ud83d\udea7 This documentation is under active development and will be updated soon. \ud83d\udea7</p>"},{"location":"bedbase/user/bed_classification/","title":"BED Classification","text":"<p>BED classification is a calculated metadata about RegionSet file that classifies RegionSet files based on the number of columns and the types of data contained within those columns.</p> <p>BED classifier calculates 2 metadata fields: BED compliance and Data Format. (BED classifier tutorial can be found here: BED Classifier tutorial)</p>"},{"location":"bedbase/user/bed_classification/#bed-compliance-compliant_columns-non_compliant_columns","title":"\ud83d\udfe2 BED compliance (<code>compliant_columns</code> + <code>non_compliant_columns</code>)","text":"<p>BED compliance is a string that indicates the number of compliant and non-compliant columns in the BED file.  It is represented as <code>bedn+m</code>, where <code>n</code> is the number of compliant columns, and <code>m</code> is the number of non-compliant columns.  For example, <code>bed3+0</code> indicates that there are 3 compliant columns and 0 non-compliant columns.</p> Example <p><pre><code>bed3+0, bed7+3\n</code></pre> Where:  - <code>bed3+0</code> indicates that there are 3 compliant columns and 0 non-compliant columns.  - <code>bed7+3</code> indicates that there are 7 compliant columns and 3 non-compliant columns.</p>"},{"location":"bedbase/user/bed_classification/#data-formats","title":"\ud83d\udfe2 Data formats","text":"<p>The data format is a string that indicates flavor of the RegionSet format. We defined the following data formats:</p> <pre><code>class DATA_FORMAT(str, Enum):\n    UNKNOWN = \"unknown_data_format\"\n    UCSC_BED = \"ucsc_bed\"\n    UCSC_BED_RS = \"ucsc_bed_rs\"\n    BED_LIKE = \"bed_like\"\n    BED_LIKE_RS = \"bed_like_rs\"\n    ENCODE_NARROWPEAK = \"encode_narrowpeak\"\n    ENCODE_NARROWPEAK_RS = \"encode_narrowpeak_rs\"\n    ENCODE_BROADPEAK = \"encode_broadpeak\"\n    ENCODE_BROADPEAK_RS = \"encode_broadpeak_rs\"\n    ENCODE_GAPPEDPEAK = \"encode_gappedpeak\"\n    ENCODE_GAPPEDPEAK_RS = \"encode_gappedpeak_rs\"\n    ENCODE_RNA_ELEMENTS = \"encode_rna_elements\"\n    ENCODE_RNA_ELEMENTS_RS = \"encode_rna_elements_rs\"\n</code></pre> Note <p><code>rs</code> refers to <code>relaxed_score</code> which indicates that a fifth column was present where the values are integers greater than 0.  In contrast, a strict interpretation for column 5 is: A score between 0 and 1000.</p>"},{"location":"bedbase/user/bed_classification/#unknown","title":"UNKNOWN","text":"<p>Classification was unable to determine the data format.</p>"},{"location":"bedbase/user/bed_classification/#ucsc_bed","title":"UCSC_BED","text":"<p>Conforms to ucsc bed</p>"},{"location":"bedbase/user/bed_classification/#ucsc_bed_rs","title":"UCSC_BED_RS","text":"<p>Conforms to ucsc bed but with a relaxed interpretation for the fifth column.</p>"},{"location":"bedbase/user/bed_classification/#bed_like","title":"BED_LIKE","text":"<p>Data is tab delimited but contains columns that are not compliant with ucsc bed. Example: <code>bedn+m</code> where <code>n</code> are compliant columns, <code>m</code> are non-compliant columns and <code>m &gt; 0</code> </p>"},{"location":"bedbase/user/bed_classification/#bed_like_rs","title":"BED_LIKE_RS","text":"<p>Data is tab delimited but contains columns that are not compliant with ucsc bed but with a relaxed interpretation for the fifth column. Example: <code>bedn+m</code> where <code>n</code> are compliant columns, <code>m</code> are non-compliant columns and <code>m &gt; 0</code>, Column 5 = <code>integer &gt; 0</code> </p>"},{"location":"bedbase/user/bed_classification/#encode_narrowpeak","title":"ENCODE_NARROWPEAK","text":"<p>Conforms to ENCODE narrowPeak</p>"},{"location":"bedbase/user/bed_classification/#encode_narrowpeak_rs","title":"ENCODE_NARROWPEAK_RS","text":"<p>Conforms to ENCODE narrowPeak but with a relaxed interpretation for the fifth column.</p>"},{"location":"bedbase/user/bed_classification/#encode_broadpeak","title":"ENCODE_BROADPEAK","text":"<p>Conforms to ENCODE broadPeak </p>"},{"location":"bedbase/user/bed_classification/#encode_broadpeak_rs","title":"ENCODE_BROADPEAK_RS","text":"<p>Conforms to ENCODE broadPeak but with a relaxed interpretation for the fifth column.</p>"},{"location":"bedbase/user/bed_classification/#encode_gappedpeak","title":"ENCODE_GAPPEDPEAK","text":"<p>Conforms to ENCODE gappedPeak</p>"},{"location":"bedbase/user/bed_classification/#encode_gappedpeak_rs","title":"ENCODE_GAPPEDPEAK_RS","text":"<p>Conforms to ENCODE gappedPeak but with a relaxed interpretation for the fifth column.</p>"},{"location":"bedbase/user/bed_classification/#encode_rna_elements","title":"ENCODE_RNA_ELEMENTS","text":"<p>Conforms to ENCODE RNA elements</p>"},{"location":"bedbase/user/bed_classification/#encode_rna_elements_rs","title":"ENCODE_RNA_ELEMENTS_RS","text":"<p>Conforms to ENCODE RNA elements  but with a relaxed interpretation for the fifth column.</p>"},{"location":"bedbase/user/bed_classification/#i-references","title":"\u2139\ufe0f References:","text":"<ul> <li> <p>Information on various data formats can be found here: https://genome.ucsc.edu/FAQ/FAQformat.html</p> </li> <li> <p>Additional detailed specifications for the Browser Extensible Data (BED) format can be found here: https://samtools.github.io/hts-specs/BEDv1.pdf</p> </li> <li> <p>BEDBoss tutorial of bedclassifier can be found here: BED Classifier tutorial</p> </li> </ul>"},{"location":"bedbase/user/bedbase-api-user-guide/","title":"BEDbase API user guide","text":""},{"location":"bedbase/user/bedbase-api-user-guide/#introduction","title":"Introduction","text":""},{"location":"bedbase/user/bedbase-api-user-guide/#data-types","title":"Data types","text":"<p>BEDbase stores two types of data, which we call records. They are 1. BEDs, and 2. BEDsets.  BEDsets are simply collections of BEDs. Each record in the database is either a BED or a BEDset.</p>"},{"location":"bedbase/user/bedbase-api-user-guide/#endpoint-organization","title":"Endpoint organization","text":"<p>The endpoints are divided into 3 groups:</p> <ol> <li><code>/bed</code> endpoints are used to interact with metadata for BED records.</li> <li><code>/bedset</code> endpoints are used to interact with metadata for BEDset records.</li> <li><code>/objects</code> endpoints are used to download metadata and get URLs to retrieve the underlying data itself. These endpoints implement the GA4GH DRS standard.</li> </ol> <p>Therefore, to get information and statistics about BED or BEDset records, or what is contained in the database, look through the <code>/bed</code> and <code>/bedset</code> endpoints. But if you need to write a tool that gets the actual underlying files, then you'll need to use the <code>/objects</code> endpoints. The type of identifiers used in each case differ.</p>"},{"location":"bedbase/user/bedbase-api-user-guide/#record-identifiers-vs-object-identifiers","title":"Record identifiers vs. object identifiers","text":"<p>Each record has an identifier. For example, <code>eaf9ee97241f300f1c7e76e1f945141f</code> is a BED identifier. You can use this identifier for the metadata endpoints. To download files, you'll need something slightly different -- you need an object identifier. This is because each BED record includes multiple files, such as the original BED file, the BigBed file, analysis plots, and so on. To download a file, you will construct what we call the <code>object_id</code>, which identifies the specific file.</p>"},{"location":"bedbase/user/bedbase-api-user-guide/#how-to-construct-object-identifiers","title":"How to construct object identifiers","text":"<p>Object IDs take the form <code>&lt;record_type&gt;.&lt;record_identifier&gt;.&lt;result_id&gt;</code>. An example of an object_id for a BED file is <code>bed.eaf9ee97241f300f1c7e76e1f945141f.bedfile</code></p> <p>So, you can get information about this object like this:</p> <p><code>GET</code> /objects/bed.eaf9ee97241f300f1c7e76e1f945141f.bedfile</p> <p>Or, you can get a URL to download the actual file with:</p> <p><code>GET</code> /objects/bed.eaf9ee97241f300f1c7e76e1f945141f.bedfile/access/http</p>"},{"location":"bedbase/user/bedbase-search/","title":"BEDbase Search","text":"<p>BEDbase supports three types of search methods:</p>"},{"location":"bedbase/user/bedbase-search/#text2bed-search","title":"\ud83d\udfe2 Text2BED search","text":"<p>Text2BED search allows you to find relevant BED files using natural language queries. This search is a first tab in the search page, and is a default search.</p> <p>To use the search, provide text query (e.g., \u201cheart\u201d, or  \u201ck562\u201d) and click search.</p> <p>At this moment, we are using semantic search. Compared to traditional keyword-based search, semantic search understands the meaning behind your query, allowing for more relevant results. Most of the available platforms (GEO, ENCODE, etc) use traditional search, that limits findability.</p> <p>How does it work? 1. The query text is encoded into a vector using a pre-trained language model. 2. This vector is compared against precomputed vectors for all BED files in the database. 3. The system retrieves and ranks BED files based on similarity to your query vector.</p> <p>Example of the search page: https://bedbase.org/search?q=USF2</p> <p>Additionally, we have provided filters to narrow down your search results. You can filter by:  - Assay type (e.g., ChIP-seq, ATAC-seq) - Reference genome (e.g., hg19, hg38, mm10)</p> <p>* We are actively working on improving the search experience. We are developing Bi-Vector search,  which will combine metadata and BED content for more accurate results. Stay tuned for updates!</p>"},{"location":"bedbase/user/bedbase-search/#bed2bed-search","title":"\ud83d\udfe2 BED2BED Search","text":"<p>(Available only for hg38)</p> <p>BED to BED search allows you to find relevant BED files using your own BED file as a query.</p> <p>It is a powerful way to find similar files to compare with your own data. This search can be used as a quality control step of your own data, finding functional similarities to prove the correctness of your data, or just to find similar datasets for your analysis.</p> <p>To use the search, go to the second tab of the search page, and upload your BED file. This bed file should be less than 20MB and should be a valid BED format. BED-like file can be both gzipped or unzipped, we will handle both formats. \ud83d\ude03</p> <p>After file has been uploaded, we are running quality control steps to make sure the file is valid.</p> <p>How does it work?  1. BED-like has been uploaded to the server. 2. The file is encoded into a vector representation. 3. BEDbase compares it to stored BED file vectors. 4. The system retrieves the stored BED files most similar to your query.  </p>"},{"location":"bedbase/user/bedbase-search/#bedset-search","title":"\ud83d\udfe2 BEDSet Search","text":"<p>Third type of search is BEDset search. This search is a  string matching search. It is looking for exact matches of BEDset names, descriptions, and summaries. BEDset search is in the third tab of the search page. In the future, we will add semantic search for BEDsets.</p>"},{"location":"bedbase/user/bedbase-ui-user-guide/","title":"BEDbase public instance","text":"<p>The public BEDbase instance at bedbase.org hosts public BED files.</p>"},{"location":"bedbase/user/bedbase-ui-user-guide/#resource-links","title":"\ud83d\udef0\ufe0f Resource links","text":"<ul> <li>Documentation: https://docs.bedbase.org/</li> <li>Deployed public instance UI: https://bedbase.org/</li> <li>Dev UI: https://dev.bedhost.pages.dev/</li> <li>API: https://api.bedbase.org/</li> <li>Dev API: https://api-dev.bedbase.org/</li> </ul>"},{"location":"bedbase/user/bedbase-ui-user-guide/#finding-relevant-bed-files","title":"\ud83d\udd0d Finding relevant BED files","text":"<p>The best way to locate data is to use the search interface on the bedbase.org home page. This search interface is smart. It relies on our Text2BED models, which allow you to search the genome using natural language. We have previously computed embeddings for each BED file in BEDbase, and then you can search them.</p>"},{"location":"bedbase/user/bedbase-ui-user-guide/#downloading-data","title":"\ud83d\udce5 Downloading data","text":"<p>From the search interface, you can add results to your cart, then download all the files in your cart.</p>"},{"location":"bedbase/user/bedbase-ui-user-guide/#bedsets","title":"\ud83d\uddc3\ufe0f BEDsets","text":"<p>BEDsets are collections of BED files. BEDbase holds tens of thousands of files, which span reference genome assemblies.  You aren't likely to want to use all the data for one project.  BEDsets provide a way to group together a subset of the files in BEDbase for a particular purpose. In the future, all users  will be able to create their own BEDsets.</p>"},{"location":"bedbase/user/bedbase-ui-user-guide/#reference-genomes-compatibility-comparison","title":"\ud83e\uddec Reference genomes compatibility comparison","text":"<p>BED files in BEDbase are mapped to different reference genomes. Bedbase provides a validation of BED files to the reference genomes  available on https://ui.refgenie.org/ . It provides an insight into which reference genome is most likely used in mapping the BED file. More information about reference genome validation can be found here.</p>"},{"location":"bedbase/user/bedbase-ui-user-guide/#bed-classification","title":"\ud83d\udccb BED classification","text":"<p>BEDbase classifies BED files into different categories based on their content. It provides user with bed compliance and data format was used in generating the BED file. Often, BED files are not strictly following the UCSC BED format, or narrowPeak/broadPeak formats. BED classification provides a way to understand the content of the BED file. More information about BED classification can be found here.</p>"},{"location":"bedbase/user/bedbase-ui-user-guide/#bed-file-statistics","title":"\ud83d\udcca BED file statistics","text":"<p>BEDbase provides statistics about the BED files in the database. It includes:  - Genomic features information - Regions distributions over chromosomes - GC content distribution - Cell specific enrichment analysis - and more.</p>"},{"location":"bedbase/user/bedbase-ui-user-guide/#similarity-search","title":"\ud83d\udcab Similarity search","text":"<p>BED files page include information about most similar files based on genomic regions similarity. It incorporates BED-to-BED search and provides insights on similarity even if  metadata of some of the files is missing or incomplete.</p>"},{"location":"bedbase/user/reference-genome-compatibility/","title":"Reference genome compatibility","text":"<p>Reference genome compatibility is one of the features of BEDbase. Each file that is uploaded to the database is  being processed by BEDboss pipeline that allows to calculate reference genome compatibility with multiple reference genomes.</p> <p>On the bedbase UI user can find comparison of the BED compatibility with most popular reference genomes.</p> <p>Compatibility is assessed by 3 criteria: </p> <ul> <li>XS (Sensitivity/Recall of Chrom Names), </li> <li>OOBR (Sensitivity/Recall of Chrom Lengths),</li> <li>SF (Specificity of Chrom Names with Respect to Sequence Lengths).</li> </ul> <p>After these 3 main statistics are calculated, the tiers are assigned by taking into consideration each score.</p> <p>The tiers are:</p> <ul> <li>TIER 1: Excellent</li> <li>TIER 2: Good</li> <li>TIER 3: Medium</li> <li>TIER 4: Poor</li> </ul>"},{"location":"bedbase/user/reference-genome-compatibility/#quantitatively-assess-name-overlaps-xs","title":"\ud83d\udfe2 Quantitatively Assess Name Overlaps (XS)","text":"<p>True Positive (TP): Sequence name is in both the query and the reference</p> <p>False negative (FN): Sequence name is in the query but not in the reference.</p> <p></p>"},{"location":"bedbase/user/reference-genome-compatibility/#out-of-bounds-regions-oobs","title":"\ud83d\udfe2 Out of Bounds Regions (OOBS)","text":"<p>True Positive (TP): Query region is contained within the reference sequence</p> <p>False Negative (FN): Query region extends beyond the reference sequence</p> <p></p>"},{"location":"bedbase/user/reference-genome-compatibility/#sequence-fit-sf","title":"\ud83d\udfe2 Sequence Fit (SF)","text":"<p>Sequence fit is similar to the concept of precision where P=TP/(TP+FP) and we define:</p> <p>True Positive (TP): Sequence Name in Query is within the Reference</p> <p>False Positive (FP): Sequence Name is within the Reference but NOT within the Query.</p> <p>However, we must also consider the lengths of each region as well.</p> <p></p>"},{"location":"bedbase/user/reference-genome-compatibility/#assigning-tiers","title":"\ud83d\udfe3 Assigning  Tiers","text":"<p>Finally, tiers are assigned based on the combination of how well each query file ranks for XS, OOBR, and SF.</p> <p></p>"},{"location":"bedbase/user/reference-genome-compatibility/#i-references","title":"\u2139\ufe0f References:","text":"<ul> <li>BEDBoss tutorial of assessing reference genome compatibility can be found here: Reference genome validator tutorial</li> </ul>"},{"location":"bedbase/user/umap-visualizer/","title":"\ud83e\udea9 Embedding Visualization","text":"<p>\ud83d\udea7 This documentation is under active development and will be updated soon. \ud83d\udea7</p>"},{"location":"bedboss/","title":"\ud83e\uddf0 BEDBoss processing pipeline","text":"<p>A command-line tool and Python package for managing and processing genomic interval region files and bedsets in BEDbase. BEDboss is highly related to BEDbase, nevertheless, it can be used as a standalone tool for calculating statistics, converting files, and verifying the quality of BED files.</p>"},{"location":"bedboss/#installation","title":"\ud83d\udcbf Installation","text":"<p>To install <code>bedboss</code> use this command:  <pre><code>pip install bedboss\n</code></pre> or install the latest version from the GitHub repository: <pre><code>pip install git+https://github.com/databio/bedboss.git\n</code></pre></p>"},{"location":"bedboss/#cli-usage","title":"\ud83d\udcbb CLI usage:","text":"<p>Command line documentation is available here: \ud83d\udcd1 CLI usage </p>"},{"location":"bedboss/#bedbase-configuration-file","title":"\ud83d\udcd1 BEDbase configuration file","text":"<p>To run most of the pipelines, you need to create a BEDbase configuration file.</p> <p>How to create a BEDbase configuration file is described in the configuration section.</p>"},{"location":"bedboss/#main-components","title":"\ud83d\uddc3\ufe0f Main components:","text":"<p>1) bedmaker - pipeline to convert various genomic interval file types into BED format and bigBed format.  2) bedqc - quality assessment pipeline of bed files  3) bedstat - pipeline for obtaining statistics about bed files.  4) bedbuncher - pipeline for grouping bed files in collections and calculation statistics about them.  5) bedclassifier - scripts for classifying bed files based on their columns.  6) refgenome_validator - pipeline for validating the reference genome of the bed files.  7) bbuploader - pipeline for uploading bed files from GEO database to the BEDbase database and processing them. </p> <p>Mainly pipelines are intended to be run from command line but nevertheless,  they are also available as a python functions, so that user can use them independently.</p>"},{"location":"bedboss/#bedboss-dependencies","title":"\ud83d\udce6 BEDboss dependencies","text":"<p>Before running any of the pipelines, you need to install the required dependencies.</p> <p>To check if all dependencies are installed, you can run the following command:</p> <pre><code>bedboss check-requirements\n</code></pre> <p>To install all R dependencies, you can run the following command:</p> <pre><code>bedboss install-requirements\n</code></pre> <p>Additionally, sometimes you would need to have UCSC tools installed on your system. To install UCSC tools, follow initial instructions from the UCSC website.</p>"},{"location":"bedboss/#i-bedboss-pipelines","title":"\u2139\ufe0f BEDboss pipelines:","text":""},{"location":"bedboss/#bedmaker","title":"\ud83d\udfe2 bedmaker","text":"<p>Bedmaker can convert different interval region set files to BED and bigBed format, cache it using Geniml bbclient.</p> <p>Supported formats are: - bedGraph - bigBed - bigWig - wig</p>"},{"location":"bedboss/#bedqc","title":"\ud83d\udfe2 bedqc","text":"<p>Evaluates bed files if statistically they are correct, and if they should be included in the downstream analysis.  Currently, it flags bed files that are larger than 2G, has over 5 milliom regions, and/or has mean region width less than 10 bp. This threshold can be changed in bedqc function arguments.</p>"},{"location":"bedboss/#bedstat","title":"\ud83d\udfe2 bedstat","text":"<p>Pipeline for obtaining statistics about bed files. Statistics include:</p> <ul> <li>GC content.The average GC content of the region set. </li> <li>Number of regions. The total number of regions in the BED file. </li> <li>Median TSS distance. The median absolute distance to the Transcription Start Sites (TSS)</li> <li>Mean region width. The average region width of the region set.</li> <li>Exon percentage.  The percentage of the regions in the BED file that are annotated as exon. </li> <li>Intron percentage.    The percentage of the regions in the BED file that are annotated as intron.</li> <li>Promoter proc percentage. The percentage of the regions in the BED file that are annotated as promoter-prox.</li> <li>Intergenic percentage. The percentage of the regions in the BED file that are annotated as intergenic.</li> <li>Promoter core percentage. The percentage of the regions in the BED file that are annotated as promoter-core.</li> <li>5' UTR percentage. The percentage of the regions in the BED file that are annotated as 5'-UTR.</li> <li>3' UTR percentage. The percentage of the regions in the BED file that are annotated as 3'-UTR.</li> </ul>"},{"location":"bedboss/#bedbuncher","title":"\ud83d\udfe2 bedbuncher","text":"<p>Pipeline designed to create bedsets (collections of BED files) that will be retrieved from bedbase.</p> <p>Example bedsets:</p> <ul> <li>Bed files from the AML database.</li> <li>Bed files from the Excluderanges database.</li> <li>Bed files from the LOLA database https://databio.org/regiondb</li> </ul> <p>*This pipeline is available only in for bedbase processing, and can't be use as a standalone tool.</p>"},{"location":"bedboss/#bedclassifier","title":"\ud83d\udfe2 bedclassifier","text":"<p>Pipeline for classifying bed files based on their columns. The example output of the bedclassifier is bed_format: <code>nerrowopeak</code>/<code>broadpeak</code>/<code>bed</code> and bed_type: <code>bed3+5</code>.</p>"},{"location":"bedboss/#refgenome_validator","title":"\ud83d\udfe2 refgenome_validator","text":"<p>Pipeline for validating the reference genome of the bed files.  It is standalone tool, and can be used independently. It tries to validate and predict the reference genome of the bed files. by comparing the regions in the bed file with the reference genome. It produces the ranking of the reference genomes where 1 is the best match and 4 is the worst match.</p>"},{"location":"bedboss/#geouploader","title":"\ud83d\udfe2 GEOuploader","text":"<p>Module for uploading bed files from GEO database to the BEDbase database and processing them. Data for uploading files  are taken from the PEPhub database, where all GEO metadata is stored.</p>"},{"location":"bedboss/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format.</p>"},{"location":"bedboss/changelog/#080-2025-09-11","title":"[0.8.0] - 2025-09-11","text":""},{"location":"bedboss/changelog/#added","title":"Added:","text":"<ul> <li>Added CLI for automatic UMAP generation from qdrant</li> <li>Added \"generate UMAP\" cli command</li> </ul>"},{"location":"bedboss/changelog/#changed","title":"Changed:","text":"<ul> <li>Changed data source of chrom sizes in bed reference validator to new refgenie.</li> </ul>"},{"location":"bedboss/changelog/#fixed","title":"Fixed:","text":"<ul> <li>Fixed bedboss logger</li> <li>Improved vector db reindexing</li> <li>Improved efficiency of bdd Reference validator function</li> </ul>"},{"location":"bedboss/changelog/#073-2025-06-28","title":"[0.7.3] - 2025-06-28","text":""},{"location":"bedboss/changelog/#added_1","title":"Added:","text":"<ul> <li>Added filter for rerunning unprocessed bed files based on genome</li> </ul>"},{"location":"bedboss/changelog/#072-2025-06-21","title":"[0.7.2] - 2025-06-21","text":""},{"location":"bedboss/changelog/#changed_1","title":"Changed:","text":"<ul> <li>Updated path to the bigbed output folder</li> <li>Added update of metadata of the bed file</li> <li>Updated docker file </li> <li>Remove bedqc module</li> </ul>"},{"location":"bedboss/changelog/#071-2025-04-22","title":"[0.7.1] - 2025-04-22","text":""},{"location":"bedboss/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Minor bug fixes</li> </ul>"},{"location":"bedboss/changelog/#070-2025-04-21","title":"[0.7.0] - 2025-04-21","text":""},{"location":"bedboss/changelog/#added_2","title":"Added","text":"<ul> <li>Added dockerfile</li> <li>Added R processing as separate service</li> <li>Added initial qc for GEO data (QC, without loading full data)</li> <li>Refactored R code</li> <li>Added summary for bedsets #112 </li> <li>Added original submission date of geo to the table #95 </li> </ul>"},{"location":"bedboss/changelog/#changed_2","title":"Changed:","text":"<ul> <li>Improved logging</li> <li>Fixed python3.12 warnings</li> <li>Improved bedmaker. (identifier, bigbed, qc)</li> <li>Removed bedqc module - moved it to handle everything by RegionSet object</li> <li>Improved bedclasifier #101</li> <li>Fixed #97 </li> <li>Fixed #110</li> <li>Fixed multiple bugs in geo uploader</li> <li>Switched RegionSet from geniml to RegionSet from Gtars</li> </ul>"},{"location":"bedboss/changelog/#060-2025-01-17","title":"[0.6.0] - 2025-01-17","text":""},{"location":"bedboss/changelog/#added_3","title":"Added:","text":"<ul> <li>Added open_chromatin plot back into processing.</li> <li>Added gtrs dependency, that calculates gc content.</li> <li>Added skipper that automatically skips samples in pep that were already processed.</li> <li>Added lite functionality to main functions that allows to run uploading without using any heavy processing.</li> <li>Added function that will reprocess files, if they were unprocessed in the bedbase.</li> <li>Added function that predicts genome if genome wasn't provided</li> </ul>"},{"location":"bedboss/changelog/#changed_3","title":"Changed:","text":"<ul> <li>Important speed improvements.</li> </ul>"},{"location":"bedboss/changelog/#050-2025-01-16","title":"[0.5.0] - 2025-01-16","text":""},{"location":"bedboss/changelog/#added_4","title":"Added","text":"<ul> <li>Added open_chromatin plot back into processing.</li> <li>Added gtrs dependency, that calculates gc content.</li> <li>Added skipper that automatically skips samples in pep that were already processed.</li> <li>Added lite functionality to main functions that allows to run uploading without using any heavy processing.</li> <li>Added function that will reprocess files, if they were unprocessed in the bedbase.</li> <li>Added function that predicts genome if genome wasn't provided.</li> </ul>"},{"location":"bedboss/changelog/#fixes","title":"Fixes","text":"<ul> <li>Important speed improvements.</li> <li>Improved requirements checker.</li> <li>Minor bug fixes.</li> </ul>"},{"location":"bedboss/changelog/#041-2024-09-20","title":"[0.4.1] - 2024-09-20","text":""},{"location":"bedboss/changelog/#added_5","title":"Added","text":"<ul> <li>Standardization of peps using bedbase bedms schema</li> <li>Reference validator module</li> </ul>"},{"location":"bedboss/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Pipeline failures (due to pipeline manager)</li> <li>Failure in cleaning temp files</li> </ul>"},{"location":"bedboss/changelog/#040-2024-08-26","title":"[0.4.0] - 2024-08-26","text":""},{"location":"bedboss/changelog/#added_6","title":"Added","text":"<ul> <li>Added bbuploader (GEO uploader)</li> </ul>"},{"location":"bedboss/changelog/#030-2024-08-21","title":"[0.3.0] - 2024-08-21","text":""},{"location":"bedboss/changelog/#added_7","title":"Added","text":"<ul> <li>Added classifier</li> <li>Added create universe uploader</li> <li>Added tokenization and token uploader</li> </ul>"},{"location":"bedboss/changelog/#changes","title":"Changes","text":"<ul> <li>Updated efficiency of CLI help</li> </ul>"},{"location":"bedboss/changelog/#021-2024-04-09","title":"[0.2.1] - 2024-04-09","text":""},{"location":"bedboss/changelog/#changed_4","title":"Changed","text":"<ul> <li>small naming tweaks</li> </ul>"},{"location":"bedboss/changelog/#added_8","title":"Added","text":"<ul> <li>added requirement check to cli</li> </ul>"},{"location":"bedboss/changelog/#020-2024-04-08","title":"[0.2.0] - 2024-04-08","text":""},{"location":"bedboss/changelog/#changed_5","title":"Changed","text":"<ul> <li>moved all uploading functionality to the <code>bbconf</code> package</li> </ul>"},{"location":"bedboss/changelog/#added_9","title":"Added","text":"<ul> <li>added commands for indexing bedfiles</li> <li>added commands for deleting bedfiles and bedsets</li> </ul>"},{"location":"bedboss/changelog/#010-2024-01-26","title":"[0.1.0] - 2024-01-26","text":""},{"location":"bedboss/changelog/#added_10","title":"Added","text":"<ul> <li>Initial alpha release</li> </ul>"},{"location":"bedboss/usage/","title":"Command line interface reference","text":"<p>BEDboss is command-line tool-manager and a set of tools for working with BED files and BEDbase. Main components of BEDboss are:  1) Pipeline for processing BED files: bedmaker, bedqc, and bedstats. 2) Indexing of the Bed files in bedbase 3) Managing bed and bedsets in the database</p> <p>Here you can see the command-line usage instructions for the main bedboss command and for each subcommand:</p>"},{"location":"bedboss/usage/#bedboss-help","title":"<code>bedboss --help</code>","text":"<pre><code> Usage: bedboss [OPTIONS] COMMAND [ARGS]...                                             \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version             -v        App version                                          \u2502\n\u2502 --install-completion            Install completion for the current shell.            \u2502\n\u2502 --show-completion               Show completion for the current shell, to copy it or \u2502\n\u2502                                 customize the installation.                          \u2502\n\u2502 --help                          Show this message and exit.                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 run-all                Run all the bedboss pipeline for a single bed file            \u2502\n\u2502 run-pep                Run the all bedboss pipeline for a bed files in a PEP         \u2502\n\u2502 reprocess-all          Run unprocessed files, or reprocess them                      \u2502\n\u2502 reprocess-one          Run unprocessed file, or reprocess it [Only 1 file]           \u2502\n\u2502 reprocess-bedset       Reprocess a bedset                                            \u2502\n\u2502 make-bed               Create a bed files form a [bigwig, bedgraph, bed, bigbed,     \u2502\n\u2502                        wig] file                                                     \u2502\n\u2502 make-bigbed            Create a bigbed files form a bed file                         \u2502\n\u2502 run-qc                 Run the quality control for a bed file                        \u2502\n\u2502 run-stats              Create the statistics for a single bed file.                  \u2502\n\u2502 reindex                Reindex the bedbase database and insert all files to the      \u2502\n\u2502                        qdrant database.                                              \u2502\n\u2502 make-bedset            Create a bedset from a pep file, and insert it to the bedbase \u2502\n\u2502                        database.                                                     \u2502\n\u2502 init-config            Initialize the new, sample configuration file                 \u2502\n\u2502 delete-bed             Delete bed from the bedbase database                          \u2502\n\u2502 delete-bedset          Delete BedSet from the bedbase database                       \u2502\n\u2502 tokenize-bed           Tokenize a bedfile                                            \u2502\n\u2502 delete-tokenized       Delete tokenized bed file                                     \u2502\n\u2502 convert-universe       Convert bed file to universe                                  \u2502\n\u2502 check-requirements     Check installed R packages                                    \u2502\n\u2502 install-requirements   Install R dependencies                                        \u2502\n\u2502 verify-config          Verify configuration file                                     \u2502\n\u2502 geo                    Automatic BEDbase uploader for GEO data                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-geo-help","title":"<code>bedboss geo --help</code>","text":"<pre><code> Usage: bedboss geo [OPTIONS] COMMAND [ARGS]...                                         \n\n Automatic BEDbase uploader for GEO data                                                \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version  -v        App version                                                     \u2502\n\u2502 --help               Show this message and exit.                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 upload-all   Run bedboss uploading pipeline for specified genome in specified period \u2502\n\u2502              of time.                                                                \u2502\n\u2502 upload-gse   Run bedboss uploading pipeline for GSE.                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-geo-upload-all-help","title":"<code>bedboss geo upload-all --help</code>","text":"<pre><code> Usage: bedboss geo upload-all [OPTIONS]                                                \n\n Run bedboss uploading pipeline for specified genome in specified period of time.       \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bedbase-config                               TEXT     Path to bedbase config    \u2502\n\u2502                                                            file                      \u2502\n\u2502                                                            [default: None]           \u2502\n\u2502                                                            [required]                \u2502\n\u2502 *  --outfolder                                    TEXT     Path to output folder     \u2502\n\u2502                                                            [default: None]           \u2502\n\u2502                                                            [required]                \u2502\n\u2502    --start-date                                   TEXT     The earliest date when    \u2502\n\u2502                                                            opep was updated          \u2502\n\u2502                                                            [Default: 2000/01/01]     \u2502\n\u2502                                                            [default: None]           \u2502\n\u2502    --end-date                                     TEXT     The latest date when opep \u2502\n\u2502                                                            was updated [Default:     \u2502\n\u2502                                                            today's date]             \u2502\n\u2502                                                            [default: None]           \u2502\n\u2502    --search-limit                                 INTEGER  Limit of projects to be   \u2502\n\u2502                                                            searched. [Default: 50]   \u2502\n\u2502                                                            [default: 50]             \u2502\n\u2502    --search-offset                                INTEGER  Limit of projects to be   \u2502\n\u2502                                                            searched. [Default: 0]    \u2502\n\u2502                                                            [default: 0]              \u2502\n\u2502    --download-limit                               INTEGER  Limit of projects to be   \u2502\n\u2502                                                            downloaded [Default: 100] \u2502\n\u2502                                                            [default: 100]            \u2502\n\u2502    --genome                                       TEXT     Reference genome          \u2502\n\u2502                                                            [Default: None] (e.g.     \u2502\n\u2502                                                            hg38) - if None, all      \u2502\n\u2502                                                            genomes will be processed \u2502\n\u2502                                                            [default: None]           \u2502\n\u2502    --preload             --no-preload                      Download bedfile before   \u2502\n\u2502                                                            caching it. [Default:     \u2502\n\u2502                                                            True]                     \u2502\n\u2502                                                            [default: preload]        \u2502\n\u2502    --create-bedset       --no-create-bedset                Create bedset from bed    \u2502\n\u2502                                                            files. [Default: True]    \u2502\n\u2502                                                            [default: create-bedset]  \u2502\n\u2502    --overwrite           --no-overwrite                    Overwrite existing        \u2502\n\u2502                                                            bedfiles. [Default:       \u2502\n\u2502                                                            False]                    \u2502\n\u2502                                                            [default: no-overwrite]   \u2502\n\u2502    --overwrite-bedset    --no-overwrite-bedset             Overwrite existing        \u2502\n\u2502                                                            bedset. [Default: False]  \u2502\n\u2502                                                            [default:                 \u2502\n\u2502                                                            overwrite-bedset]         \u2502\n\u2502    --rerun               --no-rerun                        Re-run all the samples.   \u2502\n\u2502                                                            [Default: False]          \u2502\n\u2502                                                            [default: no-rerun]       \u2502\n\u2502    --run-skipped         --no-run-skipped                  Run skipped projects.     \u2502\n\u2502                                                            [Default: False]          \u2502\n\u2502                                                            [default: run-skipped]    \u2502\n\u2502    --run-failed          --no-run-failed                   Run failed projects.      \u2502\n\u2502                                                            [Default: False]          \u2502\n\u2502                                                            [default: run-failed]     \u2502\n\u2502    --standardize-pep     --no-standardize-pep              Standardize pep with      \u2502\n\u2502                                                            BEDMESS. [Default: False] \u2502\n\u2502                                                            [default:                 \u2502\n\u2502                                                            no-standardize-pep]       \u2502\n\u2502    --use-skipper         --no-use-skipper                  Use skipper to skip       \u2502\n\u2502                                                            projects if they were     \u2502\n\u2502                                                            processed locally         \u2502\n\u2502                                                            [Default: False]          \u2502\n\u2502                                                            [default: no-use-skipper] \u2502\n\u2502    --reinit-skipper      --no-reinit-skipper               Reinitialize skipper.     \u2502\n\u2502                                                            [Default: False]          \u2502\n\u2502                                                            [default:                 \u2502\n\u2502                                                            no-reinit-skipper]        \u2502\n\u2502    --lite                --no-lite                         Run the pipeline in lite  \u2502\n\u2502                                                            mode. [Default: False]    \u2502\n\u2502                                                            [default: no-lite]        \u2502\n\u2502    --help                                                  Show this message and     \u2502\n\u2502                                                            exit.                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-geo-upload-gse-help","title":"<code>bedboss geo upload-gse --help</code>","text":"<pre><code> Usage: bedboss geo upload-gse [OPTIONS]                                                \n\n Run bedboss uploading pipeline for GSE.                                                \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bedbase-config                               TEXT  Path to bedbase config file  \u2502\n\u2502                                                         [default: None]              \u2502\n\u2502                                                         [required]                   \u2502\n\u2502 *  --outfolder                                    TEXT  Path to output folder        \u2502\n\u2502                                                         [default: None]              \u2502\n\u2502                                                         [required]                   \u2502\n\u2502 *  --gse                                          TEXT  GSE number that can be found \u2502\n\u2502                                                         in pephub. eg. GSE123456     \u2502\n\u2502                                                         [default: None]              \u2502\n\u2502                                                         [required]                   \u2502\n\u2502    --create-bedset       --no-create-bedset             Create bedset from bed       \u2502\n\u2502                                                         files. [Default: True]       \u2502\n\u2502                                                         [default: create-bedset]     \u2502\n\u2502    --genome                                       TEXT  reference genome to upload   \u2502\n\u2502                                                         to database. If None, all    \u2502\n\u2502                                                         genomes will be processed    \u2502\n\u2502                                                         [default: None]              \u2502\n\u2502    --preload             --no-preload                   Download bedfile before      \u2502\n\u2502                                                         caching it. [Default: True]  \u2502\n\u2502                                                         [default: preload]           \u2502\n\u2502    --rerun               --no-rerun                     Re-run all the samples.      \u2502\n\u2502                                                         [Default: False]             \u2502\n\u2502                                                         [default: rerun]             \u2502\n\u2502    --run-skipped         --no-run-skipped               Run skipped projects.        \u2502\n\u2502                                                         [Default: False]             \u2502\n\u2502                                                         [default: run-skipped]       \u2502\n\u2502    --run-failed          --no-run-failed                Run failed projects.         \u2502\n\u2502                                                         [Default: False]             \u2502\n\u2502                                                         [default: run-failed]        \u2502\n\u2502    --overwrite           --no-overwrite                 Overwrite existing bedfiles. \u2502\n\u2502                                                         [Default: False]             \u2502\n\u2502                                                         [default: no-overwrite]      \u2502\n\u2502    --overwrite-bedset    --no-overwrite-bedset          Overwrite existing bedset.   \u2502\n\u2502                                                         [Default: False]             \u2502\n\u2502                                                         [default: overwrite-bedset]  \u2502\n\u2502    --standardize-pep     --no-standardize-pep           Standardize pep with         \u2502\n\u2502                                                         BEDMESS. [Default: False]    \u2502\n\u2502                                                         [default:                    \u2502\n\u2502                                                         no-standardize-pep]          \u2502\n\u2502    --use-skipper         --no-use-skipper               Use local skipper to skip    \u2502\n\u2502                                                         projects if they were        \u2502\n\u2502                                                         processed locally [Default:  \u2502\n\u2502                                                         False]                       \u2502\n\u2502                                                         [default: no-use-skipper]    \u2502\n\u2502    --reinit-skipper      --no-reinit-skipper            Reinitialize skipper.        \u2502\n\u2502                                                         [Default: False]             \u2502\n\u2502                                                         [default: no-reinit-skipper] \u2502\n\u2502    --lite                --no-lite                      Run the pipeline in lite     \u2502\n\u2502                                                         mode. [Default: False]       \u2502\n\u2502                                                         [default: no-lite]           \u2502\n\u2502    --help                                               Show this message and exit.  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-run-all-help","title":"<code>bedboss run-all --help</code>","text":"<pre><code> Usage: bedboss run-all [OPTIONS]                                                       \n\n Run all the bedboss pipeline for a single bed file                                     \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --input-file                                    TEXT  Path to the input file      \u2502\n\u2502                                                          [default: None]             \u2502\n\u2502                                                          [required]                  \u2502\n\u2502 *  --input-type                                    TEXT  Type of the input file.     \u2502\n\u2502                                                          Options are: bigwig,        \u2502\n\u2502                                                          bedgraph, bed, bigbed, wig  \u2502\n\u2502                                                          [default: None]             \u2502\n\u2502                                                          [required]                  \u2502\n\u2502 *  --outfolder                                     TEXT  Path to the output folder   \u2502\n\u2502                                                          [default: None]             \u2502\n\u2502                                                          [required]                  \u2502\n\u2502 *  --genome                                        TEXT  Genome name. Example:       \u2502\n\u2502                                                          'hg38'                      \u2502\n\u2502                                                          [default: None]             \u2502\n\u2502                                                          [required]                  \u2502\n\u2502 *  --bedbase-config                                TEXT  Path to the bedbase config  \u2502\n\u2502                                                          file                        \u2502\n\u2502                                                          [default: None]             \u2502\n\u2502                                                          [required]                  \u2502\n\u2502    --license-id                                    TEXT  License ID. If not provided \u2502\n\u2502                                                          for in PEPfor each bed      \u2502\n\u2502                                                          file, this license will be  \u2502\n\u2502                                                          used                        \u2502\n\u2502                                                          [default: DUO:0000042]      \u2502\n\u2502    --rfg-config                                    TEXT  Path to the rfg config file \u2502\n\u2502                                                          [default: None]             \u2502\n\u2502    --narrowpeak            --no-narrowpeak               Is the input file a         \u2502\n\u2502                                                          narrowpeak file?            \u2502\n\u2502                                                          [default: no-narrowpeak]    \u2502\n\u2502    --check-qc              --no-check-qc                 Check the quality of the    \u2502\n\u2502                                                          input file?                 \u2502\n\u2502                                                          [default: check-qc]         \u2502\n\u2502    --chrom-sizes                                   TEXT  Path to the chrom sizes     \u2502\n\u2502                                                          file                        \u2502\n\u2502                                                          [default: None]             \u2502\n\u2502    --open-signal-matrix                            TEXT  Path to the open signal     \u2502\n\u2502                                                          matrix file                 \u2502\n\u2502                                                          [default: None]             \u2502\n\u2502    --ensdb                                         TEXT  Path to the EnsDb database  \u2502\n\u2502                                                          file                        \u2502\n\u2502                                                          [default: None]             \u2502\n\u2502    --just-db-commit        --no-just-db-commit           Just commit to the          \u2502\n\u2502                                                          database?                   \u2502\n\u2502                                                          [default:                   \u2502\n\u2502                                                          no-just-db-commit]          \u2502\n\u2502    --force-overwrite       --no-force-overwrite          Force overwrite the output  \u2502\n\u2502                                                          files                       \u2502\n\u2502                                                          [default:                   \u2502\n\u2502                                                          no-force-overwrite]         \u2502\n\u2502    --update                --no-update                   Update the bedbase database \u2502\n\u2502                                                          with the new record if it   \u2502\n\u2502                                                          exists. This overwrites     \u2502\n\u2502                                                          'force_overwrite' option    \u2502\n\u2502                                                          [default: no-update]        \u2502\n\u2502    --lite                  --no-lite                     Run the pipeline in lite    \u2502\n\u2502                                                          mode. [Default: False]      \u2502\n\u2502                                                          [default: no-lite]          \u2502\n\u2502    --upload-qdrant         --no-upload-qdrant            Upload to Qdrant            \u2502\n\u2502                                                          [default: no-upload-qdrant] \u2502\n\u2502    --upload-s3             --no-upload-s3                Upload to S3                \u2502\n\u2502                                                          [default: no-upload-s3]     \u2502\n\u2502    --upload-pephub         --no-upload-pephub            Upload to PEPHub            \u2502\n\u2502                                                          [default: no-upload-pephub] \u2502\n\u2502    --universe              --no-universe                 Create a universe           \u2502\n\u2502                                                          [default: no-universe]      \u2502\n\u2502    --universe-method                               TEXT  Method used to create the   \u2502\n\u2502                                                          universe                    \u2502\n\u2502                                                          [default: None]             \u2502\n\u2502    --universe-bedset                               TEXT  Bedset used used to create  \u2502\n\u2502                                                          the universe                \u2502\n\u2502                                                          [default: None]             \u2502\n\u2502    --multi                 --no-multi                    Run multiple samples        \u2502\n\u2502                                                          [default: no-multi]         \u2502\n\u2502    --recover               --no-recover                  Recover from previous run   \u2502\n\u2502                                                          [default: recover]          \u2502\n\u2502    --dirty                 --no-dirty                    Run without removing        \u2502\n\u2502                                                          existing files              \u2502\n\u2502                                                          [default: no-dirty]         \u2502\n\u2502    --help                                                Show this message and exit. \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-run-pep-help","title":"<code>bedboss run-pep --help</code>","text":"<pre><code> Usage: bedboss run-pep [OPTIONS]                                                       \n\n Run the all bedboss pipeline for a bed files in a PEP                                  \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --pep                                        TEXT  PEP file. Local or remote path \u2502\n\u2502                                                       [default: None]                \u2502\n\u2502                                                       [required]                     \u2502\n\u2502 *  --outfolder                                  TEXT  Path to the output folder      \u2502\n\u2502                                                       [default: None]                \u2502\n\u2502                                                       [required]                     \u2502\n\u2502 *  --bedbase-config                             TEXT  Path to the bedbase config     \u2502\n\u2502                                                       file                           \u2502\n\u2502                                                       [default: None]                \u2502\n\u2502                                                       [required]                     \u2502\n\u2502    --create-bedset      --no-create-bedset            Create a new bedset            \u2502\n\u2502                                                       [default: create-bedset]       \u2502\n\u2502    --bedset-heavy       --no-bedset-heavy             Run the heavy version of the   \u2502\n\u2502                                                       bedbuncher pipeline            \u2502\n\u2502                                                       [default: no-bedset-heavy]     \u2502\n\u2502    --bedset-id                                  TEXT  Bedset ID [default: None]      \u2502\n\u2502    --rfg-config                                 TEXT  Path to the rfg config file    \u2502\n\u2502                                                       [default: None]                \u2502\n\u2502    --check-qc           --no-check-qc                 Check the quality of the input \u2502\n\u2502                                                       file?                          \u2502\n\u2502                                                       [default: check-qc]            \u2502\n\u2502    --ensdb                                      TEXT  Path to the EnsDb database     \u2502\n\u2502                                                       file                           \u2502\n\u2502                                                       [default: None]                \u2502\n\u2502    --just-db-commit     --no-just-db-commit           Just commit to the database?   \u2502\n\u2502                                                       [default: no-just-db-commit]   \u2502\n\u2502    --force-overwrite    --no-force-overwrite          Force overwrite the output     \u2502\n\u2502                                                       files                          \u2502\n\u2502                                                       [default: no-force-overwrite]  \u2502\n\u2502    --update             --no-update                   Update the bedbase database    \u2502\n\u2502                                                       with the new record if it      \u2502\n\u2502                                                       exists. This overwrites        \u2502\n\u2502                                                       'force_overwrite' option       \u2502\n\u2502                                                       [default: no-update]           \u2502\n\u2502    --upload-qdrant      --no-upload-qdrant            Upload to Qdrant               \u2502\n\u2502                                                       [default: upload-qdrant]       \u2502\n\u2502    --upload-s3          --no-upload-s3                Upload to S3                   \u2502\n\u2502                                                       [default: upload-s3]           \u2502\n\u2502    --upload-pephub      --no-upload-pephub            Upload to PEPHub               \u2502\n\u2502                                                       [default: upload-pephub]       \u2502\n\u2502    --no-fail            --no-no-fail                  Do not fail on error           \u2502\n\u2502                                                       [default: no-no-fail]          \u2502\n\u2502    --license-id                                 TEXT  License ID                     \u2502\n\u2502                                                       [default: DUO:0000042]         \u2502\n\u2502    --standardize-pep    --no-standardize-pep          Standardize the PEP using      \u2502\n\u2502                                                       bedMS                          \u2502\n\u2502                                                       [default: no-standardize-pep]  \u2502\n\u2502    --lite               --no-lite                     Run the pipeline in lite mode. \u2502\n\u2502                                                       [Default: False]               \u2502\n\u2502                                                       [default: no-lite]             \u2502\n\u2502    --rerun              --no-rerun                    Rerun already processed        \u2502\n\u2502                                                       samples                        \u2502\n\u2502                                                       [default: no-rerun]            \u2502\n\u2502    --multi              --no-multi                    Run multiple samples           \u2502\n\u2502                                                       [default: no-multi]            \u2502\n\u2502    --recover            --no-recover                  Recover from previous run      \u2502\n\u2502                                                       [default: recover]             \u2502\n\u2502    --dirty              --no-dirty                    Run without removing existing  \u2502\n\u2502                                                       files                          \u2502\n\u2502                                                       [default: no-dirty]            \u2502\n\u2502    --help                                             Show this message and exit.    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-reprocess-all-help","title":"<code>bedboss reprocess-all --help</code>","text":"<pre><code> Usage: bedboss reprocess-all [OPTIONS]                                                 \n\n Run unprocessed files, or reprocess them                                               \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bedbase-config                    TEXT     Path to the bedbase config file      \u2502\n\u2502                                                 [default: None]                      \u2502\n\u2502                                                 [required]                           \u2502\n\u2502 *  --outfolder                         TEXT     Path to the output folder            \u2502\n\u2502                                                 [default: None]                      \u2502\n\u2502                                                 [required]                           \u2502\n\u2502    --limit                             INTEGER  Limit the number of files to         \u2502\n\u2502                                                 reprocess                            \u2502\n\u2502                                                 [default: 100]                       \u2502\n\u2502    --no-fail           --no-no-fail             Do not fail on error                 \u2502\n\u2502                                                 [default: no-fail]                   \u2502\n\u2502    --help                                       Show this message and exit.          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-reprocess-one-help","title":"<code>bedboss reprocess-one --help</code>","text":"<pre><code> Usage: bedboss reprocess-one [OPTIONS]                                                 \n\n Run unprocessed file, or reprocess it [Only 1 file]                                    \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bedbase-config        TEXT  Path to the bedbase config file [default: None]     \u2502\n\u2502                                  [required]                                          \u2502\n\u2502 *  --outfolder             TEXT  Path to the output folder [default: None]           \u2502\n\u2502                                  [required]                                          \u2502\n\u2502 *  --identifier            TEXT  Identifier of the bed file [default: None]          \u2502\n\u2502                                  [required]                                          \u2502\n\u2502    --help                        Show this message and exit.                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-reprocess-bedset-help","title":"<code>bedboss reprocess-bedset --help</code>","text":"<pre><code> Usage: bedboss reprocess-bedset [OPTIONS]                                              \n\n Reprocess a bedset                                                                     \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bedbase-config                    TEXT  Path to the bedbase config file         \u2502\n\u2502                                              [default: None]                         \u2502\n\u2502                                              [required]                              \u2502\n\u2502 *  --outfolder                         TEXT  Path to the output folder               \u2502\n\u2502                                              [default: None]                         \u2502\n\u2502                                              [required]                              \u2502\n\u2502 *  --identifier                        TEXT  Bedset ID [default: None] [required]    \u2502\n\u2502    --no-fail           --no-no-fail          Do not fail on error [default: no-fail] \u2502\n\u2502    --heavy             --no-heavy            Run the heavy version of the pipeline   \u2502\n\u2502                                              [default: no-heavy]                     \u2502\n\u2502    --help                                    Show this message and exit.             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-make-bed-help","title":"<code>bedboss make-bed --help</code>","text":"<pre><code> Usage: bedboss make-bed [OPTIONS]                                                      \n\n Create a bed files form a [bigwig, bedgraph, bed, bigbed, wig] file                    \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --input-file                        TEXT  Path to the input file [default: None]  \u2502\n\u2502                                              [required]                              \u2502\n\u2502 *  --input-type                        TEXT  Type of the input file. Options are:    \u2502\n\u2502                                              bigwig, bedgraph, bed, bigbed, wig      \u2502\n\u2502                                              [default: None]                         \u2502\n\u2502                                              [required]                              \u2502\n\u2502 *  --outfolder                         TEXT  Path to the output folder               \u2502\n\u2502                                              [default: None]                         \u2502\n\u2502                                              [required]                              \u2502\n\u2502 *  --genome                            TEXT  Genome name. Example: 'hg38'            \u2502\n\u2502                                              [default: None]                         \u2502\n\u2502                                              [required]                              \u2502\n\u2502    --rfg-config                        TEXT  Path to the rfg config file             \u2502\n\u2502                                              [default: None]                         \u2502\n\u2502    --narrowpeak     --no-narrowpeak          Is the input file a narrowpeak file?    \u2502\n\u2502                                              [default: no-narrowpeak]                \u2502\n\u2502    --chrom-sizes                       TEXT  Path to the chrom sizes file            \u2502\n\u2502                                              [default: None]                         \u2502\n\u2502    --multi          --no-multi               Run multiple samples                    \u2502\n\u2502                                              [default: no-multi]                     \u2502\n\u2502    --recover        --no-recover             Recover from previous run               \u2502\n\u2502                                              [default: recover]                      \u2502\n\u2502    --dirty          --no-dirty               Run without removing existing files     \u2502\n\u2502                                              [default: no-dirty]                     \u2502\n\u2502    --help                                    Show this message and exit.             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-make-bigbed-help","title":"<code>bedboss make-bigbed --help</code>","text":"<pre><code> Usage: bedboss make-bigbed [OPTIONS]                                                   \n\n Create a bigbed files form a bed file                                                  \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bed-file                       TEXT  Path to the input file [default: None]     \u2502\n\u2502                                           [required]                                 \u2502\n\u2502 *  --bed-type                       TEXT  bed type to be used for bigBed file        \u2502\n\u2502                                           generation 'bed{bedtype}+{n}' [Default:    \u2502\n\u2502                                           None] (e.g bed3+1)                         \u2502\n\u2502                                           [default: None]                            \u2502\n\u2502                                           [required]                                 \u2502\n\u2502 *  --outfolder                      TEXT  Path to the output folder [default: None]  \u2502\n\u2502                                           [required]                                 \u2502\n\u2502 *  --genome                         TEXT  Genome name. Example: 'hg38'               \u2502\n\u2502                                           [default: None]                            \u2502\n\u2502                                           [required]                                 \u2502\n\u2502    --rfg-config                     TEXT  Path to the rfg config file                \u2502\n\u2502                                           [default: None]                            \u2502\n\u2502    --chrom-sizes                    TEXT  Path to the chrom sizes file               \u2502\n\u2502                                           [default: None]                            \u2502\n\u2502    --multi          --no-multi            Run multiple samples [default: no-multi]   \u2502\n\u2502    --recover        --no-recover          Recover from previous run                  \u2502\n\u2502                                           [default: recover]                         \u2502\n\u2502    --dirty          --no-dirty            Run without removing existing files        \u2502\n\u2502                                           [default: no-dirty]                        \u2502\n\u2502    --help                                 Show this message and exit.                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-run-qc-help","title":"<code>bedboss run-qc --help</code>","text":"<pre><code> Usage: bedboss run-qc [OPTIONS]                                                        \n\n Run the quality control for a bed file                                                 \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bed-file                             TEXT     Path to the bed file to check the \u2502\n\u2502                                                    quality control on.               \u2502\n\u2502                                                    [default: None]                   \u2502\n\u2502                                                    [required]                        \u2502\n\u2502 *  --outfolder                            TEXT     Path to the output folder         \u2502\n\u2502                                                    [default: None]                   \u2502\n\u2502                                                    [required]                        \u2502\n\u2502    --max-file-size                        INTEGER  Maximum file size threshold to    \u2502\n\u2502                                                    pass the quality                  \u2502\n\u2502                                                    [default: 2147483648]             \u2502\n\u2502    --max-region-number                    INTEGER  Maximum number of regions         \u2502\n\u2502                                                    threshold to pass the quality     \u2502\n\u2502                                                    [default: 5000000]                \u2502\n\u2502    --min-region-width                     INTEGER  Minimum region width threshold to \u2502\n\u2502                                                    pass the quality                  \u2502\n\u2502                                                    [default: 10]                     \u2502\n\u2502    --multi                --no-multi               Run multiple samples              \u2502\n\u2502                                                    [default: no-multi]               \u2502\n\u2502    --recover              --no-recover             Recover from previous run         \u2502\n\u2502                                                    [default: recover]                \u2502\n\u2502    --dirty                --no-dirty               Run without removing existing     \u2502\n\u2502                                                    files                             \u2502\n\u2502                                                    [default: no-dirty]               \u2502\n\u2502    --help                                          Show this message and exit.       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-run-stats-help","title":"<code>bedboss run-stats --help</code>","text":"<pre><code> Usage: bedboss run-stats [OPTIONS]                                                     \n\n Create the statistics for a single bed file.                                           \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bed-file                                     TEXT  Path to the bed file         \u2502\n\u2502                                                         [default: None]              \u2502\n\u2502                                                         [required]                   \u2502\n\u2502 *  --genome                                       TEXT  Genome name. Example: 'hg38' \u2502\n\u2502                                                         [default: None]              \u2502\n\u2502                                                         [required]                   \u2502\n\u2502 *  --outfolder                                    TEXT  Path to the output folder    \u2502\n\u2502                                                         [default: None]              \u2502\n\u2502                                                         [required]                   \u2502\n\u2502    --ensdb                                        TEXT  Path to the EnsDb database   \u2502\n\u2502                                                         file                         \u2502\n\u2502                                                         [default: None]              \u2502\n\u2502    --open-signal-matrix                           TEXT  Path to the open signal      \u2502\n\u2502                                                         matrix file                  \u2502\n\u2502                                                         [default: None]              \u2502\n\u2502    --just-db-commit        --no-just-db-commit          Just commit to the database? \u2502\n\u2502                                                         [default: no-just-db-commit] \u2502\n\u2502    --multi                 --no-multi                   Run multiple samples         \u2502\n\u2502                                                         [default: no-multi]          \u2502\n\u2502    --recover               --no-recover                 Recover from previous run    \u2502\n\u2502                                                         [default: recover]           \u2502\n\u2502    --dirty                 --no-dirty                   Run without removing         \u2502\n\u2502                                                         existing files               \u2502\n\u2502                                                         [default: no-dirty]          \u2502\n\u2502    --help                                               Show this message and exit.  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-reindex-help","title":"<code>bedboss reindex --help</code>","text":"<pre><code> Usage: bedboss reindex [OPTIONS]                                                       \n\n Reindex the bedbase database and insert all files to the qdrant database.              \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bedbase-config        TEXT  Path to the bedbase config file [default: None]     \u2502\n\u2502                                  [required]                                          \u2502\n\u2502    --help                        Show this message and exit.                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-make-bedset-help","title":"<code>bedboss make-bedset --help</code>","text":"<pre><code> Usage: bedboss make-bedset [OPTIONS]                                                   \n\n Create a bedset from a pep file, and insert it to the bedbase database.                \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --pep                                        TEXT  PEP file. Local or remote path \u2502\n\u2502                                                       [default: None]                \u2502\n\u2502                                                       [required]                     \u2502\n\u2502 *  --outfolder                                  TEXT  Path to the output folder      \u2502\n\u2502                                                       [default: None]                \u2502\n\u2502                                                       [required]                     \u2502\n\u2502 *  --bedbase-config                             TEXT  Path to the bedbase config     \u2502\n\u2502                                                       file                           \u2502\n\u2502                                                       [default: None]                \u2502\n\u2502                                                       [required]                     \u2502\n\u2502 *  --bedset-name                                TEXT  Name of the bedset             \u2502\n\u2502                                                       [default: None]                \u2502\n\u2502                                                       [required]                     \u2502\n\u2502    --heavy              --no-heavy                    Run the heavy version of the   \u2502\n\u2502                                                       pipeline                       \u2502\n\u2502                                                       [default: no-heavy]            \u2502\n\u2502    --force-overwrite    --no-force-overwrite          Force overwrite the output     \u2502\n\u2502                                                       files                          \u2502\n\u2502                                                       [default: no-force-overwrite]  \u2502\n\u2502    --upload-s3          --no-upload-s3                Upload to S3                   \u2502\n\u2502                                                       [default: no-upload-s3]        \u2502\n\u2502    --upload-pephub      --no-upload-pephub            Upload to PEPHub               \u2502\n\u2502                                                       [default: no-upload-pephub]    \u2502\n\u2502    --no-fail            --no-no-fail                  Do not fail on error           \u2502\n\u2502                                                       [default: no-no-fail]          \u2502\n\u2502    --help                                             Show this message and exit.    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-init-config-help","title":"<code>bedboss init-config --help</code>","text":"<pre><code> Usage: bedboss init-config [OPTIONS]                                                   \n\n Initialize the new, sample configuration file                                          \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --outfolder        TEXT  Path to the output folder [default: None] [required]     \u2502\n\u2502    --help                   Show this message and exit.                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-delete-bed-help","title":"<code>bedboss delete-bed --help</code>","text":"<pre><code> Usage: bedboss delete-bed [OPTIONS]                                                    \n\n Delete bed from the bedbase database                                                   \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --sample-id        TEXT  Sample ID [default: None] [required]                     \u2502\n\u2502 *  --config           TEXT  Path to the bedbase config file [default: None]          \u2502\n\u2502                             [required]                                               \u2502\n\u2502    --help                   Show this message and exit.                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-delete-bedset-help","title":"<code>bedboss delete-bedset --help</code>","text":"<pre><code> Usage: bedboss delete-bedset [OPTIONS]                                                 \n\n Delete BedSet from the bedbase database                                                \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --identifier        TEXT  BedSet ID [default: None] [required]                    \u2502\n\u2502 *  --config            TEXT  Path to the bedbase config file [default: None]         \u2502\n\u2502                              [required]                                              \u2502\n\u2502    --help                    Show this message and exit.                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-tokenize-bed-help","title":"<code>bedboss tokenize-bed --help</code>","text":"<pre><code> Usage: bedboss tokenize-bed [OPTIONS]                                                  \n\n Tokenize a bedfile                                                                     \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bed-id                              TEXT  Path to the bed file [default: None]  \u2502\n\u2502                                                [required]                            \u2502\n\u2502 *  --universe-id                         TEXT  Universe ID [default: None]           \u2502\n\u2502                                                [required]                            \u2502\n\u2502    --cache-folder                        TEXT  Path to the cache folder              \u2502\n\u2502                                                [default: None]                       \u2502\n\u2502    --add-to-db         --no-add-to-db          Add the tokenized bed file to the     \u2502\n\u2502                                                bedbase database                      \u2502\n\u2502                                                [default: no-add-to-db]               \u2502\n\u2502    --bedbase-config                      TEXT  Path to the bedbase config file       \u2502\n\u2502                                                [default: None]                       \u2502\n\u2502    --overwrite         --no-overwrite          Overwrite the existing tokenized bed  \u2502\n\u2502                                                file                                  \u2502\n\u2502                                                [default: no-overwrite]               \u2502\n\u2502    --help                                      Show this message and exit.           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-delete-tokenized-help","title":"<code>bedboss delete-tokenized --help</code>","text":"<pre><code> Usage: bedboss delete-tokenized [OPTIONS]                                              \n\n Delete tokenized bed file                                                              \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --universe-id        TEXT  Universe ID [default: None] [required]                 \u2502\n\u2502 *  --bed-id             TEXT  Bed ID [default: None] [required]                      \u2502\n\u2502    --config             TEXT  Path to the bedbase config file [default: None]        \u2502\n\u2502    --help                     Show this message and exit.                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-convert-universe-help","title":"<code>bedboss convert-universe --help</code>","text":"<pre><code> Usage: bedboss convert-universe [OPTIONS]                                              \n\n Convert bed file to universe                                                           \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bed-id        TEXT  Path to the bed file [default: None] [required]             \u2502\n\u2502 *  --config        TEXT  Path to the bedbase config file [default: None] [required]  \u2502\n\u2502    --method        TEXT  Method used to create the universe [default: None]          \u2502\n\u2502    --bedset        TEXT  Bedset used to create the universe [default: None]          \u2502\n\u2502    --help                Show this message and exit.                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-check-requirements-help","title":"<code>bedboss check-requirements --help</code>","text":"<pre><code> Usage: bedboss check-requirements [OPTIONS]                                            \n\n Check installed R packages                                                             \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-install-requirements-help","title":"<code>bedboss install-requirements --help</code>","text":"<pre><code> Usage: bedboss install-requirements [OPTIONS]                                          \n\n Install R dependencies                                                                 \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-verify-config-help","title":"<code>bedboss verify-config --help</code>","text":"<pre><code> Usage: bedboss verify-config [OPTIONS]                                                 \n\n Verify configuration file                                                              \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --config        TEXT  Path to the bedbase config file [default: None] [required]  \u2502\n\u2502    --help                Show this message and exit.                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-get-commands-help","title":"<code>bedboss get-commands --help</code>","text":"<pre><code> Usage: bedboss get-commands [OPTIONS]                                                  \n\n Get available commands                                                                 \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/how-to/bedfiles-geo-count/","title":"Plot total number of bed files in GEO database.","text":"<p>All GEO bed files are related to GSM (samples) and samples are related to GSE (series).</p> <p>All projects (series) are downloaded and served in PEPhub: https://pephub.databio.org/bedbase.  So the easiest way to count total number of bed files in  GEO is to count the number of samples under <code>bedbase</code> namespace. Important to note, that one sample can have multiple project, that's why we need to count distinct samples.</p> <p>To do so we can use sql query: <pre><code>select EXTRACT(YEAR FROM b.submission_date) AS year,\n    count(distinct a.sample_name) from samples as a join projects as b on a.project_id = b.id \n    where b.namespace='bedbase' \n    group by EXTRACT(YEAR FROM b.submission_date)\n    order by year;\n</code></pre></p> <p>It is a heavy query, that will take a while to run. (up to 3 minutes)</p> <p>This will give you the number of samples per year. e.g. <pre><code>year    count\n2007    47\n2008    197\n2009    2260\n2010    1608\n2011    5800\n2012    5796\n2013    5154\n2014    4145\n2015    3800\n2016    12242\n2017    12238\n2018    10876\n2019    19227\n2020    11886\n2021    24966\n2022    21280\n2023    17458\n2024    13348\n2025    2602\n</code></pre></p> <p>After you have a csv with the number of samples per year, you can use it to plot the number of samples per year.</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"geo2025_06.csv\")\n\ndata[\"summary\"] = data[\"count\"].cumsum()\ndata[\"year_str\"] = data[\"year\"].astype(str)\n\n\nfig, ax = plt.subplots()\nplt.xticks(rotation=45)\nax.bar(data[\"year_str\"], \n       data[\"summary\"], \n       label=data[\"year_str\"], \n       color=\"green\")\n\nax.set_xlabel('Year')\nax.set_ylabel('Number of files')\nax.set_title('Cumulative number of BED files')\n\n\nfig.savefig('./bed_geo_2025_06_summary.svg')\n</code></pre>"},{"location":"bedboss/how-to/bedfiles-geo-count/#the-output-plot","title":"The output plot:","text":""},{"location":"bedboss/how-to/how-to-create-database/","title":"How to create BEDbase database","text":"<p>To run bedboss and upload data to the database we need to create postgres database, or use existing one.</p>"},{"location":"bedboss/how-to/how-to-create-database/#to-create-local-database","title":"To create local database:","text":"<p>We are initiating postgres db in docker. If you don't have docker installed, you can install it with  <pre><code>sudo apt-get update &amp;&amp; apt-get install docker-engine -y\n</code></pre></p> <p>Now, create a persistent volume to house PostgreSQL data:</p> <pre><code>docker volume create postgres-data\n</code></pre> <pre><code>docker run -d --name bedbase-postgres -p 5432:5432 \\\n  -e POSTGRES_PASSWORD=bedbasepassword \\\n  -e POSTGRES_USER=postgres \\\n  -e POSTGRES_DB=postgres \\\n  -v postgres-data:/var/lib/postgresql/data \\\n  postgres:13\n</code></pre> <p>Now we have created docker and can run pipelines. To connect to the database, change your credentials in your configuration file.</p>"},{"location":"bedboss/how-to/how-to-create-umap/","title":"How to create UMAP embeddings from indexed BED files","text":"<p>To create UMAP embeddings from indexed BED files in the BEDbase database, you can use the <code>bedboss download-umap</code> command.  This command generates UMAP embeddings for visualizing the relationships between different BED files based on their genomic regions.</p>"},{"location":"bedboss/how-to/how-to-create-umap/#key-features-of-the-bedboss-download-umap-command","title":"Key features of the <code>bedboss download-umap</code> command:","text":"<ul> <li>User can specify the number of dimensions to return (2 or 3).</li> <li>If user selected 2 dimensions, script can generate a umap plot.</li> <li>If user choose to generate a plot, user can specify what metadata column to color the points by.</li> </ul> <p>Example</p> <p>Let's say we want to create a 2D UMAP embedding of the top 10 assays and top 10 cell lines from our BEDbase database,  and generate a plot colored by assay type. We can run the following command:</p> <pre><code>bedboss download-umap --config &lt;path_to_bedbase_config&gt;  \\\n    --output-file umap_embeddings.csv \\\n    --n-components 2 --plot-name umap_plot.png  \\\n    --plot-label assay --top-assays 10 --top-cell-lines 10\n</code></pre> Full bedboss download-umap help\" <pre><code> bedboss download-umap --help\n\n Usage: bedboss download-umap [OPTIONS]                                                                                                                                                                                             \n\n Download UMAP                                                                                                                                                                                                                      \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --config                TEXT     Path to the bedbase config file [default: None] [required]                                                                                                                                   \u2502\n\u2502 *  --output-file           TEXT     Path to the output file where UMAP embeddings will be saved [default: None] [required]                                                                                                       \u2502\n\u2502    --n-components          INTEGER  Number of UMAP components [default: 3]                                                                                                                                                       \u2502\n\u2502    --plot-name             TEXT     Name of the plot file [default: None]                                                                                                                                                        \u2502\n\u2502    --plot-label            TEXT     Label for the plot [default: None]                                                                                                                                                           \u2502\n\u2502    --top-assays            INTEGER  Number of top assays to include [default: 15]                                                                                                                                                \u2502\n\u2502    --top-cell-lines        INTEGER  Number of top cell lines to include [default: 15]                                                                                                                                            \u2502\n\u2502    --help                           Show this message and exit.                                                                                                                                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/how-to/how-to-install-requirements/","title":"How to install R dependencies","text":"<p>Before running any of the pipelines, you need to install the required R dependencies.</p> <p>To do so, you can run the following command: <pre><code>bedboss install-requirements\n</code></pre></p>"},{"location":"bedboss/how-to/how-to-install-requirements/#how-to-install-genomic-interval-region-conversion-tools","title":"How to install genomic interval region conversion tools:","text":"<ul> <li>bedToBigBed: http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/bedToBigBed</li> <li>bigBedToBed: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/bigBedToBed</li> <li>bigWigToBedGraph: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/bigWigToBedGraph</li> <li>wigToBigWig: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/wigToBigWig</li> </ul>"},{"location":"bedboss/how-to/how-to-reindex-db/","title":"How to Reindex the BEDbase Database","text":"<p>BEDbase uses two main vector collections:</p> <ul> <li>bed_collection: Stores all genomic regions from the BED files for the hg38 reference genome.</li> <li>text_collection: Stores text embeddings for all BED files based on their metadata.</li> </ul> <p>Information about whether a file is present in the vector database is stored together with its metadata in the relational database. This setup allows us to easily track which files are indexed in the vector database.</p> <p>To reindex the database, BEDboss provides two commands:</p> <ul> <li>\ud83d\udfe2 reindex: Reindex all the available hg38 files in qdrant database.                                                                                                                        \u2502</li> <li>\ud83d\udfe2 reindex-text: Reindex the semantic (text) search.</li> </ul> <p>Example</p> <p>Exapme of reindexing the entire database:</p> <pre><code>bedboss reindex --bedbase-config &lt;path_to_bedbase_config&gt; --purge\n</code></pre> <p>\u2757 Purge option will set all files in the database as not indexed, and reindex all files from the database. \u2757 If purge will be set to false, only files that are not indexed will be indexed.</p> Full bedboss reindex help <pre><code>bedboss reindex --help\n\n Usage: bedboss reindex [OPTIONS]                                                                                                                                                                                                   \n\n Reindex the bedbase database and insert all files to the qdrant database.                                                                                                                                                          \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bedbase-config                  TEXT     Path to the bedbase config file [default: None] [required]                                                                                                                         \u2502\n\u2502    --purge             --no-purge             Purge existing index before reindexing [default: no-purge]                                                                                                                         \u2502\n\u2502    --batch                           INTEGER  Number of items to upload in one batch [default: 1000]                                                                                                                             \u2502\n\u2502    --help                                     Show this message and exit.                                                                                                                                                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code> bedboss reindex-text --help\n\n Usage: bedboss reindex-text [OPTIONS]                                                                                                                                                                                              \n\n Reindex semantic (text) search.                                                                                                                                                                                                    \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bedbase-config                  TEXT     Path to the bedbase config file [default: None] [required]                                                                                                                         \u2502\n\u2502    --purge             --no-purge             Purge existing index before reindexing [default: no-purge]                                                                                                                         \u2502\n\u2502    --batch                           INTEGER  Number of items to upload in one batch [default: 1000]                                                                                                                             \u2502\n\u2502    --help                                     Show this message and exit.                                                                                                                                                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/how-to/how-to-run-in-docker/","title":"Running Bedboss in Docker","text":"<p>To run bedboss in docker, go through the following steps:</p> <ol> <li> <p>Clone the bedboss repository:</p> <pre><code>git clone git@github.com:databio/bedboss.git\n</code></pre> </li> <li> <p>Go to the bedboss directory.</p> </li> <li> <p>Build the docker image:</p> <pre><code>docker build -t my_bedboss .\n</code></pre> </li> <li> <p>source the environment variables:</p> <pre><code>source production/production.env\n</code></pre> </li> <li> <p>Run the docker container with setting limit of the files that have to be processed:</p> <pre><code>docker run --rm -it \\\n    --env POSTGRES_DB=$POSTGRES_DB \\\n    --env POSTGRES_HOST=$POSTGRES_HOST \\\n    --env POSTGRES_USER=$POSTGRES_USER \\\n    --env POSTGRES_PASSWORD=$POSTGRES_PASSWORD \\\n    --env QDRANT_API_KEY=$QDRANT_API_KEY \\\n    --env QDRANT_API_HOST=$QDRANT_API_HOST \\\n    --env AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\n    --env AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\n    --env AWS_ENDPOINT_URL=$AWS_ENDPOINT_URL \\\n    --env UPLOAD_LIMIT=1 \\\n    -t my_bedboss\n</code></pre> </li> </ol> <p>If everything is set up correctly, you should see the bedboss prompt.</p>"},{"location":"bedboss/how-to/how-to-run-pep/","title":"How to upload BED files using PEP","text":""},{"location":"bedboss/how-to/how-to-run-pep/#1-create-pep-with-bed-files-metadata","title":"1. Create PEP with BED files metadata","text":"<p>To upload BED files from PEP, first we should create a PEP project locally or on PEPhub.</p> <p>PEP must include this fields in sample table:</p> <ul> <li>\u2705 \"sample_name\"</li> <li>\u2705 \"input_file\"</li> <li>\u2705 \"input_type\"</li> <li>\u2705 \"genome\"</li> </ul> <p>\u2139\ufe0f All other fields are optional. Schema for PEP can be found here</p> <p>\u2139\ufe0f All peps before their upload are validated against the schema using <code>eido</code> tool.</p> <p>Example</p> <p>https://pephub.databio.org/khoroshevskyi/encode_batch_1</p>"},{"location":"bedboss/how-to/how-to-run-pep/#2-make-sure-all-paths-or-urls-to-input-bed-files-are-correct","title":"2. Make sure all paths or urls to input bed files are correct.","text":"<p>To complete this step, you should manually check that all paths or urls to input bed files are correct.</p>"},{"location":"bedboss/how-to/how-to-run-pep/#3-run-bedboss-upload-command","title":"3. Run bedboss upload command:","text":"<pre><code>bedboss run-pep --pep &lt;path_to_pep&gt; --outfolder &lt;path_to_output_folder&gt; --bedbase-config &lt;path_to_bedbase_config&gt;\n</code></pre> Full bedboss run-pep help <pre><code> bedboss run-pep --help\n\n Usage: bedboss run-pep [OPTIONS]                                                                                                                                                                                                   \n\n Run the all bedboss pipeline for a bed files in a PEP                                                                                                                                                                              \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --pep                                        TEXT  PEP file. Local or remote path [default: None] [required]                                                                                                                  \u2502\n\u2502 *  --outfolder                                  TEXT  Path to the output folder [default: None] [required]                                                                                                                       \u2502\n\u2502 *  --bedbase-config                             TEXT  Path to the bedbase config file [default: None] [required]                                                                                                                 \u2502\n\u2502    --create-bedset      --no-create-bedset            Create a new bedset [default: create-bedset]                                                                                                                               \u2502\n\u2502    --bedset-heavy       --no-bedset-heavy             Run the heavy version of the bedbuncher pipeline [default: no-bedset-heavy]                                                                                                \u2502\n\u2502    --bedset-id                                  TEXT  Bedset ID [default: None]                                                                                                                                                  \u2502\n\u2502    --rfg-config                                 TEXT  Path to the rfg config file [default: None]                                                                                                                                \u2502\n\u2502    --check-qc           --no-check-qc                 Check the quality of the input file? [default: check-qc]                                                                                                                   \u2502\n\u2502    --ensdb                                      TEXT  Path to the EnsDb database file [default: None]                                                                                                                            \u2502\n\u2502    --just-db-commit     --no-just-db-commit           Just commit to the database? [default: no-just-db-commit]                                                                                                                  \u2502\n\u2502    --force-overwrite    --no-force-overwrite          Force overwrite the output files [default: no-force-overwrite]                                                                                                             \u2502\n\u2502    --update             --no-update                   Update the bedbase database with the new record if it exists. This overwrites 'force_overwrite' option [default: no-update]                                                \u2502\n\u2502    --upload-qdrant      --no-upload-qdrant            Upload to Qdrant [default: upload-qdrant]                                                                                                                                  \u2502\n\u2502    --upload-s3          --no-upload-s3                Upload to S3 [default: upload-s3]                                                                                                                                          \u2502\n\u2502    --upload-pephub      --no-upload-pephub            Upload to PEPHub [default: upload-pephub]                                                                                                                                  \u2502\n\u2502    --no-fail            --no-no-fail                  Do not fail on error [default: no-no-fail]                                                                                                                                 \u2502\n\u2502    --license-id                                 TEXT  License ID [default: DUO:0000042]                                                                                                                                          \u2502\n\u2502    --standardize-pep    --no-standardize-pep          Standardize the PEP using bedMS [default: no-standardize-pep]                                                                                                              \u2502\n\u2502    --lite               --no-lite                     Run the pipeline in lite mode. [Default: False] [default: no-lite]                                                                                                         \u2502\n\u2502    --rerun              --no-rerun                    Rerun already processed samples [default: no-rerun]                                                                                                                        \u2502\n\u2502    --multi              --no-multi                    Run multiple samples [default: no-multi]                                                                                                                                   \u2502\n\u2502    --recover            --no-recover                  Recover from previous run [default: recover]                                                                                                                               \u2502\n\u2502    --dirty              --no-dirty                    Run without removing existing files [default: no-dirty]                                                                                                                    \u2502\n\u2502    --help                                             Show this message and exit.                                                                                                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/how-to/upload-geo-data/","title":"How to upload GEO data to bedbase","text":"<p>BEDboss provides geo submodule with functionality to upload GEO data to bedbase.</p> <pre><code>$ bedboss geo --help\n\n Usage: bedboss geo [OPTIONS] COMMAND [ARGS]...                                                                                                                                                                                     \n\n Automatic BEDbase uploader for GEO data                                                                                                                                                                                            \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version  -v        App version                                                                                                                                                                                                 \u2502\n\u2502 --help               Show this message and exit.                                                                                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 upload-all   Run bedboss uploading pipeline for specified genome in specified period of time.                                                                                                                                    \u2502\n\u2502 upload-gse   Run bedboss uploading pipeline for GSE.                                                                                                                                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>It has two main commands:</p> <ul> <li> <p>\ud83d\udfe2 upload-all: Runs the BEDboss uploading pipeline for a specified genome within a given time period.</p> </li> <li> <p>\ud83d\udfe2 upload-gse: Uploads or reprocesses a specific GSE accession.</p> </li> </ul>"},{"location":"bedboss/how-to/upload-geo-data/#how-does-it-work","title":"How does it work?","text":"<ul> <li>First, PEPhub automatically, every day (using a GitHub Actions cron job), uploads the metadata of GEO projects toEach PEP  corresponds to a GEO project (GSE) with all samples containing narrowPeak, broadPeak, or BED files.  These files are not quality-checked and may include incorrectly formatted files that users labeled as BED-like.</li> <li>Next, the GEO uploader retrieves all PEPs from PEPhub for a specific time period (e.g., the last month) or a specific GSE.<ol> <li><code>upload-all</code>: fetches all PEPs for the given time period.</li> <li><code>upload-gse</code>: fetches a specific GSE.</li> </ol> </li> </ul> <p>Then, for each sample in the PEP, the bedboss-all function processes the files.</p> <p>Info</p> <p>BEDboss upload-all supports two upload modes: - Full mode: Downloads, upload and processes file if it can. - Lite mode: Downloads and uploads file if it can, but does not process it.      This mode is useful if you want to upload a large number of files quickly, and process them later.</p> <p>For reprocessing files, there is a separate command: <code>bedboss  reprocess-all</code> and <code>bedboss reprocess-one</code>.</p> <ul> <li>When a GSM (sample) is processed, it is flagged in the database as processed, so it will not be processed again in the future.  In addition to the metadata from GEO, we also store the processing time and file digest in the database. The same applies to GSEs (projects): if a GSE or PEP in PEPhub has been processed and the reprocess flag is not set,  the project will not be processed again.</li> </ul>"},{"location":"bedboss/how-to/upload-geo-data/#full-cli-docs","title":"Full CLI docs:","text":"<p>bedboss geo upload-all --help <pre><code> Usage: bedboss geo upload-all [OPTIONS]                                                                              \n\n Run bedboss uploading pipeline for specified genome in specified period of time.                                     \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bedbase-config                               TEXT     Path to bedbase config file [default: None] [required]  \u2502\n\u2502 *  --outfolder                                    TEXT     Path to output folder [default: None] [required]        \u2502\n\u2502    --start-date                                   TEXT     The earliest date when opep was updated [Default:       \u2502\n\u2502                                                            2000/01/01]                                             \u2502\n\u2502                                                            [default: None]                                         \u2502\n\u2502    --end-date                                     TEXT     The latest date when opep was updated [Default: today's \u2502\n\u2502                                                            date]                                                   \u2502\n\u2502                                                            [default: None]                                         \u2502\n\u2502    --search-limit                                 INTEGER  Limit of projects to be searched. [Default: 50]         \u2502\n\u2502                                                            [default: 50]                                           \u2502\n\u2502    --search-offset                                INTEGER  Limit of projects to be searched. [Default: 0]          \u2502\n\u2502                                                            [default: 0]                                            \u2502\n\u2502    --download-limit                               INTEGER  Limit of projects to be downloaded [Default: 100]       \u2502\n\u2502                                                            [default: 100]                                          \u2502\n\u2502    --genome                                       TEXT     Reference genome [Default: None] (e.g. hg38) - if None, \u2502\n\u2502                                                            all genomes will be processed                           \u2502\n\u2502                                                            [default: None]                                         \u2502\n\u2502    --preload             --no-preload                      Download bedfile before caching it. [Default: True]     \u2502\n\u2502                                                            [default: preload]                                      \u2502\n\u2502    --create-bedset       --no-create-bedset                Create bedset from bed files. [Default: True]           \u2502\n\u2502                                                            [default: create-bedset]                                \u2502\n\u2502    --overwrite           --no-overwrite                    Overwrite existing bedfiles. [Default: False]           \u2502\n\u2502                                                            [default: no-overwrite]                                 \u2502\n\u2502    --overwrite-bedset    --no-overwrite-bedset             Overwrite existing bedset. [Default: False]             \u2502\n\u2502                                                            [default: overwrite-bedset]                             \u2502\n\u2502    --rerun               --no-rerun                        Re-run all the samples. [Default: False]                \u2502\n\u2502                                                            [default: no-rerun]                                     \u2502\n\u2502    --run-skipped         --no-run-skipped                  Run skipped projects. [Default: False]                  \u2502\n\u2502                                                            [default: run-skipped]                                  \u2502\n\u2502    --run-failed          --no-run-failed                   Run failed projects. [Default: False]                   \u2502\n\u2502                                                            [default: run-failed]                                   \u2502\n\u2502    --standardize-pep     --no-standardize-pep              Standardize pep with BEDMESS. [Default: False]          \u2502\n\u2502                                                            [default: no-standardize-pep]                           \u2502\n\u2502    --use-skipper         --no-use-skipper                  Use skipper to skip projects if they were processed     \u2502\n\u2502                                                            locally [Default: False]                                \u2502\n\u2502                                                            [default: no-use-skipper]                               \u2502\n\u2502    --reinit-skipper      --no-reinit-skipper               Reinitialize skipper. [Default: False]                  \u2502\n\u2502                                                            [default: no-reinit-skipper]                            \u2502\n\u2502    --lite                --no-lite                         Run the pipeline in lite mode. [Default: False]         \u2502\n\u2502                                                            [default: no-lite]                                      \u2502\n\u2502    --help                                                  Show this message and exit.                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p> <p>bedboss geo upload-gse --help <pre><code> Usage: bedboss geo upload-gse [OPTIONS]                                                                              \n\n Run bedboss uploading pipeline for GSE.                                                                              \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bedbase-config                               TEXT  Path to bedbase config file [default: None] [required]     \u2502\n\u2502 *  --outfolder                                    TEXT  Path to output folder [default: None] [required]           \u2502\n\u2502 *  --gse                                          TEXT  GSE number that can be found in pephub. eg. GSE123456      \u2502\n\u2502                                                         [default: None]                                            \u2502\n\u2502                                                         [required]                                                 \u2502\n\u2502    --create-bedset       --no-create-bedset             Create bedset from bed files. [Default: True]              \u2502\n\u2502                                                         [default: create-bedset]                                   \u2502\n\u2502    --genome                                       TEXT  reference genome to upload to database. If None, all       \u2502\n\u2502                                                         genomes will be processed                                  \u2502\n\u2502                                                         [default: None]                                            \u2502\n\u2502    --preload             --no-preload                   Download bedfile before caching it. [Default: True]        \u2502\n\u2502                                                         [default: preload]                                         \u2502\n\u2502    --rerun               --no-rerun                     Re-run all the samples. [Default: False] [default: rerun]  \u2502\n\u2502    --run-skipped         --no-run-skipped               Run skipped projects. [Default: False]                     \u2502\n\u2502                                                         [default: run-skipped]                                     \u2502\n\u2502    --run-failed          --no-run-failed                Run failed projects. [Default: False]                      \u2502\n\u2502                                                         [default: run-failed]                                      \u2502\n\u2502    --overwrite           --no-overwrite                 Overwrite existing bedfiles. [Default: False]              \u2502\n\u2502                                                         [default: no-overwrite]                                    \u2502\n\u2502    --overwrite-bedset    --no-overwrite-bedset          Overwrite existing bedset. [Default: False]                \u2502\n\u2502                                                         [default: overwrite-bedset]                                \u2502\n\u2502    --standardize-pep     --no-standardize-pep           Standardize pep with BEDMESS. [Default: False]             \u2502\n\u2502                                                         [default: no-standardize-pep]                              \u2502\n\u2502    --use-skipper         --no-use-skipper               Use local skipper to skip projects if they were processed  \u2502\n\u2502                                                         locally [Default: False]                                   \u2502\n\u2502                                                         [default: no-use-skipper]                                  \u2502\n\u2502    --reinit-skipper      --no-reinit-skipper            Reinitialize skipper. [Default: False]                     \u2502\n\u2502                                                         [default: no-reinit-skipper]                               \u2502\n\u2502    --lite                --no-lite                      Run the pipeline in lite mode. [Default: False]            \u2502\n\u2502                                                         [default: no-lite]                                         \u2502\n\u2502    --help                                               Show this message and exit.                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p>"},{"location":"bedboss/tutorials/bedms_tutorial/","title":"BEDMS tutorial","text":""},{"location":"bedboss/tutorials/bedms_tutorial/#bedms","title":"BEDMS","text":"<p>BEDMS (BED Metadata Standardizer) is a tool designed to standardize genomics and epigenomics metadata attributes according to user-selected or user-trained schemas. BEDMS ensures consistency and FAIRness of metadata across different platforms.  Users can interact with BEDMS either through Python or via PEPhub choosing from predefined schemas provided by the tool. Additionally, BEDMS allows users to create and train custom schemas as per their project requirements. For detailed information on the available schemas, please visit HuggingFace. </p>"},{"location":"bedboss/tutorials/bedms_tutorial/#installation","title":"Installation","text":"<p>To install bedms use this command:</p> <pre><code>pip install bedms\n</code></pre> <p>or install the latest version from the GitHub repository:</p> <pre><code>pip install git+https://github.com/databio/bedms.git\n</code></pre>"},{"location":"bedboss/tutorials/bedms_tutorial/#usage","title":"Usage","text":"<p>BEDMS can be used to standardize metadata attributes based on available schemas, train models on custom schemas, and standardize attributes based on the custom schema models.</p>"},{"location":"bedboss/tutorials/bedms_tutorial/#standardizing-based-on-available-schemas","title":"Standardizing based on available schemas","text":"<p>If you want to standardize the attributes in your PEP based on our available schemas, you can either visit PEPhub or using Python:</p> <p><pre><code>from bedms import AttrStandardizer\n\nmodel = AttrStandardizer(\n    repo_id=\"databio/attribute-standardizer-model6\", model_name=\"encode\"\n)\nresults = model.standardize(pep=\"geo/gse228634:default\")\n\nprint(results) #Dictionary of suggested predictions with their confidence: {'attr_1':{'prediction_1': 0.70, 'prediction_2':0.30}}\n</code></pre> In the above example, we have provided the <code>repo_id</code> which is the path to the repository that holds the models on HuggingFace. The <code>model_name</code> selection can vary based on your choice of schema. You can view the schemas on PEPhub for encode, fairtracks, and bedbase. For standardization, you need to provide the path to your PEP which in the above example is <code>pep=\"geo/gse228634:default\"</code>.</p>"},{"location":"bedboss/tutorials/bedms_tutorial/#training-custom-schemas","title":"Training custom schemas","text":"<p>If you want to train your custom schema-based models, you would need two things to get started:     1. Training sets      2. HuggingFace model and associated files</p>"},{"location":"bedboss/tutorials/bedms_tutorial/#training-sets","title":"Training sets","text":"<p>To develop training sets, follow the step by step protocol mentioned below:   </p> <ol> <li> <p>Select what attributes would be most suitable for your project metadata. For example, here are some attributes that you might choose: <pre><code>sample_name: Name of the sample_name\nassembly: Genome assembly (e.g. hg38)\nspecies_name: Scientific name of the species \n</code></pre></p> </li> <li> <p>Fetch training data from ontologies, publications and other available surces to make two directories: <code>values_directory</code> and <code>headers_directory</code>. <code>values_directory</code> has all the values associated with that attribute while the <code>headers_directory</code> has various synonyms for the attribute names.      The directory structure would look like this:     <pre><code>values_directory/\n    values_1.csv\n    values_2.csv\n    values_3.csv\n    .\n    .\n    values_1000.csv\n\nheaders_directory/\n    headers_1.csv\n    headers_2.csv\n    headers_3.csv\n    .\n    .\n    values_1000.csv\n</code></pre>         To see an example of what a <code>values_*.csv</code> and <code>headers_*.csv</code> might look like, you can check our sample csv files on PEPhub: sample_bedms_values_1.csv and sample_bedms-headers_1.csv.     While these are only samples and are not information dense, we recommend having large vocabulary for the training files for both the <code>values_directory</code> and <code>headers_directory</code>. To get a better understanding of the training data that we trained BEDMS on, you can visit this link</p> </li> <li> <p>Once your training sets are ready, you can make a directory for your schema in your HuggingFace repository. If the name of your schema is <code>new_schema</code> and the name of your repository is <code>new_repo</code>, this is what the directory structure will look like:     <pre><code>new_repo/\n    new_schema/\n        new_schema_design.yaml #This has the schema design defining the attributes with their data types and descriptions\n</code></pre></p> </li> <li> <p>You can now start training your model using the <code>AttrStandardizerTrainer</code> module. For this, you would need a <code>training_config.yaml</code>. Please follow the config file schema given here.</p> <p>To instantiate <code>AttrStandardizerTrainer</code> class:</p> <p><pre><code>from bedms.train import TrainStandardizer\n\ntrainer = TrainStandardizer(\"training_config.yaml\")\n</code></pre> To load the datasets and encode them:</p> <pre><code>train_data, val_data, test_data, label_encoder, vectorizer = trainer.load_data()\n</code></pre> <p>To train the custom model:</p> <pre><code>trainer.train()\n</code></pre> <p>To test the custom model:</p> <pre><code>test_results_dict = trainer.test() #Dictionary with Precision, Recall, and F1 values\n</code></pre> <p>To generate visualizations such as Learning Curves, Confusion Matrices, and ROC Curve:</p> <p><pre><code>acc_fig, loss_fig, conf_fig, roc_fig = trainer.plot_visualizations() \n</code></pre> Where <code>acc_fig</code> is Accuracy Curve figure object, <code>loss_fig</code> is Loss Curve figure object, <code>conf_fig</code> is the Confusion Matrix figure object, and <code>roc_fig</code> is the ROC Curve figure object. </p> </li> <li> <p>After your model is trained, you will have three files for it (paths to which you mentioned in the <code>training_config.yaml</code>):         i. <code>model_pth</code> : Path to your model. Let us assume it is named <code>model_new_schema.pth</code>.         ii. <code>label_encoder_pth</code>: Path to the Label Encoder. Let us assume it is named <code>label_encoder_new_schema.pkl</code>.         iii. <code>vectorizer_pth</code>: Path to the Vectorizer. Let us assume it is named <code>vectorizer_new_schema.pkl</code>.     Upload these files to your HuggingFace repository in the directory you had made earlier <code>new_repo/new_schema</code>.     Now, your HuggingFace repository would look something like this:</p> <pre><code>new_repo/\n    new_schema/\n        new_schema_design.yaml #This has the schema design defining the attributes with their data types and descriptions\n        model_new_schema.pth \n        label_encoder_new_schema.pkl\n        vectorizer.pkl\n</code></pre> </li> <li> <p>You're just one step away from standardizing metadata according to your custom schema. You would need to add a config file with the parameters you trained your model on to the <code>new_schema/</code> directory. Name this config file as <code>config_new_schema.yaml</code>. The config file should have the following keys:     <pre><code>params:\ninput_size_bow: int\nembedding_size: int\nhidden_size: int\noutput_size: int\ndropout_prob: float\n</code></pre>     Provide the values that you trained your model on. Now, the completely trained repository should have the following structure:</p> <p><pre><code>new_repo/\n    new_schema/\n        new_schema_design.yaml #This has the schema design defining the attributes with their data types and descriptions\n        model_new_schema.pth \n        label_encoder_new_schema.pkl\n        vectorizer.pkl\n        config_new_schema.yaml\n</code></pre> Before moving on to standardization, confirm that all the above files are present in your repository.</p> </li> </ol>"},{"location":"bedboss/tutorials/bedms_tutorial/#standardizing-on-custom-schema-models","title":"Standardizing on custom schema models","text":"<p>For standardizing on custom schema model, instantiate <code>AttrStandardizer</code> and provide the repo_id:</p> <pre><code>from bedms import AttrStandardizer\n\nmodel = AttrStandardizer(\n    repo_id=\"new_repo\", model_name=\"new_schema\"\n)\nresults = model.standardize(pep=\"geo/gse228634:default\")\n\nprint(results) #Dictionary of suggested predictions with their confidence: {'attr_1':{'prediction_1': 0.70, 'prediction_2':0.30}}\n</code></pre>"},{"location":"bedboss/tutorials/cli/","title":"BEDboss cli","text":"<p>To get information about the BEDboss command line interface, please refer to the \ud83d\udcd1 CLI usage  documentation.</p>"},{"location":"bedboss/tutorials/python/bedbuncher_tutorial/","title":"BEDbuncher","text":""},{"location":"bedboss/tutorials/python/bedbuncher_tutorial/#bedbuncher","title":"BEDbuncher","text":"<p>Bedbuncher is used to create bedset of bed files in the bedbase database.</p>"},{"location":"bedboss/tutorials/python/bedbuncher_tutorial/#1-create-bedbase-config-file","title":"1) Create bedbase config file","text":"<p>How to create config file: configuration section.</p>"},{"location":"bedboss/tutorials/python/bedbuncher_tutorial/#2-create-pep-with-bed-file-record-identifiers","title":"2) Create pep with bed file record identifiers.","text":"<p>To do so, you need to create a PEP with the following fields: sample_name (where sample_name is record_identifier), or <code>sample_name</code> + <code>record_identifier</code> e.g. sample_table:</p> sample_name record_identifier sample1 asdf3215f34 sample2 a23452f34tf"},{"location":"bedboss/tutorials/python/bedbuncher_tutorial/#3-run-bedboss-bunch","title":"3) Run bedboss bunch","text":""},{"location":"bedboss/tutorials/python/bedbuncher_tutorial/#from-command-line","title":"From command line","text":"<pre><code>bedboss bunch \\\n  --bedbase-config path/to/bedbase_config.yaml \\\n  --bedset-name bedset1 \\\n  --pep path/to/pep.yaml \\\n  --outfolder path/to/output/dir \\\n  --heavy \\\n  --upload-pephub \\\n  --upload-s3 \n</code></pre>"},{"location":"bedboss/tutorials/python/bedbuncher_tutorial/#run-bedboss-bunch-from-within-python","title":"Run bedboss bunch from within Python","text":"<pre><code>from bedboss.bedbuncher.bedbuncher import run_bedbuncher_form_pep\n\nrun_bedbuncher_form_pep(\n    bedbase_config=bedbase_config,\n    bedset_pep=pep,\n    output_folder=outfolder,\n    bedset_name=bedset_name,\n    heavy=heavy,\n    upload_pephub=upload_pephub,\n    upload_s3=upload_s3,\n    no_fail=no_fail,\n    force_overwrite=force_overwrite,\n    )\n</code></pre>"},{"location":"bedboss/tutorials/python/bedclassifier_tutorial/","title":"BED classifier","text":"<p>BED classifier is a utility that allows you to classify BED files based on the number of columns and the types of data contained within those columns.</p>"},{"location":"bedboss/tutorials/python/bedclassifier_tutorial/#get_bed_classification","title":"<code>get_bed_classification</code>","text":"<p>The function, <code>get_bed_classification</code>, takes a path to bed-like file or a dataframe and returns a <code>BedClassification</code> object with the following attributes:</p> <pre><code>class BedClassification(BaseModel):\n    bed_compliance: str\n    data_format: DATA_FORMAT\n    compliant_columns: int\n    non_compliant_columns: int\n</code></pre> <p>where <code>DATA_FORMAT</code> is defined as:</p> <pre><code>class DATA_FORMAT(str, Enum):\n    UNKNOWN = \"unknown_data_format\"\n    UCSC_BED = \"ucsc_bed\"\n    BED_RS = \"bed_rs\"\n    BED_LIKE = \"bed_like\"\n    BED_LIKE_RS = \"bed_like_rs\"\n    ENCODE_NARROWPEAK = \"encode_narrowpeak\"\n    ENCODE_NARROWPEAK_RS = \"encode_narrowpeak_rs\"\n    ENCODE_BROADPEAK = \"encode_broadpeak\"\n    ENCODE_BROADPEAK_RS = \"encode_broadpeak_rs\"\n    ENCODE_GAPPEDPEAK = \"encode_gappedpeak\"\n    ENCODE_GAPPEDPEAK_RS = \"encode_gappedpeak_rs\"\n    ENCODE_RNA_ELEMENTS = \"encode_rna_elements\"\n    ENCODE_RNA_ELEMENTS_RS = \"encode_rna_elements_rs\"\n</code></pre> Note <p>To read documentation of DATA Formats visit this page: BEDBASE data formats</p>"},{"location":"bedboss/tutorials/python/bedclassifier_tutorial/#example-usage-of-the-bed-classifier","title":"Example usage of the BED classifier:","text":"<pre><code>from bedboss.bedclassifier import get_bed_classification\n\nclasif = get_bed_classification(\"ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM8208nnn/GSM8208095/suppl/GSM8208095_Day4_WT-1_aligned_reads_peaks.narrowPeak.gz\")\n\nprint(clasif)\n\n# &gt;&gt; bed_compliance='bed6+4' \n# &gt;&gt; data_format=&lt;DATA_FORMAT.ENCODE_NARROWPEAK_RS: 'encode_narrowpeak_rs'&gt;\n# &gt;&gt; compliant_columns=6\n# &gt;&gt; non_compliant_columns=4\n</code></pre>"},{"location":"bedboss/tutorials/python/bedindex_tutorial/","title":"BEDindex","text":""},{"location":"bedboss/tutorials/python/bedindex_tutorial/#indexing-to-qdrant-database","title":"Indexing to qdrant database","text":""},{"location":"bedboss/tutorials/python/bedindex_tutorial/#1-create-bedbase-config-file","title":"1. Create bedbase config file","text":"<p>How to create a BEDbase configuration file is described in the configuration section.</p>"},{"location":"bedboss/tutorials/python/bedindex_tutorial/#2-run-bedboss-index","title":"2. Run bedboss index","text":""},{"location":"bedboss/tutorials/python/bedindex_tutorial/#from-command-line","title":"From command line","text":"<pre><code>bedboss reindex --bedbase-config path/to/bedbase_config.yaml\n</code></pre> <p>After running this command all files that are in the database and weren't indexed will be indexed to qdrant database.</p>"},{"location":"bedboss/tutorials/python/bedindex_tutorial/#from-within-python","title":"From within Python","text":"<pre><code>from bedboss.qdrant_index.qdrant_index import add_to_qdrant\n\nadd_to_qdrant(config=bedbase_config)\n</code></pre>"},{"location":"bedboss/tutorials/python/bedmaker_tutorial/","title":"BEDmaker","text":""},{"location":"bedboss/tutorials/python/bedmaker_tutorial/#bedmaker","title":"BEDmaker","text":"<p>The BEDmaker is a tool that allows you to convert various file types into BED format and bigBed format. Currently supported formats are: - bed  - bigBed - bigWig - wig</p> <p>Before running pipeline first, you have to install bedboss and check if bedmaker requirements are satisfied. To do so, you can run the next command: <pre><code>bedboss check-requirements\n</code></pre></p>"},{"location":"bedboss/tutorials/python/bedmaker_tutorial/#run-bedmaker-from-command-line","title":"Run BEDmaker from command line","text":"<pre><code>bedboss make-bed \\\n    --input-file path/to/input/file \\\n    --input-type bed\\\n    --outfolder path/to/output/dir \\\n    --genome hg38\n</code></pre>"},{"location":"bedboss/tutorials/python/bedmaker_tutorial/#run-bedmaker-from-within-python","title":"Run BEDmaker from within Python","text":"<pre><code>from bedboss.bedmaker.bedmaker import make_all\n\nmake_all(\n    input_file=\"path/to/input/file\",\n    input_type=\"bed\",\n    output_path=\"path/to/output/dir\",\n    genome=\"hg38\",\n)\n</code></pre>"},{"location":"bedboss/tutorials/python/bedstat_tutorial/","title":"BEDstats","text":"<p>BEDstats is a tool that calculates the statistics of a BED file and provides plots to visualize the results.</p> <p>It produces BED file Statistics:</p> <ul> <li>GC content.The average GC content of the region set. </li> <li>Number of regions. The total number of regions in the BED file. </li> <li>Median TSS distance. The median absolute distance to the Transcription Start Sites (TSS)</li> <li>Mean region width. The average region width of the region set.</li> <li>Exon percentage.  The percentage of the regions in the BED file that are annotated as exon. </li> <li>Intron percentage.    The percentage of the regions in the BED file that are annotated as intron.</li> <li>Promoter proc percentage. The percentage of the regions in the BED file that are annotated as promoter-prox.</li> <li>Intergenic percentage. The percentage of the regions in the BED file that are annotated as intergenic.</li> <li>Promoter core percentage. The percentage of the regions in the BED file that are annotated as promoter-core.</li> <li>5' UTR percentage. The percentage of the regions in the BED file that are annotated as 5'-UTR.</li> <li>3' UTR percentage. The percentage of the regions in the BED file that are annotated as 3'-UTR.</li> </ul>"},{"location":"bedboss/tutorials/python/bedstat_tutorial/#step-1-install-all-dependencies","title":"Step 1: Install all dependencies","text":"<p>First you have to install bedboss and check if all requirements are satisfied.  To do so, you can run next command: <pre><code>bedboss check-requirements\n</code></pre> If requirements are not satisfied, you will see the list of missing packages.</p>"},{"location":"bedboss/tutorials/python/bedstat_tutorial/#step-2-run-bedstats","title":"Step 2: Run bedstats","text":""},{"location":"bedboss/tutorials/python/bedstat_tutorial/#run-bedstats-from-command-line","title":"Run BEDstats from command line","text":"<pre><code>bedboss stats \\\n    --bedfile path/to/bedfile.bed \\\n    --outfolder path/to/output/dir \\\n    --genome hg38 \\\n</code></pre>"},{"location":"bedboss/tutorials/python/bedstat_tutorial/#run-bedstats-from-within-python","title":"Run BEDstats from within Python","text":"<pre><code>from bedboss import bedstats\n\nbedstat(\n    bedfile=\"path/to/bedfile.bed\",\n    outfolder=\"path/to/output/dir\",\n    genome=\"hg19\",\n    )\n</code></pre> <p>After running BEDstats, you will find the following files in the output directory + all statistics will be saved in output file.</p>"},{"location":"bedboss/tutorials/python/ref_genome_tutorial/","title":"Reference genome validator","text":"<p>Reference genome validator is a set of functions that allow to validate the reference genome of the bed files. Functions include different statistical analysis and comparison of the reference genome of the bed files that produce a compatibility rating.</p> <p>To run the reference genome validator, you need to have the input bed file and the chrom sizes files.</p> <p>Examples usage of the reference genome validator:</p>"},{"location":"bedboss/tutorials/python/ref_genome_tutorial/#example-1","title":"Example 1","text":"<p>Run validator with default parameters, with include default chrom sizes files.</p> <pre><code>from bedboss.refgenome_validator.main import ReferenceValidator\n\nvalidator_obj = ReferenceValidator()\n\nresults = validator_obj.determine_compatibility(\"path/to/bedfile.bed\")\n\nprint(results)\n# &gt;&gt;{'ensembl_hg38.chrom.sizes': CompatibilityStats(chrom_name_stats=ChromNameStats(xs=0.0, q_and_m=0.0, q_and_not_m=86.0, not_q_and_m=68.0, jaccard_index=0.0, jaccard_index_binary=0.0, passed_chrom_names=False), chrom_length_stats=ChromLengthStats(oobr=None, beyond_range=False, num_of_chrom_beyond=0, percentage_bed_chrom_beyond=0.0, percentage_genome_chrom_beyond=0.0), chrom_sequence_fit_stats=SequenceFitStats(sequence_fit=None), igd_stats=None, compatibility=RatingModel(assigned_points=10, tier_ranking=4)),\n# &gt;&gt; 'ncbi_hg38.chrom.sizes': CompatibilityStats(chrom_name_stats=ChromNameStats(xs=0.0, q_and_m=0.0, q_and_not_m=86.0, not_q_and_m=705.0, jaccard_index=0.0, jaccard_index_binary=0.0, passed_chrom_names=False), chrom_length_stats=ChromLengthStats(oobr=None, beyond_range=False, num_of_chrom_beyond=0, percentage_bed_chrom_beyond=0.0, percentage_genome_chrom_beyond=0.0), chrom_sequence_fit_stats=SequenceFitStats(sequence_fit=None), igd_stats=None, compatibility=RatingModel(assigned_points=10, tier_ranking=4)),\n</code></pre>"},{"location":"bedboss/tutorials/python/ref_genome_tutorial/#example-2","title":"Example 2","text":"<p>Run validator with custom reference genome chrom sizes files.</p> <pre><code>from bedboss.refgenome_validator.main import ReferenceValidator\n\n\nref_gen_list = [GenomeModel(\n    genome_alias=\"ensembl_hg38\",\n    chrom_sizes_path=\"path/to/ensembl_hg38.chrom.sizes\"\n)\n    ]\nvalidator_obj = ReferenceValidator(\n    genome_models=ref_gen_list,\n)\n\nresults = validator_obj.determine_compatibility(\"path/to/bedfile.bed\")\n\nprint(results)\n# &gt;&gt;{'ensembl_hg38': CompatibilityStats(chrom_name_stats=ChromNameStats(xs=0.0, q_and_m=0.0, q_and_not_m=86.0, not_q_and_m=68.0, jaccard_index=0.0, jaccard_index_binary=0.0, passed_chrom_names=False), chrom_length_stats=ChromLengthStats(oobr=None, beyond_range=False, num_of_chrom_beyond=0, percentage_bed_chrom_beyond=0.0, percentage_genome_chrom_beyond=0.0), chrom_sequence_fit_stats=SequenceFitStats(sequence_fit=None), igd_stats=None, compatibility=RatingModel(assigned_points=10, tier_ranking=4))}\n</code></pre>"},{"location":"bedboss/tutorials/python/ref_genome_tutorial/#example-3","title":"Example 3","text":"<p>Run genome predictor on a bed file with default reference genomes chrom sizes files.</p> <pre><code>from bedboss.refgenome_validator.main import ReferenceValidator\n\nvalidator_obj = ReferenceValidator()\n\nresults = validator_obj.predict(\"path/to/bedfile.bed\")\n\nprint(results)\n</code></pre>"},{"location":"bedboss/tutorials/python/tutorial_all/","title":"BEDboss-all pipeline","text":""},{"location":"bedboss/tutorials/python/tutorial_all/#bedboss-run-all","title":"Bedboss run-all","text":"<p>Bedboss run-all is intended to run on ONE sample (bed file) and run all bedboss pipelines:  bedmaker (+ bedclassifier + bedqc) -&gt; bedstat. After that optionally it can run bedbuncher, qdrant indexing and upload metadata to PEPhub.</p>"},{"location":"bedboss/tutorials/python/tutorial_all/#step-1-install-all-dependencies","title":"Step 1: Install all dependencies","text":"<p>First you have to install bedboss and check if all requirements are satisfied.  To do so, you can run next command: <pre><code>bedboss check-requirements\n</code></pre> If requirements are not satisfied, you will see the list of missing packages.</p>"},{"location":"bedboss/tutorials/python/tutorial_all/#step-2-create-bedconfyaml-file","title":"Step 2: Create bedconf.yaml file","text":"<p>To run bedboss, you need to create a bedconf.yaml file with configuration.  Detail instructions are in the configuration section.</p>"},{"location":"bedboss/tutorials/python/tutorial_all/#step-3-run-bedboss","title":"Step 3: Run bedboss","text":"<p>To run bedboss, you need to run the next command: <pre><code>bedboss all \\\n    --bedbase-config bedconf.yaml \\\n    --input-file path/to/bedfile.bed \\\n    --outfolder path/to/output/dir \\\n    --input-type bed \\\n    --genome hg38 \\\n</code></pre></p> <p>Above command will run bedboss on the bed file and create a bedstat file in the output directory. It contains only required parameters. For more details, please check the usage section.</p> <p>By default, results will be uploaded only to the PostgreSQL database.</p> <ul> <li>To upload results to PEPhub, you need to make the <code>databio</code> org available on GitHub, then login to PEPhub, and add the <code>--upload-pephub</code> flag to the command.</li> <li>To upload results to Qdrant, you need to add the <code>--upload-qdrant</code> flag to the command.</li> <li>To upload actual files to S3, you need to add the <code>--upload-s3</code> flag to the command, and before uploading, you have to set up all necessary environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_ENDPOINT_URL.</li> <li>To ignore errors and continue processing, you need to add the <code>--no-fail</code> flag to the command.</li> </ul>"},{"location":"bedboss/tutorials/python/tutorial_all/#run-bedboss-all-from-within-python","title":"Run bedboss all from within Python","text":"<p>To run bedboss all from within Python, instead of using the command line in the step #3, you can use the following code:</p> <pre><code>from bedboss import bedboss\n\nbedboss.run_all(\n    name=\"sample1\",\n    input_file=\"path/to/bedfile.bed\",\n    input_type=\"bed\",\n    outfolder=\"path/to/output/dir\",\n    genome=\"hg38\",\n    bedbase_config=\"bedconf.yaml\",\n    other_metadata=None, # optional\n    upload_pephub=True, # optional\n    upload_qdrant=True, # optional\n    upload_s3=True, # optional\n)\n</code></pre>"},{"location":"bedboss/tutorials/python/tutorial_run_pep/","title":"BEDboss run-pep","text":""},{"location":"bedboss/tutorials/python/tutorial_run_pep/#bedboss-run-pep","title":"Bedboss run-pep","text":"<p>Bedboss insert is designed to process each sample in the provided PEP.  The PEP can be provided either as a path to config file or as a registry path of the PEPhub.</p>"},{"location":"bedboss/tutorials/python/tutorial_run_pep/#step-1-install-all-dependencies","title":"Step 1: Install all dependencies","text":"<p>First, you have to install bedboss and check if all requirements are satisfied.  To do so, you can run the following command: <pre><code>bedboss check-requirements\n</code></pre> If requirements are not satisfied, you will see the list of missing packages.</p>"},{"location":"bedboss/tutorials/python/tutorial_run_pep/#step-2-create-bedconfyaml-file","title":"Step 2: Create bedconf.yaml file","text":"<p>To run bedboss run-pep, you need to create a bedconf.yaml file with configuration.  Detailed instructions are in the configuration section.</p>"},{"location":"bedboss/tutorials/python/tutorial_run_pep/#step-3-create-pep-with-bed-files","title":"Step 3: Create PEP with bed files.","text":"<p>BEDboss PEP should contain next fields: sample_name, input_file, input_type, genome. Before running bedboss, you need to validate provided PEP with bedboss_insert schema. The easiest way to do so is to use PEPhub, where you create a new PEP and validate it with the schema. Example PEP: https://pephub.databio.org/databio/excluderanges?tag=default</p>"},{"location":"bedboss/tutorials/python/tutorial_run_pep/#step-4-run-bedboss-insert","title":"Step 4: Run bedboss insert","text":"<p>To run bedboss insert , you need to run the next command: <pre><code>bedboss insert \\\n    --bedbase-config bedconf.yaml \\\n    --pep path/to/pep.yaml \\\n    --outfolder path/to/output/dir\n</code></pre></p> <p>Above command will run bedboss on the bed file and create a file with statistics in the output directory.  It contains only required parameters. For more details, please check the usage section.</p> <p>By default, results will be uploaded only to the PostgreSQL database.</p> <ul> <li>To upload results to PEPhub, you need to make the <code>databio</code> org available on GitHub, then login to PEPhub, and add the <code>--upload-pephub</code> flag to the command.</li> <li>To upload results to Qdrant, you need to add the <code>--upload-qdrant</code> flag to the command.</li> <li>To upload actual files to S3, you need to add the <code>--upload-s3</code> flag to the command, and before uploading, you have to set up all necessary environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_ENDPOINT_URL.</li> <li>To create a bedset of provided pep files, you need to add the <code>--create-bedset</code> flag to the command.</li> <li>To ignore errors and continue processing, you need to add the <code>--no-fail</code> flag to the command.</li> </ul>"},{"location":"bedboss/tutorials/python/tutorial_run_pep/#run-bedboss-insert-from-within-python","title":"Run bedboss insert from within Python","text":"<p>To run bedboss insert from within Python, instead of using the command line in the step #4, you can use the following code:</p> <pre><code>from bedboss import bedboss\n\nbedboss.insert_pep(\n    bedbase_config=\"bedconf.yaml\",\n    pep=\"path/to/pep.yaml\",\n    output_folder=\"path/to/output/dir\",\n    upload_pephub=True, # optional\n    upload_qdrant=True, # optional\n    upload_s3=True, # optional\n    create_bedset=True, # optional\n    no_fail=True, # optional\n)\n</code></pre>"},{"location":"bedhost/","title":"BEDhost overview","text":"bedhost <p><code>bedhost</code> is a Python FastAPI module for the API that powers BEDbase. It needs a path to the bedbase configuration file, which can be provided either via <code>-c</code>/<code>--config</code> argument or read from <code>$BEDBASE_CONFIG</code> environment variable. </p>"},{"location":"bedhost/#introduction","title":"Introduction","text":"<p>You can find the formal OpenAPI documentation and interactive interface at https://api.bedbase.org/v1/docs. This document provides more conceptual introduction and explanations to how to use the API effectively.</p>"},{"location":"bedhost/#general-api-organization","title":"General API organization","text":""},{"location":"bedhost/#data-types","title":"Data types","text":"<p>BEDbase stores two types of data, which we call records. They are 1. BEDs, and 2. BEDsets. BEDsets are simply collections of BEDs. Each record in the database is either a BED or a BEDset.</p>"},{"location":"bedhost/#endpoint-organization","title":"Endpoint organization","text":"<p>The endpoints are divided into 3 groups:</p> <ol> <li><code>/bed</code> endpoints are used to interact with metadata for BED records.</li> <li><code>/bedset</code> endpoints are used to interact with metadata for BEDset records.</li> <li><code>/objects</code> endpoints are used to download metadata and get URLs to retrieve the underlying data itself. These endpoints implement the GA4GH DRS standard.</li> </ol> <p>Therefore, to get information and statistics about BED or BEDset records, or what is contained in the database, look through the <code>/bed</code> and <code>/bedset</code> endpoints. But if you need to write a tool that gets the actual underlying files, then you'll need to use the <code>/objects</code> endpoints. The type of identifiers used in each case differ.</p>"},{"location":"bedhost/#record-identifiers-vs-object-identifiers","title":"Record identifiers vs. object identifiers","text":"<p>Each record has an identifier. For example, <code>eaf9ee97241f300f1c7e76e1f945141f</code> is a BED identifier. You can use this identifier for the metadata endpoints. To download files, you'll need something slightly different -- you need an object identifier. This is because each BED record includes multiple files, such as the original BED file, the BigBed file, analysis plots, and so on. To download a file, you will construct what we call the <code>object_id</code>, which identifies the specific file.</p>"},{"location":"bedhost/#how-to-construct-object-identifiers","title":"How to construct object identifiers","text":"<p>Object IDs take the form <code>&lt;record_type&gt;.&lt;record_identifier&gt;.&lt;result_id&gt;</code>. An example of an object_id for a BED file is <code>bed.eaf9ee97241f300f1c7e76e1f945141f.bedfile</code></p> <p>So, you can get information about this object like this:</p> <p><code>GET</code> https://api.bedbase.org/objects/bed.eaf9ee97241f300f1c7e76e1f945141f.bedfile</p> <p>Or, you can get a URL to download the actual file with:</p> <p><code>GET</code> https://api.bedbase.org/objects/bed.eaf9ee97241f300f1c7e76e1f945141f.bedfile/access/http</p>"},{"location":"bedhost/build_image/","title":"Build the container locally","text":"<p>Running with <code>uvicorn</code> provides auto-reload. To configure, this assumes you have previously set up <code>databio/secrets</code>. </p> <ol> <li>Source <code>.env</code> file to populate the environment variables referenced in the configuration file.</li> <li>Start <code>bedhost</code> using <code>uvicorn</code> and pass the configuration file via the <code>BEDBASE_CONFIG</code> env var.</li> </ol> <pre><code>source ../bedbase.org/environment/production.env\nBEDBASE_CONFIG=../bedbase.org/config/api.bedbase.org.yaml uvicorn bedhost.main:app --reload\n</code></pre> <p>You can change the database you're connecting to by using a different config file: - Using a local config: <code>BEDBASE_CONFIG=../bbconf/tests/data/config.yaml uvicorn bedhost.main:app --reload</code> - With new database: <code>BEDBASE_CONFIG=../bedbase.org/config/bedbase2.yaml uvicorn bedhost.main:app --reload</code></p> <p>Now, you can access the service at http://127.0.0.1:8000. Example endpoints: - http://127.0.0.1:8000/v1/bed/bbad85f21962bb8d972444f7f9a3a932/metadata?full=true - http://127.0.0.1:8000/v1/bed/bbad85f21962bb8d972444f7f9a3a932/metadata/plots?full=true - http://127.0.0.1:8000/v1/objects/bed.bbad85f21962bb8d972444f7f9a3a932.chrombins - http://127.0.0.1:8000/v1/bed/list?limit=10&amp;offset=0</p>"},{"location":"bedhost/build_image/#running-the-server-in-docker","title":"Running the server in Docker","text":""},{"location":"bedhost/build_image/#building-image","title":"Building image","text":"<ul> <li>Primary image: <code>docker build -t databio/bedhost -f .Dockerfile .</code></li> <li>Dev image <code>docker build -t databio/bedhost:dev -f dev.Dockerfile .</code></li> <li>Test image: <code>docker build -t databio/bedhost:dev -f test.Dockerfile .</code></li> </ul> <p>Existing images can be found at dockerhub.</p>"},{"location":"bedhost/build_image/#deploying-updates-automatically","title":"Deploying updates automatically","text":"<ul> <li>Deploying bedbase.</li> </ul>"},{"location":"bedhost/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format. </p>"},{"location":"bedhost/changelog/#0120-2025-12-01","title":"[0.12.0] -- 2025-12-01","text":""},{"location":"bedhost/changelog/#added","title":"Added:","text":"<ul> <li>umap calculation [API]</li> <li>BED-analyzer (qc), which is backed by gtars-wasm package [UI]</li> <li>New improved home page [UI]</li> <li>Added visualization of the UMAP, and interactive [UI]</li> </ul>"},{"location":"bedhost/changelog/#0110-2025-09-11","title":"[0.11.0] -- 2025-09-11","text":""},{"location":"bedhost/changelog/#added_1","title":"Added:","text":"<ul> <li>Added umap visualizations of bed embeddings</li> <li>Added bedbase-verse metrics</li> <li>Added filtering by assay and genome in the semantic search</li> <li>Added bed file qc check before running bed to bed search</li> </ul>"},{"location":"bedhost/changelog/#changed","title":"Changed:","text":"<ul> <li>Changed bivec search to text semantic search</li> <li>Multiple UI bug fix</li> </ul>"},{"location":"bedhost/changelog/#0100-2025-04-21","title":"[0.10.0] -- 2025-04-21","text":""},{"location":"bedhost/changelog/#added_2","title":"Added:","text":"<ul> <li>Added usage statistics</li> <li>Added Bed file statistic page</li> </ul>"},{"location":"bedhost/changelog/#changed_1","title":"Changed:","text":"<ul> <li>Updated bed compliance and data formats</li> </ul>"},{"location":"bedhost/changelog/#090-2025-01-03","title":"[0.9.0] -- 2025-01-03","text":""},{"location":"bedhost/changelog/#added_3","title":"Added:","text":"<ul> <li>Added a new class <code>CreateBEDsetRequest</code> in <code>bedhost/data_models.py</code> to handle BEDset creation requests.</li> <li>Introduced a new API endpoint <code>/v1/bed/{bed_id}/neighbours</code> to get nearest neighbors for a BED record in <code>bedhost/routers/bed_api.py</code>.</li> <li>Implemented a new API endpoint <code>/v1/bedset/create/</code> to create a new BEDset by providing a registry path to the PEPhub project in <code>bedhost/routers/bedset_api.py</code>.</li> </ul>"},{"location":"bedhost/changelog/#changed_2","title":"Changed:","text":"<ul> <li>Refactored <code>text_to_bed_search</code> function to include additional logic for handling specific queries in <code>bedhost/routers/bed_api.py</code>.</li> </ul>"},{"location":"bedhost/changelog/#ui-improvements","title":"UI improvements:","text":"<ul> <li>Added Most similar files table to bed page</li> <li>Improved Mobile friendly ui to both bed and bedset page</li> <li>Improved metadata tables on bed page</li> <li>Added <code>Download pdf</code> button for plots</li> <li>Improved search tables</li> <li>Added creation of bedset UI </li> </ul>"},{"location":"bedhost/changelog/#080-2024-11-07","title":"[0.8.0] -- 2024-11-07","text":""},{"location":"bedhost/changelog/#added_4","title":"Added:","text":"<ul> <li>Added endpoint showing available genomes</li> <li>Added endpoint listing bed_ids with missing plots</li> </ul>"},{"location":"bedhost/changelog/#070-2024-10-23","title":"[0.7.0] -- 2024-10-23","text":""},{"location":"bedhost/changelog/#added_5","title":"Added:","text":"<ul> <li>New text2bed search (bivec search)</li> <li>Added track_hub endpoints and pointing link</li> <li>Added pep generating endpoint for bedsets</li> </ul>"},{"location":"bedhost/changelog/#060-2024-10-15","title":"[0.6.0] -- 2024-10-15","text":""},{"location":"bedhost/changelog/#060-2024-10-15_1","title":"[0.6.0] -- 2024-10-15","text":""},{"location":"bedhost/changelog/#060-2024-10-15_2","title":"[0.6.0] -- 2024-10-15","text":"<ul> <li>Multiple ui improvements and fixes</li> <li>Updated bed metadata endpoint: added <code>annotation</code> to metadata return model, with standard schema</li> <li>Updated metadata in search endpoints.</li> <li>Added embed endpoint. #136</li> </ul>"},{"location":"bedhost/changelog/#050-2024-06-11","title":"[0.5.0] -- 2024-06-11","text":"<ul> <li>Improved Bed search (speed and quality)</li> <li>Added licenses</li> <li>UI tweaks</li> <li>Added universes and bed tokens to the database</li> <li>Added embedding endpoint</li> </ul>"},{"location":"bedhost/changelog/#040-2024-04-08","title":"[0.4.0] -- 2024-04-08","text":"<ul> <li>Support of new bbconf.</li> <li>Updated endpoints.</li> </ul>"},{"location":"bedhost/changelog/#030-2023-03-01","title":"[0.3.0] -- 2023-03-01","text":"<ul> <li>switch to pydantic2</li> <li>updated requirements</li> <li>updated docs</li> </ul>"},{"location":"bedhost/changelog/#020-2023-10-17","title":"[0.2.0] -- 2023-10-17","text":"<ul> <li>remove all graphql</li> <li>remove local static hosting of UI</li> <li>update to new pipestat-based bbconf (pending)</li> <li>major refactor of API that introduces backwards-incompatible changes</li> </ul>"},{"location":"bedhost/changelog/#013-2023-09-01","title":"[0.1.3] -- 2023-09-01","text":"<ul> <li>allow all origins</li> </ul>"},{"location":"bedhost/changelog/#012-2023-02-06","title":"[0.1.2] -- 2023-02-06","text":""},{"location":"bedhost/changelog/#change","title":"change","text":"<ul> <li>change <code>/bedset/my_bedset/file_paths</code>endpoint from GET to POST</li> </ul>"},{"location":"bedhost/changelog/#011-2021-10-30","title":"[0.1.1] -- 2021-10-30","text":""},{"location":"bedhost/changelog/#change_1","title":"change","text":"<ul> <li><code>/bed/genomes</code> and <code>bedset/genomes</code>: improve speed</li> </ul>"},{"location":"bedhost/changelog/#010-2021-10-25","title":"[0.1.0] -- 2021-10-25","text":""},{"location":"bedhost/changelog/#add","title":"add","text":"<ul> <li>GraphQL endpoints</li> </ul>"},{"location":"bedhost/changelog/#change_2","title":"change","text":"<ul> <li>endpoints update due to <code>bbconf</code> and <code>pipestat</code> changes</li> </ul>"},{"location":"bedhost/changelog/#006-2021-05-17","title":"[0.0.6] -- 2021-05-17","text":""},{"location":"bedhost/changelog/#add_1","title":"add","text":"<ul> <li>Add endpoints that serve:</li> <li>a list of genome assemblies in bedsets and bedfiles table</li> <li>bed files by search term(s)</li> <li>remote file path (http / s3)</li> </ul>"},{"location":"bedhost/changelog/#005-2021-04-15","title":"[0.0.5] -- 2021-04-15","text":""},{"location":"bedhost/changelog/#add_2","title":"add","text":"<ul> <li>Add examples of API endpoints</li> </ul>"},{"location":"bedhost/changelog/#fix","title":"fix","text":"<ul> <li>resolve <code>/about</code> page not found when typing/editing url in the address bar. </li> </ul>"},{"location":"bedhost/changelog/#004-2021-04-01","title":"[0.0.4] -- 2021-04-01","text":""},{"location":"bedhost/changelog/#add_3","title":"add","text":"<ul> <li>add endpoint for region-based query </li> </ul>"},{"location":"bedhost/changelog/#fix_1","title":"fix","text":"<ul> <li>constrauction of local file/img path</li> </ul>"},{"location":"bedhost/changelog/#003-2021-02-22","title":"[0.0.3] -- 2021-02-22","text":"<ul> <li>Initial project release</li> </ul>"},{"location":"bedhost/deployment/","title":"Deploying bedbase.org","text":"<p>This repository deploys the API for bedbase. It will run these services:</p> <ol> <li>production API: https://api.bedbase.org/</li> <li>dev API: https://api-dev.bedbase.org/</li> </ol> <p>This repo will deploy a new service by following these steps:</p> <ol> <li>Build an image by packaging the bedhost image (from dockerhub) with the bbconf file in this repository.</li> <li>Push that image to AWS.</li> <li>Deploy it to yeti cluster with aws task def.</li> </ol>"},{"location":"bedhost/deployment/#build-the-container","title":"Build the container","text":"<p>Here we use the <code>databio/bedhost</code> container on dockerhub, and just add the configuration file in this repo to it, so build is super fast.</p> <pre><code>docker build -t databio/bedhost-configured -f Dockerfiles/primary.Dockerfile .\n</code></pre> <p>Or for dev:</p> <pre><code>docker build -t databio/bedhost-configured-dev -f Dockerfiles/dev1.Dockerfile .\n</code></pre>"},{"location":"bedhost/deployment/#run-it-locally-to-test","title":"Run it locally to test","text":"<p>First, source the .env file to set env vars in the calling environment. Then, use <code>--env-file</code> to pass those env vars through to the container</p> <pre><code>source environment/production.env\ndocker run --rm --network=\"host\" \\\n  --env-file environment/docker.env \\\n  databio/bedhost-configured-dev\n</code></pre> <p>Here's another example for running the container:</p> <pre><code>docker run --rm --init -p 8000:8000 --name bedstat-rest-server \\\n  --network=\"host\" \\\n  --volume ~/code/bedbase.org/config/api.bedbase.org.yaml:/bedbase.yaml \\\n  --env-file ../bedbase.org/environment/docker.env \\\n  --env BEDBASE_CONFIG=/bedbase.yaml \\\n  databio/bedhost  uvicorn bedhost.main:app --reload\n</code></pre>"},{"location":"bedhost/deployment/#building-the-amazon-tagged-version","title":"Building the Amazon-tagged version","text":"<p>You could build and push to ECR like this if you need it... but the github action will do this for you.</p> <p>Authenticate with AWS ECR: <pre><code>aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 235728444054.dkr.ecr.us-east-1.amazonaws.com\n</code></pre></p> <p>Build/tag/push image: <pre><code>docker build -t 235728444054.dkr.ecr.us-east-1.amazonaws.com/bedhost -f Dockerfiles/primary.Dockerfile .\ndocker push 235728444054.dkr.ecr.us-east-1.amazonaws.com/bedhost\n</code></pre></p>"},{"location":"geniml/","title":"Geniml","text":""},{"location":"geniml/#introduction","title":"Introduction","text":"<p>Geniml is a genomic interval machine learning toolkit, a Python package for building machine learning models of genomic interval data (BED files). It also includes ancillary functions to support other types of analyses of genomic interval data.</p> <p>As of February 2024, this package and its documentation are undergoing rapid development, leading to some tutorials getting outdated. Please raise github issues if you find outdated or unclear directions, so we know where to focus effort that will benefit users.</p>"},{"location":"geniml/#installation","title":"Installation","text":""},{"location":"geniml/#to-install-geniml-use-this-commands","title":"To install <code>geniml</code> use this commands.","text":"<p>Without specifying dependencies, the default dependencies will be installed,  which DO NOT include machine learning (ML) or heavy processing libraries.</p> <p>From pypi: <pre><code>pip install geniml\n</code></pre> or install the latest version from the GitHub repository: <pre><code>pip install git+https://github.com/databio/geniml.git\n</code></pre></p>"},{"location":"geniml/#to-install-machine-learning-dependencies-use-this-command","title":"To install Machine learning dependencies use this command:","text":"<p>From pypi: <pre><code>pip install geniml[ml]\n</code></pre></p>"},{"location":"geniml/#development","title":"Development","text":"<p>Run tests (from <code>/tests</code>) with <code>pytest</code>. Please read the contributor guide to contribute.</p>"},{"location":"geniml/#modules-and-resources","title":"Modules and resources","text":""},{"location":"geniml/#organization","title":"Organization","text":"<p><code>geniml</code> is organized into modules. The modules section gives an overview of each module.</p>"},{"location":"geniml/#browsing-by-publication","title":"Browsing by publication","text":"<p>If you're coming here from a manuscript, you might find it easier to identify the tutorials relevant for a particular manuscript by visiting the landing page for the publication of interest. You can find documentation organized by manuscript in the manuscripts section.</p>"},{"location":"geniml/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format. </p>"},{"location":"geniml/changelog/#081-2025-09-23","title":"[0.8.1] -- 2025-09-23","text":"<ul> <li>Fixed bedspace </li> <li>Fixed tests in bbclient </li> <li>Removed unused tests and files</li> </ul>"},{"location":"geniml/changelog/#080-2025-09-23","title":"[0.8.0] -- 2025-09-23","text":"<ul> <li>Added Atacformer</li> </ul>"},{"location":"geniml/changelog/#070-2025-04-15","title":"[0.7.0] -- 2025-04-15","text":"<ul> <li>Improved encoding (indexing) of the bed file (region2vec)</li> <li>Updated code due to recent changes in the gtars</li> <li>Switched RegionSet from io module to Gtars regionSet</li> </ul>"},{"location":"geniml/changelog/#060-2025-01-03","title":"[0.6.0] -- 2025-01-03","text":"<ul> <li>Fixed: biocfilecache (upgraded to v0.6.0)</li> <li>Added: inspect functions that lists all cached beds and bedsets</li> </ul>"},{"location":"geniml/changelog/#052-2024-12-03","title":"[0.5.2] -- 2024-12-03","text":"<ul> <li>Switched bivec search to fastembed</li> <li>Fixed: incorrect caching of the files</li> </ul>"},{"location":"geniml/changelog/#051-2024-10-18","title":"[0.5.1] -- 2024-10-18","text":"<ul> <li>Fixed: Fix search score. #187</li> <li>Fixed: Batch search for Qdrant - Speed up search #186</li> <li>Fixed: Some packages not in requirements #189</li> </ul>"},{"location":"geniml/changelog/#050-2024-10-15","title":"[0.5.0] -- 2024-10-15","text":"<ul> <li>Added: new bi-vector search</li> </ul>"},{"location":"geniml/changelog/#043-2024-12-09","title":"[0.4.3] -- 2024-12-09","text":"<ul> <li>Fixed: required torch version</li> </ul>"},{"location":"geniml/changelog/#042-2024-10-07","title":"[0.4.2] -- 2024-10-07","text":"<ul> <li>Cleaned dependencies and separated into 2 groups: basic and ml</li> <li>Improved efficiency of CLI</li> <li>Fixed bbcache incorrectly saving files from url #175</li> </ul>"},{"location":"geniml/changelog/#041-2024-09-18","title":"[0.4.1] -- 2024-09-18","text":"<ul> <li>Fixed: bbclient ignores --cache-folder on CLI #164</li> <li>Fixed bug in opening bed files</li> </ul>"},{"location":"geniml/changelog/#040-2024-06-04","title":"[0.4.0] -- 2024-06-04","text":"<ul> <li>Added bed tokens caching to bbclient [bbclient] Add tokenized file cache and download #153</li> <li>Added pyBiocFileCache for bedfiles to support R caching [bbclient] Integrate bedbase caching with R #151</li> <li>Added support of Python3.12</li> <li>Optimized encoding of regions for Region2Vec models</li> <li>Added updates to the new Atacformer</li> <li>Renamed tokenizers to TreeTokenizer and AnnDataTokenizer</li> <li>Bump genimtools which includes performance upgrades and stability updates to the tokenizers</li> </ul>"},{"location":"geniml/changelog/#030-2024-04-04","title":"[0.3.0] -- 2024-04-04","text":"<ul> <li>Added S3 uploading to bbclient</li> <li>Added and cleaned tests</li> <li>Added bed2bed search interface</li> </ul>"},{"location":"geniml/changelog/#020-2024-02-20","title":"[0.2.0] -- 2024-02-20","text":"<ul> <li>Fixed a bug with pydantic</li> <li>Integrate <code>lightning</code> for easier training of models with SLURM and the DDP framework</li> <li>New datasets for streaming <code>.gtok</code> files to models</li> <li>New tutorials for updated ScEmbed</li> <li>Start working on <code>Atacformer</code></li> </ul>"},{"location":"geniml/changelog/#010-2023-12-18","title":"[0.1.0] -- 2023-12-18","text":"<ul> <li>First official released version of geniml</li> <li>Integrated bedshift 1.1.1 into geniml (excluding intersection support)</li> </ul>"},{"location":"geniml/changelog/#bedshift-111-2021-04-15","title":"[bedshift 1.1.1] - 2021-04-15","text":"<ul> <li>Updated documentation</li> <li>Fixed dependencies packaging for building documentation</li> </ul>"},{"location":"geniml/changelog/#bedshift-110-2021-04-02","title":"[bedshift 1.1.0] - 2021-04-02","text":"<ul> <li>Added ability to specify chrom sizes file, or refgenie genome.</li> <li>Add perturbation will create regions proportional to chromosome size</li> <li>Improve performance of perturbations</li> <li>Add --dropfile, --addfile, and --shiftfile options</li> <li>Add --add_valid option</li> <li>Add --yaml-config option</li> <li>Improved testing framework</li> <li>Improved logging messages</li> </ul>"},{"location":"geniml/changelog/#bedshift-100-2020-05-20","title":"[bedshift 1.0.0] - 2020-05-20","text":"<ul> <li>Add, shift, drop, cut, and merge perturbations.</li> <li>Basic documentation</li> <li>Basic tests</li> </ul>"},{"location":"geniml/contributing/","title":"Contributor guide","text":""},{"location":"geniml/contributing/#repository-organization","title":"Repository organization","text":"<p>This repo is divided into modules that provide related functions, each in a subfolder. The parent <code>geniml/</code> folder holds functionality that spans across modules, such as the general (top-level) argument parser (in <code>cli.py</code>), constants (<code>const.py</code>), and any other utility or general-purpose code that is used across modules.</p>"},{"location":"geniml/contributing/#using-modules-from-python","title":"Using modules from Python","text":"<p>This repo is divided into modules. Each module should be written in a way that it provides utility as a Python library. For example, you can call functions in the <code>hmm</code> module like this:</p> <pre><code>import geniml\n\ngeniml.hmm.function()\n</code></pre>"},{"location":"geniml/contributing/#command-line-interfaces","title":"Command-line interfaces","text":"<p>In addition to being importable from Python, some modules also provide a CLI. For these, developers provide a subcommand for CLI use. The root <code>geniml</code> package provides a generalized command-line interface with the command <code>geniml</code>. The modules that provide CLIs then correspond to CLI commands, e.g <code>geniml hmm</code> or <code>geniml likelihood</code>, with the corresponding code contained within a sub-folder named after the model:</p> <pre><code>geniml &lt;module&gt; ...\n</code></pre> <p>This is implemented within each module folder with:</p> <ul> <li><code>geniml/&lt;module&gt;/cli.py</code> - defines the command-line interface and provides a subparser for this module's CLI command.</li> </ul>"},{"location":"geniml/contributing/#adding-a-new-module","title":"Adding a new module","text":"<p>To add functionality to geniml, you could add it to an existing module. Or, if no existing module fit, you could add a new module.</p>"},{"location":"geniml/contributing/#creating-your-module","title":"Creating your module","text":"<p>Each module should be written in a way that it provides utility as a Python library. Organize your module with these files:</p> <ul> <li><code>/docs/tutorials/&lt;module&gt;.md</code> - describes how to use the code</li> <li><code>/geniml/&lt;module&gt;/main.py</code>, and other <code>.py</code> files - functions that provide utility for this module.</li> </ul> <p>All the functions should be written to be useful via import, calling with <code>geniml.&lt;module&gt;.&lt;function&gt;</code>. For example:</p> <pre><code>import geniml\n\ngeniml.hmm.function()\n</code></pre>"},{"location":"geniml/contributing/#adding-your-module-to-geniml","title":"Adding your module to geniml","text":"<ol> <li>Put your module in a subfolder.</li> <li>Make sure to include a <code>__init__.py</code> so it's importable.</li> <li>Add it to list of packages in <code>setup.py</code>.</li> <li>If it makes sense to have a CLI for this module, implement it in <code>geniml/&lt;module_name&gt;/cli.py</code>. Link this into the main cli by putting it under an appropriate command name following the pattern for other modules in <code>geniml/cli.py</code>.</li> </ol>"},{"location":"geniml/contributing/#shared-code","title":"Shared code","text":"<p>Any variables, functions, or other code that is shared across modules should be placed in the parent module, which is held in the geniml folder.</p>"},{"location":"geniml/datasets/","title":"Genomic datasets with <code>geniml</code>","text":""},{"location":"geniml/datasets/#overview","title":"Overview","text":"<p>Genomic datasets are well known for their incredible size. Therefore, using these for machine learning pipelines requires clever strategies and considerations to effectively handle the large volumes of data. This is particularly problematic when training new models. To that end, <code>geniml</code> has two separate workflows for handling genomic data: one for model training and a second for model inference (using a pre-trained model). These workflows have big differences on when and where genomic datasets are tokenized and stored.</p>"},{"location":"geniml/datasets/#model-training","title":"Model training","text":"<p>Model training, especially pre-training, usually requires large datasets with billions of genomic tokens. These datasets are way too large to fit in memory and therefore must be streamed from disk during training. Because of this, the data must be tokenized \"on the fly\" for each epoch. This is wildly inefficient and for particularly large datasets, results in the majority of training time being dedicated to tokenization alone. Therefore, the data need be pre-tokenized into an intermediate form which can then be streamed in for each epoch. This removes tokenization entirely from the training procedure and therefore increase efficiency (Fig. 1A).</p>"},{"location":"geniml/datasets/#model-inference","title":"Model inference","text":"<p>Model inference is the process of utilizing a pre-trained model to analyze some new, unseen data. While the output of the model might vary (embeddings, label, new data), the input is always the same: genomic tokens. Except under rare circumstances, it's not typical that model inference involves large volumes of data. Therefore, pre-tokenization is not necessary. Because of this, data is to be tokenized in-memory and directly to the format required to pass through the model (Fig. 1B).</p>"},{"location":"geniml/datasets/#tokenization-forms","title":"Tokenization forms","text":"<p>Given the above requirements, tokenizers need to be able to output tokenized genomic data into different forms and locations. For model training: tokenizers should take either bed files or <code>.h5ad</code> single-cell datasets and convert them into an intermediary <code>.gtok</code> file format. These <code>.gtok</code> files will be directly consumed during model training. For model inference: tokenizers should take either bed files or <code>.h5ad</code> single-cell datasets and output an in-memory representation of these tokens; typically in the form of a <code>torch.Tensor</code> or python list. The following table summarizes the format, location, and scenario in which data is tokenized:</p>   |                 | Where  | What        | When              | | --------------- | --------- | ------------- | ----------------- | | Model training  | On disk   | `.gtok` files | Prior to training | | Model inference | In memory | `torch.Tensors`              | On the fly                  |"},{"location":"geniml/datasets/#datasets-in-geniml","title":"Datasets in <code>geniml</code>","text":"<p><code>geniml</code> uses <code>pytorch</code> + <code>lightning</code> to train models. This ecosystem encourages the use of <code>torch</code>s built-in <code>Dataset</code> class to parallelize and batch the loading of data. Because training and fine-tuning models requires pre-tokenized data (<code>.gtok</code> files), <code>geniml</code> needs datasets to handle this. It most likely will look like: <pre><code>from typing import List\n\nfrom torch.data.utils import IterableDataset\n\nclass PretokenizedDataset(IterableDataset):\n  def __init__(self, data: Union[str, List[str]):\n    self.data = data\n      if isinstance(data, str):\n        self._is_folder = True\n      elif isinstance(data, list) and isinstance(data[0], str):\n        self._is_folder = False\n      else:\n        raise ValueError(\"`data` must be a path to a folder or a list of `.gtok` files\")\n\n  def __iter__(self, indx: int):\n    if self._is_folder:\n      for file in os.listdir(self.data):\n        with open(file, 'r') as f:\n          for line in file.readlines():\n            yield line.split(\",\")\n    else:\n      for file in self.data:\n        with open(file, 'r') as f:\n          for line in file.readlines():\n            yield line.split(\",\")\n</code></pre> Here, we are no longer tokenizing each epoch, rather just streaming in data that has already been pre-tokenized. I still need to think about this in the context of fine-tuning and datasets that require targets and labels.</p>"},{"location":"geniml/datasets/#gtok-file-format","title":"<code>.gtok</code> file format","text":"<p>The <code>.gtok</code> file format is a binary file where each token is stored as a 32-bit integer. This allows tokens to be stored in a very compact format with the ability to represent up to 4 billion unique tokens. Using our companion package <code>genimtools</code>, we can convert <code>.bed</code> files into <code>.gtok</code> files after tokenization.</p>"},{"location":"geniml/modules/","title":"Module overviews","text":"<p><code>geniml</code> is organized into modules. Each module groups together related tasks. This document provides an overview of each module.</p>"},{"location":"geniml/modules/#module-assess-universe","title":"Module <code>assess-universe</code>","text":"<p>Many genomic interval analysis methods, particularly those used by <code>geniml</code> require that regions be re-defined in terms of a consensus region set, or universe. However, a universe may not be a good fit to a collection of files. This module assesses that fit. Given a collection of genomic interval sets, and a proposed universe, we can assess how well the universe fits the genomic interval sets. This module provides several complementary methods to assess fit.</p>"},{"location":"geniml/modules/#module-bbclient","title":"Module <code>bbclient</code>","text":"<p>The <code>bbclient</code> module can download BED files or BED sets from BEDbase and cache them into local folders.</p>"},{"location":"geniml/modules/#module-bedspace","title":"Module <code>bedspace</code>","text":"<p>The <code>bedspace</code> module uses the StarSpace method (Wu et al., 2018) to jointly embed genomic interval regions sets with associated metadata into a shared latent embedding space. This facilitates fast search and retrieval of similar region sets and their associated metadata. </p>"},{"location":"geniml/modules/#module-build-universe","title":"Module <code>build-universe</code>","text":"<p>This module provides multiple ways to build a genomic region universe. These include: 1. HMM: uses an HMM to create a flexible segment universe, given an input of several bed files.</p>"},{"location":"geniml/modules/#module-evaluation","title":"Module <code>evaluation</code>","text":"<p>Once a <code>geniml</code> region embedding model is trained, we may want to evaluate the embeddings. The <code>evaluation</code> module provides several functions for that. These include statistical tests, like the Cluster Tendency Test (CTT) and the Reconstruction Test (RCT), and biological tests, the Genome Distance Scaling Test (GDST) and the Neighborhood Preserving Test (NPT). These evaluation metrics can be helpful to determine if your models are working well, optimize training parameters, etc.</p>"},{"location":"geniml/modules/#module-region2vec","title":"Module <code>region2vec</code>","text":"<p><code>Region2Vec</code> is an unsupervised method for creating embeddings for genomic regions and region sets from a set of raw BED files. The program uses a variation of the word2vec algorithm by building shuffled context windows from BED files. The co-occurrence statistics of genomic regions in a collection of BED files allow the model to learn region embeddings.</p>"},{"location":"geniml/modules/#module-scembed","title":"Module <code>scembed</code>","text":"<p><code>scEmbed</code> is a single-cell implementation of <code>region2Vec</code>: a method to represent genomic region sets as vectors, or embeddings, using an adapted word2vec approach. <code>scEmbed</code> allows for dimensionality reduction and feature selection of single-cell ATAC-seq data; a notoriously sparse and high-dimensional data type. We intend for <code>scEmbed</code> to be used with the <code>scanpy</code> package. As such, it natively accepts <code>AnnData</code> objects as input and returns <code>AnnData</code> objects as output.</p>"},{"location":"geniml/modules/#module-search","title":"Module <code>search</code>","text":"<p>The search module provides a generic interface for vector search. Several geniml modules (such as <code>region2vec</code>), will create embeddings for different entities. The search module provides interfaces that store vectors and perform fast k-nearest neighbors (KNN) search with a given query vector.  Back-end options include a database backend (using <code>qdrant-client</code>) and  local file backend (using <code>hnswlib</code>.</p>"},{"location":"geniml/modules/#module-text2bednn","title":"Module <code>text2bednn</code>","text":"<p><code>Vec2Vec</code> is a feedforward neural network that maps vectors from the embedding space of natural language (such as embeddings created by <code>fastembed</code>) to the embedding space of BED (such as embeddings created by <code>Region2Vec</code>). By mapping the embedding of natural language query strings to the space of BED files, <code>Vec2Vec</code> can perform natural language search of BED files. </p>"},{"location":"geniml/modules/#module-tokenization","title":"Module <code>tokenization</code>","text":"<p>In NLP, training word embeddings requires first tokenizing words such that words in different forms are represented by one word. For example, \"orange\", \"oranges\" and \"Orange\" are all mapped to \"orange\" since they essentially convey the same meaning. This reduces the vocabulary size and improves the quality of learned embeddings. Similarly, many <code>geniml</code> modules (such as <code>region2vec</code>) require first tokenizating regions.</p> <p>To tokenize regions, we need to provide a universe, which specifies the \"vocabulary\" of genomic regions. The universe is a BED file, containing representative regions. With the given universe, we represent (tokenize) raw regions into the regions in the universe.</p> <p>Different strategies can be used to tokenize. The simplest case we call hard tokenization, which means if the overlap between a raw region in a BED file and a region in the universe exceeds a certain amount, then we use the region in the universe to represent this raw region; otherwise, we ignore this raw region. This is a \"zero or one\" process. After hard tokenization, each BED file will contain only regions from the universe, and the number of regions will be smaller or equal to the original number.</p>"},{"location":"geniml/support/","title":"Support","text":"<p>Please raise any issues or questions using the GitHub issue tracker.</p>"},{"location":"geniml/api-reference/assess/","title":"Assess Module","text":""},{"location":"geniml/api-reference/assess/#geniml.assess","title":"assess","text":""},{"location":"geniml/api-reference/assess/#geniml.assess-functions","title":"Functions","text":""},{"location":"geniml/api-reference/assess/#geniml.assess-modules","title":"Modules","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.assess","title":"assess","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.assess-functions","title":"Functions","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.assess.run_all_assessment_methods","title":"run_all_assessment_methods","text":"<pre><code>run_all_assessment_methods(raw_data_folder, file_list, universe, no_workers, folder_out, pref, save_each, overlap=False, distance_f_t_u=False, distance_f_t_u_flex=False, distance_u_t_f=False, distance_u_t_f_flex=False)\n</code></pre> <p>Assess universe fit to collection using overlap and distance metrics.</p> <p>Args:     raw_data_folder (str): path to raw files from the collection     file_list (str): path to file with list of files in the collection     universe (str): path to universe that is being assessed     no_workers (int): number of workers for multiprocessing     folder_out (str): output folder     pref (str): prefixed used for creating output files     save_each (bool): if save output of distance metrics for each region     overlap (bool): if calculate overlap metrics     distance_f_t_u (bool): if calculate distance from file to universe metrics     distance_f_t_u_flex (bool): if calculate flexible distance from file to universe metrics     distance_u_t_f (bool): if calculate distance from universes to file metrics     distance_u_t_f_flex (bool): if calculate flexible distance from universes to file metrics</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.assess.get_rbs","title":"get_rbs","text":"<pre><code>get_rbs(f_t_u, u_t_f)\n</code></pre> <p>Calculate RBS</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.assess.get_mean_rbs","title":"get_mean_rbs","text":"<pre><code>get_mean_rbs(folder, file_list, universe, no_workers, flexible=False)\n</code></pre> <p>Calculate average RBS of the collection.</p> <p>Args:     folder (str): path to folder with the collection     file_list (str): path to file with list of files in the collection     universe (str): path to the universe     no_workers (int): number of workers for multiprocessing     flexible (bool): if to calculate flexible version of the metric</p> <p>Returns:     int: average RBS</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.assess.get_rbs_from_assessment_file","title":"get_rbs_from_assessment_file","text":"<pre><code>get_rbs_from_assessment_file(file, cs_each_file=False, flexible=False)\n</code></pre> <p>Calculate RBS form file with results of metrics per file.</p> <p>Args:     file (str): path to file with assessment results     cs_each_file (bool): if report RBS for each file, not average for the collection     flexible (bool): if use flexible version of the metric</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.assess.get_f_10_score","title":"get_f_10_score","text":"<pre><code>get_f_10_score(folder, file_list, universe, no_workers)\n</code></pre> <p>Get F10 score for a universes and collection of files.</p> <p>Args:     folder (str): path to folder with the collection     file_list (str): path to file with list of files in the collection     universe (str): path to the universe     no_workers (int): number of workers for multiprocessing</p> <p>Returns:     int: average F10 score</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.assess.get_f_10_score_from_assessment_file","title":"get_f_10_score_from_assessment_file","text":"<pre><code>get_f_10_score_from_assessment_file(file, f10_each_file=False)\n</code></pre> <p>Get F10 score from assessment output file.</p> <p>Args:     file (str): path to file with assessment results     f10_each_file (bool): if report F10 for each file, not average for the collection</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.assess.get_likelihood","title":"get_likelihood","text":"<pre><code>get_likelihood(model_file, universe, cove_folder, cove_prefix='all', flexible=False, save_peak_input=False)\n</code></pre> <p>Calculate universe likelihood given collection.</p> <p>Args:     model_file (str): path to file with likelihood model     universe (str): path to the universe     cove_folder (str): path to the coverage folder     cove_prefix (str): prefixed used for generating coverage     flexible (bool): if to calculate flexible likelihood     save_peak_input (bool): if to save likelihood input of each region</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.assess.filter_universe","title":"filter_universe","text":"<pre><code>filter_universe(universe, universe_filtered, min_size=0, min_coverage=0, filter_lh=False, model_file=None, cove_folder=None, cove_prefix=None, lh_cutoff=0)\n</code></pre> <p>Filter universe by region size, coverage by collection, likelihood.</p> <p>Args:     universe (str): path to input universe     universe_filtered (str): path to output filtered universe     min_size (int): minimum size of the region in the output universe     min_coverage (int): minimum number coverage of universe region by collection     filter_lh (bool): if use likelihood to filter universe     model_file (str): path to collection likelihood model     cove_folder (str): path to folder with coverage tracks     cove_prefix (str): prefixed used for creating tracks     lh_cutoff (int): minimum likelihood input</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.cli","title":"cli","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.cli-functions","title":"Functions","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.cli.build_subparser","title":"build_subparser","text":"<pre><code>build_subparser(parser)\n</code></pre> <p>Builds argument parser.</p> <p>Returns:</p> Type Description <p>Argument parser</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.distance","title":"distance","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.distance-functions","title":"Functions","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.distance.flexible_distance_between_two_regions","title":"flexible_distance_between_two_regions","text":"<pre><code>flexible_distance_between_two_regions(region, query)\n</code></pre> <p>Calculate distance between region and flexible region from flexible universe.</p> <p>Args:     region ([int, int]): region from flexible universe     query (int): analyzed region</p> <p>Returns:     int: distance</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.distance.distance_between_two_regions","title":"distance_between_two_regions","text":"<pre><code>distance_between_two_regions(region, query)\n</code></pre> <p>Calculate distance between region in database and region from the query.</p> <p>Args:     region ([int]): region from hard universe     query (int): analysed region</p> <p>Returns:     int: distance</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.distance.distance_to_closest_region","title":"distance_to_closest_region","text":"<pre><code>distance_to_closest_region(db, db_queue, i, current_chrom, unused_db, pos_index, flexible, uni_to_file)\n</code></pre> <p>Calculate distance from given peak to the closest region in database.</p> <p>Args:     db (file): database file     db_queue (list): queue of three last positions in database     i: analyzed position from the query     current_chrom (str): current analyzed chromosome from query     unused_db (list): list of positions from universe that were not compared to query     pos_index (list): which indexes from universe region use to calculate distance     flexible (bool): whether the universe if flexible     uni_to_file (bool): whether calculate distance from universe to file</p> <p>Returns:     int: peak distance to universe</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.distance.read_in_new_universe_regions","title":"read_in_new_universe_regions","text":"<pre><code>read_in_new_universe_regions(db, q_chrom, current_chrom, unused_db, db_queue, waiting, pos_index)\n</code></pre> <p>Read in new universe regions closest to the peak.</p> <p>Args:     db (file): universe file     q_chrom (str): new peak's chromosome     current_chrom (str): chromosome that was analyzed so far     unused_db (list): list of positions from universe that were not compared to query     db_queue (list): que of three last positions in universe     waiting (bool): whether iterating through file, without calculating         distance, if present chromosome not present in universe     pos_index (list): which indexes from universe region use to calculate distance</p> <p>Returns:     tuple: (bool, str) - if iterating through chromosome not present in universe; current chromosome in query</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.distance.calc_distance_between_two_files","title":"calc_distance_between_two_files","text":"<pre><code>calc_distance_between_two_files(universe, q_folder, q_file, flexible, save_each, folder_out, pref, uni_to_file=False)\n</code></pre> <p>Main function for calculating distance between regions in file query to regions in database.</p> <p>Args:     universe (str): path to universe     q_folder (str): path to folder containing query files     q_file (str): query file     flexible (bool): whether the universe if flexible     save_each (bool): whether to save calculated distances for each file     folder_out (str): output folder     pref (str): prefix used as the name of the folder containing calculated distance for each file     uni_to_file (bool): whether to calculate distance from universe to file</p> <p>Returns:     tuple: (str, int, int) - file name; median od distance of starts to starts in universe;         median od distance of ends to ends in universe</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.distance.run_distance","title":"run_distance","text":"<pre><code>run_distance(folder, file_list, universe, no_workers, flexible=False, folder_out=None, pref=None, save_each=False, uni_to_file=False)\n</code></pre> <p>For group of files calculate distance to the nearest region in universe.</p> <p>Args:     folder (str): path to folder containing query files     file_list (str): path to file containing list of query files     universe (str): path to universe file     no_workers (int): number of parallel processes     flexible (bool): whether the universe if flexible     folder_out (str): output folder     pref (str): prefix used for saving     save_each (bool): whether to save calculated distances for each file     uni_to_file (bool): whether to calculate distance from universe to file</p> <p>Returns:     tuple: (float, float) - mean of median distances from starts in query to the nearest starts in universe;         mean of median distances from ends in query to the nearest ends in universe</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.intersection","title":"intersection","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.intersection-functions","title":"Functions","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.intersection.chrom_cmp","title":"chrom_cmp","text":"<pre><code>chrom_cmp(a, b)\n</code></pre> <p>Return smaller chromosome name</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.intersection.relationship_helper","title":"relationship_helper","text":"<pre><code>relationship_helper(region_a, region_b, only_in, overlap)\n</code></pre> <p>For two region calculate their overlap; for earlier region calculate how many base pair only in it.</p> <p>Args:     region_a ([int, int]): region that starts first     region_b ([int, int]): region that starts second     only_in (int): number of positions only in a so far     overlap (int): number of overlapping so far</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.intersection.two_region_intersection_diff","title":"two_region_intersection_diff","text":"<pre><code>two_region_intersection_diff(region_d, region_q, only_in_d, only_in_q, inside_d, inside_q, overlap, start_d, start_q, waiting_d, waiting_q)\n</code></pre> <p>Check mutual position of two regions and calculate intersection and difference of two regions.</p> <p>Args:     region_d (list): region from universe     region_q (list): region from query     only_in_d (int): number of base pair only in universe     only_in_q (int): number of base pair only in query     inside_d (bool): whether there is still part of the region from universe to analyse     inside_q (bool): whether there is still part of the region from query to analyse     overlap (int): size of overlap     start_d (int): start position of currently analyzed universe region     start_q (int): start position of currently analyzed query region     waiting_d (bool): whether waiting for the query to finish chromosome     waiting_q (bool): whether waiting for the universe to finish chromosome</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.intersection.read_in_new_line","title":"read_in_new_line","text":"<pre><code>read_in_new_line(region, start, chrom, inside, waiting, lines, c_chrom, not_e)\n</code></pre> <p>Read in a new line from query or universe file</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.intersection.calc_diff_intersection","title":"calc_diff_intersection","text":"<pre><code>calc_diff_intersection(db, folder, query)\n</code></pre> <p>Difference and overlap of two files on base pair level.</p> <p>Args:     db (str): path to universe file     folder (str): path to folder with query file     query (str): query file name</p> <p>Returns:     tuple: (str, int, int, int) - file name; bp only in universe; bp only in query; overlap in bp</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.intersection.run_intersection","title":"run_intersection","text":"<pre><code>run_intersection(folder, file_list, universe, no_workers)\n</code></pre> <p>Calculate the base pair intersection of universe and group of files.</p> <p>Args:     folder (str): path to folder containing query files     file_list (str): path to file containing list of query files     universe (str): path to universe file     no_workers (int): number of parallel processes     save_to_file (str): whether to save median of calculated distances for each file     folder_out (str): output folder     pref (str): prefix used for saving</p> <p>Returns:     tuple: (float, float) - mean of fractions of intersection of file and universe divided by universe size;         mean of fractions of intersection of file and universe divided by file size</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.likelihood","title":"likelihood","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.likelihood-classes","title":"Classes","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.likelihood.LhModel","title":"LhModel","text":"<pre><code>LhModel(model, cove)\n</code></pre> <p>Object with combined information about lh model and coverage.</p> <p>Args:     model (ndarray): lh model array     cove (ndarray): coverage array</p> Functions"},{"location":"geniml/api-reference/assess/#geniml.assess.likelihood-functions","title":"Functions","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.likelihood.calc_likelihood_hard","title":"calc_likelihood_hard","text":"<pre><code>calc_likelihood_hard(universe, chroms, model_lh, coverage_folder, coverage_prefix, name, s_index, e_index=None)\n</code></pre> <p>Calculate likelihood of universe for given type of model. To be used with binomial model.</p> <p>Args:     universe (str): path to universe file     chroms (list): list of chromosomes present in model     model_lh (ModelLH): likelihood model     coverage_folder: path to a folder with genome coverage by tracks     coverage_prefix: prefix used in uniwig for creating coverage     name (str): suffix of model file name, which contains information about model type     s_index (int): from which position in universe line take assess region start position     e_index (int): from which position in universe line take assess region end position</p> <p>Returns:     float: likelihood of universe for given model</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.likelihood.hard_universe_likelihood","title":"hard_universe_likelihood","text":"<pre><code>hard_universe_likelihood(model, universe, coverage_folder, coverage_prefix)\n</code></pre> <p>Calculate likelihood of hard universe based on core, start, end coverage model.</p> <p>Args:     model (str): path to file containing model     universe (str): path to universe     coverage_folder: path to a folder with genome coverage by tracks     coverage_prefix: prefix used in uniwig for creating coverage</p> <p>Returns:     float: likelihood</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.likelihood.likelihood_only_core","title":"likelihood_only_core","text":"<pre><code>likelihood_only_core(model_file, universe, coverage_folder, coverage_prefix)\n</code></pre> <p>Calculate likelihood of universe based only on core coverage model.</p> <p>Args:     model_file (str): path to name containing model     universe (str): path to universe     coverage_folder: path to a folder with genome coverage by tracks     coverage_prefix: prefix used in uniwig for creating coverage</p> <p>Returns:     float: likelihood</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.likelihood.background_likelihood","title":"background_likelihood","text":"<pre><code>background_likelihood(start, end, model_start, model_cove, model_end)\n</code></pre> <p>Calculate likelihood of background for given region</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.likelihood.weigh_livelihood","title":"weigh_livelihood","text":"<pre><code>weigh_livelihood(start, end, model_process, model_cove, model_out, reverse)\n</code></pre> <p>Calculate weighted likelihood of flexible part of the region.</p> <p>Args:     start (int): start of the region     end (int): end of the region     model_process (array): model for analyzed type of flexible region     model_cove (array): model for coverage     model_out (array): model for flexible region that is not being analyzed     reverse (bool): if model_process corespondents to end we have to reverse the weighs</p> <p>Returns:     float: likelihood of flexible part of the region</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.likelihood.likelihood_flexible_universe","title":"likelihood_flexible_universe","text":"<pre><code>likelihood_flexible_universe(model_file, universe, cove_folder, cove_prefix, save_peak_input=False)\n</code></pre> <p>Likelihood of given universe under the model.</p> <p>Args:     model_file (str): path to file with lh model     universe (str): path to universe     cove_folder: path to a folder with genome coverage by tracks     cove_prefix: prefix used in uniwig for creating coverage     save_peak_input (bool): whether to save universe with each peak lh</p> <p>Returns:     float: lh of the flexible universe</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.utils","title":"utils","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.utils-functions","title":"Functions","text":""},{"location":"geniml/api-reference/assess/#geniml.assess.utils.prep_data","title":"prep_data","text":"<pre><code>prep_data(folder, file, tmp_file)\n</code></pre> <p>File sort and merge</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.utils.check_if_uni_sorted","title":"check_if_uni_sorted","text":"<pre><code>check_if_uni_sorted(universe)\n</code></pre> <p>Check if regions in file are sorted</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.utils.process_line","title":"process_line","text":"<pre><code>process_line(line)\n</code></pre> <p>Helper for reading in bed file line</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.utils.chrom_cmp_bigger","title":"chrom_cmp_bigger","text":"<pre><code>chrom_cmp_bigger(a, b)\n</code></pre> <p>Natural check if chromosomes name is bigger</p>"},{"location":"geniml/api-reference/assess/#geniml.assess.utils.process_db_line","title":"process_db_line","text":"<pre><code>process_db_line(dn, pos_index)\n</code></pre> <p>Helper for reading in universe bed file line</p>"},{"location":"geniml/api-reference/atacformer/","title":"Atacformer Module","text":""},{"location":"geniml/api-reference/atacformer/#geniml.atacformer","title":"atacformer","text":""},{"location":"geniml/api-reference/atacformer/#geniml.atacformer-classes","title":"Classes","text":""},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.TrainingTokenizer","title":"TrainingTokenizer","text":"<pre><code>TrainingTokenizer(*args, **kwargs)\n</code></pre> <p>               Bases: <code>Tokenizer</code>, <code>PreTrainedTokenizerBase</code></p> <p>A special training tokenizer. This class is a subclass of both our Tokenizer and PreTrainedTokenizerBase. This is because the data collator requires a collator that is both a Tokenizer and a PreTrainedTokenizerBase. This is a workaround to make the code work with our Tokenizer.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.TrainingTokenizer-attributes","title":"Attributes","text":""},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.TrainingTokenizer.all_special_ids","title":"all_special_ids  <code>property</code>","text":"<pre><code>all_special_ids\n</code></pre> <p>Returns a list of all special token ids.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerModel","title":"AtacformerModel","text":"<pre><code>AtacformerModel(config)\n</code></pre> <p>               Bases: <code>AtacformerPreTrainedModel</code></p> <p>atacformer model with a simple embedding layer that skips positional encoding.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerModel-functions","title":"Functions","text":""},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerModel.forward","title":"forward","text":"<pre><code>forward(input_ids=None, attention_mask=None, return_dict=None, **kwargs)\n</code></pre> <p>Forward pass of the model.</p> <p>Args:     input_ids (<code>torch.LongTensor</code>):         Input tensor of shape (batch_size, sequence_length).     attention_mask (<code>torch.Tensor</code>, optional):         Mask to avoid performing attention on padding token indices.     return_dict (<code>bool</code>, optional):         Whether to return the outputs as a dict or a tuple. Raises:     ValueError: If <code>input_ids</code> is not provided. Returns:     <code>torch.Tensor</code>: The output of the model. It will be a tensor of shape (batch_size, sequence_length, hidden_size).</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerForMaskedLM","title":"AtacformerForMaskedLM","text":"<pre><code>AtacformerForMaskedLM(config)\n</code></pre> <p>               Bases: <code>EncodeTokenizedCellsMixin</code>, <code>AtacformerPreTrainedModel</code></p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerForMaskedLM-functions","title":"Functions","text":""},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerForMaskedLM.forward","title":"forward","text":"<pre><code>forward(input_ids=None, attention_mask=None, labels=None, return_dict=None)\n</code></pre> <p>Forward pass of the model.</p> <p>Args:     input_ids (<code>torch.LongTensor</code>):         Input tensor of shape (batch_size, sequence_length).     attention_mask (<code>torch.Tensor</code>, optional):         Mask to avoid performing attention on padding token indices.     labels (<code>torch.LongTensor</code>, optional):         Labels for masked language modeling.     return_dict (<code>bool</code>, optional):         Whether to return the outputs as a dict or a tuple. Returns:     <code>MaskedLMOutput</code>: The output of the model.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerForReplacedTokenDetection","title":"AtacformerForReplacedTokenDetection","text":"<pre><code>AtacformerForReplacedTokenDetection(config)\n</code></pre> <p>               Bases: <code>EncodeTokenizedCellsMixin</code>, <code>AtacformerPreTrainedModel</code></p> <p>Atacformer model for replaced token detection. This model uses the ELECTRA framework to train a discriminator (this model) to detect replaced tokens.</p> <p>https://arxiv.org/abs/2003.10555</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerForReplacedTokenDetection-functions","title":"Functions","text":""},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerForReplacedTokenDetection.forward","title":"forward","text":"<pre><code>forward(input_ids=None, attention_mask=None, labels=None, return_dict=None)\n</code></pre> <p>Forward pass of the model.</p> <p>Args:     input_ids (<code>torch.LongTensor</code>):         Input tensor of shape (batch_size, sequence_length).     attention_mask (<code>torch.Tensor</code>, optional):         Mask to avoid performing attention on padding token indices.     labels (<code>torch.LongTensor</code>, optional):         Labels for masked language modeling.     return_dict (<code>bool</code>, optional):         Whether to return the outputs as a dict or a tuple. Returns:     <code>TokenClassifierOutput</code>: The output of the model.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerForCellClustering","title":"AtacformerForCellClustering","text":"<pre><code>AtacformerForCellClustering(config)\n</code></pre> <p>               Bases: <code>EncodeTokenizedCellsMixin</code>, <code>AtacformerPreTrainedModel</code></p> <p>Atacformer model for cell clustering. It follows a similar learning framework to SentenceBERT (SBERT), where the model is trained to minimize the distance between positive pairs and maximize the distance between negative pairs.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerForUnsupervisedBatchCorrection","title":"AtacformerForUnsupervisedBatchCorrection","text":"<pre><code>AtacformerForUnsupervisedBatchCorrection(config)\n</code></pre> <p>               Bases: <code>EncodeTokenizedCellsMixin</code>, <code>AtacformerPreTrainedModel</code></p> <p>Atacformer model for batch correction. It follows a similar learning framework to the one used in domain adaptation, where the model is trained to correct batch effects in the embeddings.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerForUnsupervisedBatchCorrection-functions","title":"Functions","text":""},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerForUnsupervisedBatchCorrection.forward","title":"forward","text":"<pre><code>forward(input_ids=None, attention_mask=None, labels=None, batch_labels=None, return_dict=None)\n</code></pre> <p>Forward pass of the model.</p> <p>Args:     input_ids (<code>torch.LongTensor</code>):         Input tensor of shape (batch_size, sequence_length).     attention_mask (<code>torch.Tensor</code>, optional):         Mask to avoid performing attention on padding token indices.     labels (<code>torch.Tensor</code>, optional):         Labels for masked language modeling (ELECTRA).     batch_labels (<code>torch.Tensor</code>, optional):         Labels for batch prediction. Should be of shape (batch_size,).     return_dict (<code>bool</code>, optional):         Whether to return the outputs as a dict or a tuple. Returns:     <code>BaseModelOutput</code>: The output of the model.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AtacformerConfig","title":"AtacformerConfig","text":"<pre><code>AtacformerConfig(use_pos_embeddings=True, vocab_size=890711, max_position_embeddings=8192, hidden_size=384, intermediate_size=1536, num_hidden_layers=6, num_attention_heads=8, pad_token_id=890705, eos_token_id=890708, bos_token_id=890709, cls_token_id=890707, sep_token_id=890710, sparse_prediction=True, norm_eps=1e-05, embedding_dropout=0.0, initializer_range=0.02, initializer_cutoff_factor=2.0, tie_word_embeddings=True, num_batches=None, lambda_adv=1.0, grl_alpha=1.0, bc_unfreeze_last_n_layers=2, **kwargs)\n</code></pre> <p>               Bases: <code>PretrainedConfig</code></p> <p>This is the configuration class to store the configuration of an <code>AtacformerModel</code>. it inherits from [<code>ModernBertConfig</code>] and expands it for Atacformer specific settings. instantiating a configuration with the defaults will yield a similar configuration to that of the modernbert base configuration.</p> <p>Args:     use_pos_embeddings (<code>bool</code>, optional, defaults to <code>True</code>):         whether to use positional embeddings.     vocab_size (<code>int</code>, optional, defaults to 890711):         vocabulary size tailored for genomic regions.     max_position_embeddings (<code>int</code>, optional, defaults to 8192):         the maximum sequence length that this model might ever be used with.     hidden_size (<code>int</code>, optional, defaults to 384):         the size of the encoder layers and the pooler layer.     intermediate_size (<code>int</code>, optional, defaults to 1536):         the size of the \"intermediate\" (often named feed-forward) layer in the transformer.     num_hidden_layers (<code>int</code>, optional, defaults to 6):         the number of hidden layers in the Transformer encoder.     num_attention_heads (<code>int</code>, optional, defaults to 8):         the number of attention heads in each attention layer.     pad_token_id (<code>int</code>, optional, defaults to 890705):         the id of the token used for padding.     eos_token_id (<code>int</code>, optional, defaults to 890708):         the id of the token used for the end of a sequence.     bos_token_id (<code>int</code>, optional, defaults to 890709):         the id of the token used for the beginning of a sequence.     cls_token_id (<code>int</code>, optional, defaults to 890707):         the id of the token used for classification tasks.     sep_token_id (<code>int</code>, optional, defaults to 890710):         the id of the token used to separate segments in a sequence.     sparse_prediction (<code>bool</code>, optional, defaults to <code>True</code>):         whether to use sparse prediction for the output layer.     norm_eps (<code>float</code>, optional, defaults to 1e-5):         the epsilon value used for layer normalization.     embedding_dropout (<code>float</code>, optional, defaults to 0.0):         the dropout probability for the embedding layer.     initializer_range (<code>float</code>, optional, defaults to 0.02):         the standard deviation of the truncated_normal_initializer for initializing all weight matrices.     initializer_cutoff_factor (<code>float</code>, optional, defaults to 2.0):         the cutoff factor for the truncated normal initializer.     tie_word_embeddings (<code>bool</code>, optional, defaults to <code>True</code>):         whether to tie the word embeddings with the output layer.     num_batches (<code>int</code>, optional, defaults to 1):         the number of batches when doing batch correction training.     lambda_adv: float = 1.0,         the weight for the adversarial loss.     grl_alpha: float = 1.0,         the alpha value for the gradient reversal layer.     bc_unfreeze_last_n_layers (<code>int</code>, optional, defaults to 0):         the number of last layers to unfreeze during training for batch correction.     kwargs: (additional keyword arguments, optional):         additional configuration parameters.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.DataCollatorForReplacedTokenDetection","title":"DataCollatorForReplacedTokenDetection","text":"<pre><code>DataCollatorForReplacedTokenDetection(tokenizer, mlm_probability=0.15, seed=None)\n</code></pre> <p>               Bases: <code>WandbMixin</code>, <code>DataCollatorForLanguageModeling</code></p> <p>Like HF\u2019s MLM collator but:   \u2022 never uses [MASK]   \u2022 picks replacement tokens from a user-supplied distribution   \u2022 returns per-token 0/1 labels for ELECTRA-style discrimination</p> <p>Simple data collator for ELECTRA-style token replacement detection. Args:     tokenizer (TrainingTokenizer): The tokenizer to use.     vocab_counts (torch.Tensor | None): 1-D tensor, size == vocab, log-probs OR probs     mlm_probability (float): Probability of masking a token.     seed (int | None): Random seed for reproducibility.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.DataCollatorForReplacedTokenDetection-functions","title":"Functions","text":""},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.DataCollatorForTripletLoss","title":"DataCollatorForTripletLoss","text":"<pre><code>DataCollatorForTripletLoss(tokenizer, max_position_embeddings=None)\n</code></pre> <p>A simple data collator for triplet loss to fine-tune Atacformer for cell-type clustering</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.ModelParameterChangeCallback","title":"ModelParameterChangeCallback","text":"<pre><code>ModelParameterChangeCallback(initial_params)\n</code></pre> <p>               Bases: <code>WandbMixin</code>, <code>TrainerCallback</code></p> <p>A callback to log the changes in model parameters after training.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.ModelParameterChangeCallback-functions","title":"Functions","text":""},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.ModelParameterChangeCallback.on_log","title":"on_log","text":"<pre><code>on_log(args, state, control, **kwargs)\n</code></pre> <p>Log the changes in model parameters after training.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AdjustedRandIndexCallback","title":"AdjustedRandIndexCallback","text":"<pre><code>AdjustedRandIndexCallback(input_ids, cell_type_labels, pad_token_id, batch_size=128, log_every_n_steps=500)\n</code></pre> <p>               Bases: <code>WandbMixin</code>, <code>TrainerCallback</code></p> <p>A callback to log the adjusted Rand index (ARI) during training.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AdjustedRandIndexCallback-functions","title":"Functions","text":""},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.AdjustedRandIndexCallback.on_log","title":"on_log","text":"<pre><code>on_log(args, state, control, **kwargs)\n</code></pre> <p>Log the adjusted Rand index (ARI) during training.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer-functions","title":"Functions","text":""},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.freeze_except_last_n","title":"freeze_except_last_n","text":"<pre><code>freeze_except_last_n(model, n=2)\n</code></pre> <p>Freeze all parameters except the last n layers of the encoder. Also keeps all layer norms trainable for stability.</p> <p>Args:     model (AtacformerModel): The model to freeze.     n (int): The number of last layers to keep trainable.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.patch_atacformer_model_for_mps","title":"patch_atacformer_model_for_mps","text":"<pre><code>patch_atacformer_model_for_mps(model)\n</code></pre> <p>Look for any <code>TransformerEncoder</code> layers in the model and patch them by setting <code>enable_nested_tensor</code> to False and setting <code>use_nested_tensor</code> to False.</p> <p>Args:     model (nn.Module): The model to patch.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.get_git_hash","title":"get_git_hash","text":"<pre><code>get_git_hash()\n</code></pre> <p>Get the current git hash of the repository.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.get_decaying_cosine_with_hard_restarts_schedule_with_warmup","title":"get_decaying_cosine_with_hard_restarts_schedule_with_warmup","text":"<pre><code>get_decaying_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=1, last_epoch=-1)\n</code></pre> <p>Very similar to huggingfaces built-in cosine with restarts, however the amplitude slowly decreases so that the \"kick ups\" are less aggressive.</p> <p>Create a schedule with a learning rate that decreases following the values of the cosine function between the initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases linearly between 0 and the initial lr set in the optimizer.</p> <p>Args:     optimizer ([<code>~torch.optim.Optimizer</code>]):         The optimizer for which to schedule the learning rate.     num_warmup_steps (<code>int</code>):         The number of steps for the warmup phase.     num_training_steps (<code>int</code>):         The total number of training steps.     num_cycles (<code>int</code>, optional, defaults to 1):         The number of hard restarts to use.     last_epoch (<code>int</code>, optional, defaults to -1):         The index of the last epoch when resuming training.</p> <p>Return:     <code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>"},{"location":"geniml/api-reference/atacformer/#geniml.atacformer.tokenize_anndata","title":"tokenize_anndata","text":"<pre><code>tokenize_anndata(adata, tokenizer)\n</code></pre> <p>Tokenize an AnnData object. This is more involved, so it gets its own function.</p> <p>Args:     adata (sc.AnnData): The AnnData object to tokenize.     tokenizer (Tokenizer): The tokenizer to use.</p>"},{"location":"geniml/api-reference/bbclient/","title":"BBClient Module","text":""},{"location":"geniml/api-reference/bbclient/#geniml.bbclient","title":"bbclient","text":""},{"location":"geniml/api-reference/bbclient/#geniml.bbclient-classes","title":"Classes","text":""},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient","title":"BBClient","text":"<pre><code>BBClient(cache_folder=DEFAULT_CACHE_FOLDER, bedbase_api=DEFAULT_BEDBASE_API)\n</code></pre> <p>               Bases: <code>BedCacheManager</code></p> <p>BBClient to deal with download files from bedbase and caching them.</p> <p>Args:     cache_folder (Union[str, os.PathLike]): path to local folder as cache of files from bedbase,         if not given it will be the environment variable <code>BBCLIENT_CACHE</code>     bedbase_api (str): url to bedbase</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient-functions","title":"Functions","text":""},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.load_bedset","title":"load_bedset","text":"<pre><code>load_bedset(bedset_id)\n</code></pre> <p>Load a BEDset from cache, or download and add it to the cache with its BED files.</p> <p>Args:     bedset_id (str): unique identifier of a BED set</p> <p>Returns:     BedSet: the BedSet object</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.load_bed","title":"load_bed","text":"<pre><code>load_bed(bed_id)\n</code></pre> <p>Loads a BED file from cache, or downloads and caches it if it doesn't exist.</p> <p>Args:     bed_id (str): unique identifier of a BED file</p> <p>Returns:     RegionSet: the RegionSet object</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.add_bedset_to_cache","title":"add_bedset_to_cache","text":"<pre><code>add_bedset_to_cache(bedset)\n</code></pre> <p>Add a BED set to the cache.</p> <p>Args:     bedset (BedSet): the BED set to be added, a BedSet class</p> <p>Returns:     str: the identifier if the BedSet object</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.add_bed_to_cache","title":"add_bed_to_cache","text":"<pre><code>add_bed_to_cache(bedfile, force=False)\n</code></pre> <p>Add a BED file to the cache.</p> <p>Args:     bedfile (Union[RegionSet, str]): a RegionSet object or a path or url to the BED file     force (bool): whether to overwrite the existing file in cache</p> <p>Returns:     RegionSet: the RegionSet identifier</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.add_bed_tokens_to_cache","title":"add_bed_tokens_to_cache","text":"<pre><code>add_bed_tokens_to_cache(bed_id, universe_id)\n</code></pre> <p>Add a tokenized BED file to the cache.</p> <p>Args:     bed_id (str): the identifier of the BED file     universe_id (str): the identifier of the universe</p> <p>Returns:     str: the identifier of the tokenized BED file</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.load_bed_tokens","title":"load_bed_tokens","text":"<pre><code>load_bed_tokens(bed_id, universe_id)\n</code></pre> <p>Load a tokenized BED file from cache, or download and cache it if it doesn't exist.</p> <p>Args:     bed_id (str): the identifier of the BED file     universe_id (str): the identifier of the universe</p> <p>Returns:     Array: the zarr array of tokens</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.remove_tokens","title":"remove_tokens","text":"<pre><code>remove_tokens(bed_id, universe_id)\n</code></pre> <p>Remove all tokenized BED files from cache</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.cache_tokens","title":"cache_tokens","text":"<pre><code>cache_tokens(bed_id, universe_id, tokens)\n</code></pre> <p>Cache tokenized BED file.</p> <p>Args:     bed_id (str): the identifier of the BED file     universe_id (str): the identifier of the universe     tokens (Union[list, Array]): the list of tokens</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.add_bed_to_s3","title":"add_bed_to_s3","text":"<pre><code>add_bed_to_s3(identifier, bucket=DEFAULT_BUCKET_NAME, endpoint_url=None, aws_access_key_id=None, aws_secret_access_key=None, s3_path=DEFAULT_BUCKET_FOLDER)\n</code></pre> <p>Add a cached BED file to S3.</p> <p>Args:     identifier (str): the unique identifier of the BED file     bucket (str): the name of the bucket     endpoint_url (str): the URL of the S3 endpoint [Default: set up by the environment vars]     aws_access_key_id (str): the access key of the AWS account [Default: set up by the environment vars]     aws_secret_access_key (str): the secret access key of the AWS account [Default: set up by the environment vars]     s3_path (str): the path on S3</p> <p>Returns:     str: full path on S3</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.get_bed_from_s3","title":"get_bed_from_s3","text":"<pre><code>get_bed_from_s3(identifier, bucket=DEFAULT_BUCKET_NAME, endpoint_url=None, aws_access_key_id=None, aws_secret_access_key=None, s3_path=DEFAULT_BUCKET_FOLDER)\n</code></pre> <p>Get a cached BED file from S3 and cache it locally.</p> <p>Args:     identifier (str): the unique identifier of the BED file     bucket (str): the name of the bucket     endpoint_url (str): the URL of the S3 endpoint [Default: set up by the environment vars]     aws_access_key_id (str): the access key of the AWS account [Default: set up by the environment vars]     aws_secret_access_key (str): the secret access key of the AWS account [Default: set up by the environment vars]     s3_path (str): the path on S3</p> <p>Returns:     str: bed file id</p> <p>Raises:     FileNotFoundError: if the identifier does not exist in cache</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.seek","title":"seek","text":"<pre><code>seek(identifier)\n</code></pre> <p>Get local path to BED file or BED set with specific identifier.</p> <p>Args:     identifier (str): the unique identifier</p> <p>Returns:     str: the local path of the file</p> <p>Raises:     FileNotFoundError: if the identifier does not exist in cache</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.remove_bedset_from_cache","title":"remove_bedset_from_cache","text":"<pre><code>remove_bedset_from_cache(bedset_id, remove_bed_files=False)\n</code></pre> <p>Remove a BED set from cache.</p> <p>Args:     bedset_id (str): the identifier of BED set     remove_bed_files (bool): whether also remove BED files in the BED set</p> <p>Raises:     FileNotFoundError: if the BED set does not exist in cache</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.list_beds","title":"list_beds","text":"<pre><code>list_beds()\n</code></pre> <p>List all BED files in cache.</p> <p>Returns:     Dict[str, str]: the list of identifiers of BED files</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.list_bedsets","title":"list_bedsets","text":"<pre><code>list_bedsets()\n</code></pre> <p>List all BED sets in cache.</p> <p>Returns:     Dict[str, str]: the list of identifiers of BED sets</p>"},{"location":"geniml/api-reference/bbclient/#geniml.bbclient.BBClient.remove_bedfile_from_cache","title":"remove_bedfile_from_cache","text":"<pre><code>remove_bedfile_from_cache(bedfile_id)\n</code></pre> <p>Remove a BED file from cache.</p> <p>Args:     bedfile_id (str): the identifier of BED file</p> <p>Raises:     FileNotFoundError: if the BED set does not exist in cache</p>"},{"location":"geniml/api-reference/bedshift/","title":"BEDshift Module","text":""},{"location":"geniml/api-reference/bedshift/#geniml.bedshift","title":"bedshift","text":""},{"location":"geniml/api-reference/bedshift/#geniml.bedshift-classes","title":"Classes","text":""},{"location":"geniml/api-reference/bedspace/","title":"BEDspace Module","text":""},{"location":"geniml/api-reference/bedspace/#geniml.bedspace","title":"bedspace","text":""},{"location":"geniml/api-reference/craft/","title":"Craft Module","text":""},{"location":"geniml/api-reference/craft/#geniml.craft","title":"craft","text":""},{"location":"geniml/api-reference/craft/#geniml.craft-classes","title":"Classes","text":""},{"location":"geniml/api-reference/craft/#geniml.craft.CraftModel","title":"CraftModel","text":"<pre><code>CraftModel(config)\n</code></pre> <p>               Bases: <code>PreTrainedModel</code></p> <p>CRAFT Model with a masked language modeling head.</p>"},{"location":"geniml/api-reference/craft/#geniml.craft.CraftModel-functions","title":"Functions","text":""},{"location":"geniml/api-reference/craft/#geniml.craft.CraftModel.forward","title":"forward","text":"<pre><code>forward(gene_input_ids=None, gene_attention_mask=None, gene_token_type_ids=None, atac_input_ids=None, atac_attention_mask=None, return_dict=None, **kwargs)\n</code></pre> <p>Forward pass through the CRAFT model.</p> <p>Args:     gene_input_ids (torch.Tensor): Input IDs for the gene encoder.     gene_attention_mask (torch.Tensor): Attention mask for the gene encoder.     gene_token_type_ids (torch.Tensor): Token type IDs for the gene encoder.     atac_input_ids (torch.Tensor): Input IDs for the ATAC encoder.     atac_attention_mask (torch.Tensor): Attention mask for the ATAC encoder.     return_dict (bool): Whether to return a dictionary or tuple. Returns:     torch.Tensor: The logits from the CRAFT model.</p>"},{"location":"geniml/api-reference/craft/#geniml.craft.CraftForContrastiveLearning","title":"CraftForContrastiveLearning","text":"<pre><code>CraftForContrastiveLearning(config)\n</code></pre> <p>               Bases: <code>PreTrainedModel</code></p> <p>CRAFT model for contrastive learning between gene and ATAC embeddings. While this looks redudant with the CraftModel, it makes it easier to use the model for further tasks like gene activity prediction without needing to instantiate the CraftModel directly.</p> <p>Mostly used for pre-training tasks</p>"},{"location":"geniml/api-reference/craft/#geniml.craft.CraftForContrastiveLearning-functions","title":"Functions","text":""},{"location":"geniml/api-reference/craft/#geniml.craft.CraftForContrastiveLearning.forward","title":"forward","text":"<pre><code>forward(gene_input_ids, gene_attention_mask, gene_token_type_ids, atac_input_ids, atac_attention_mask)\n</code></pre> <p>Forward pass through the model.</p> <p>Args:     gene_input_ids (torch.Tensor): Input IDs for the gene encoder.     gene_attention_mask (torch.Tensor): Attention mask for the gene encoder.     gene_token_type_ids (torch.Tensor): Token type IDs for the gene encoder.     atac_input_ids (torch.Tensor): Input IDs for the ATAC encoder.     atac_attention_mask (torch.Tensor): Attention mask for the ATAC encoder.</p> <p>Returns:     CraftOutput: The output of the CRAFT model containing loss and logits.</p>"},{"location":"geniml/api-reference/craft/#geniml.craft.CraftForGeneActivityPrediction","title":"CraftForGeneActivityPrediction","text":"<pre><code>CraftForGeneActivityPrediction(config)\n</code></pre> <p>               Bases: <code>PreTrainedModel</code></p> <p>CRAFT model for gene activity prediction.</p>"},{"location":"geniml/api-reference/craft/#geniml.craft.CraftForGeneActivityPrediction-functions","title":"Functions","text":""},{"location":"geniml/api-reference/craft/#geniml.craft.CraftForGeneActivityPrediction.forward","title":"forward","text":"<pre><code>forward(atac_input_ids, atac_attention_mask, gene_activity=None, return_dict=True)\n</code></pre> <p>Forward pass through the model.</p> <p>Args:     atac_input_ids (torch.Tensor): Input IDs for the ATAC encoder.     atac_attention_mask (torch.Tensor): Attention mask for the ATAC encoder.     gene_activity (Optional[torch.Tensor]): Optional gene activity scores for computing loss. Returns:     torch.Tensor: The predicted gene activity scores.</p>"},{"location":"geniml/api-reference/craft/#geniml.craft.CraftConfig","title":"CraftConfig","text":"<pre><code>CraftConfig(geneformer_config=None, atacformer_config=None, projection_dim=512, logit_scale_init_value=2.6592, **kwargs)\n</code></pre> <p>               Bases: <code>PretrainedConfig</code></p> <p>Configuration for the CRAFT model, a contrastive RNA-ATAC transformer that attempts to learn leverage Geneformer and Atacformer to learn a joint representation of RNA and ATAC data.</p> <p>Joint configuration for the CRAFT model.</p> <p>Args:     geneformer_config (GeneformerConfig): Configuration for the Geneformer model.     atacformer_config (AtacformerConfig): Configuration for the Atacformer model.     projection_dim (int): Dimension of the projection layer.     logit_scale_init_value (float): Initial value for the logit scale parameter.</p>"},{"location":"geniml/api-reference/craft/#geniml.craft.CraftConfig-functions","title":"Functions","text":""},{"location":"geniml/api-reference/craft/#geniml.craft.DataCollatorForCraft","title":"DataCollatorForCraft","text":"<pre><code>DataCollatorForCraft(gene_pad, atac_pad, gene_max_len=None, atac_max_len=None)\n</code></pre> <p>Pads + builds masks for gene/ATAC pairs used by CraftModel.</p>"},{"location":"geniml/api-reference/craft/#geniml.craft.DataCollatorForCraftGeneActivityPrediction","title":"DataCollatorForCraftGeneActivityPrediction","text":"<pre><code>DataCollatorForCraftGeneActivityPrediction(atac_pad, atac_max_len=None)\n</code></pre> <p>Pads + builds masks for ATAC-gene pairs used by CraftForGeneActivityPrediction.</p> <p>Gene activity is always the same shape for everything, its just a set of floats representing the activity of each gene in the genome, so we don't pad it.</p>"},{"location":"geniml/api-reference/eval/","title":"Evaluation Module","text":""},{"location":"geniml/api-reference/eval/#geniml.eval","title":"eval","text":""},{"location":"geniml/api-reference/geneformer/","title":"Geneformer Module","text":""},{"location":"geniml/api-reference/geneformer/#geniml.geneformer","title":"geneformer","text":""},{"location":"geniml/api-reference/geneformer/#geniml.geneformer-classes","title":"Classes","text":""},{"location":"geniml/api-reference/geneformer/#geniml.geneformer.GeneformerConfig","title":"GeneformerConfig","text":"<pre><code>GeneformerConfig(vocab_size=20275, hidden_size=512, intermediate_size=1024, num_attention_heads=8, num_hidden_layers=12, attention_probs_dropout_prob=0.02, hidden_act='relu', hidden_dropout_prob=0.02, initializer_range=0.02, layer_norm_eps=1e-12, max_position_embeddings=4096, pad_token_id=0, classifier_dropout=None, **kwargs)\n</code></pre> <p>               Bases: <code>BertConfig</code></p> <p>Configuration for Geneformer model, a BERT-like transformer for gene tokens.</p>"},{"location":"geniml/api-reference/geneformer/#geniml.geneformer.GeneformerModel","title":"GeneformerModel","text":"<pre><code>GeneformerModel(config)\n</code></pre> <p>               Bases: <code>BertForMaskedLM</code></p> <p>Geneformer Model with a masked language modeling head.</p>"},{"location":"geniml/api-reference/geneformer/#geniml.geneformer.GeneformerModel-functions","title":"Functions","text":""},{"location":"geniml/api-reference/geneformer/#geniml.geneformer.GeneformerModel.get_input_embeddings","title":"get_input_embeddings","text":"<pre><code>get_input_embeddings()\n</code></pre> <p>Returns the input embeddings of the model.</p>"},{"location":"geniml/api-reference/geneformer/#geniml.geneformer.GeneformerModel.set_input_embeddings","title":"set_input_embeddings","text":"<pre><code>set_input_embeddings(value)\n</code></pre> <p>Sets the input embeddings of the model.</p>"},{"location":"geniml/api-reference/geneformer/#geniml.geneformer.TranscriptomeTokenizer","title":"TranscriptomeTokenizer","text":"<pre><code>TranscriptomeTokenizer(custom_attr_name_dict=None, nproc=1, chunk_size=512, model_input_size=4096, special_token=True, collapse_gene_ids=True, gene_median_file=None, token_dictionary_file=None, gene_mapping_file=None)\n</code></pre> <p>Initialize tokenizer.</p> <p>Parameters:</p> <p>custom_attr_name_dict : None, dict     | Dictionary of custom attributes to be added to the dataset.     | Keys are the names of the attributes in the loom file.     | Values are the names of the attributes in the dataset. nproc : int     | Number of processes to use for dataset mapping. chunk_size : int = 512     | Chunk size for anndata tokenizer. model_input_size : int = 4096     | Max input size of model to truncate input to.     | For the 30M model series, should be 2048. For the 95M model series, should be 4096. special_token : bool = True     | Adds CLS token before and EOS token after rank value encoding.     | For the 30M model series, should be False. For the 95M model series, should be True. collapse_gene_ids : bool = True     | Whether to collapse gene IDs based on gene mapping dictionary. gene_median_file : Path     | Path to pickle file containing dictionary of non-zero median     | gene expression values across Genecorpus-30M. token_dictionary_file : Path     | Path to pickle file containing token dictionary (Ensembl IDs:token). gene_mapping_file : None, Path     | Path to pickle file containing dictionary for collapsing gene IDs.</p>"},{"location":"geniml/api-reference/geneformer/#geniml.geneformer.TranscriptomeTokenizer-functions","title":"Functions","text":""},{"location":"geniml/api-reference/geneformer/#geniml.geneformer.TranscriptomeTokenizer.tokenize_data","title":"tokenize_data","text":"<pre><code>tokenize_data(data_directory, output_directory, output_prefix, file_format='loom', use_generator=False)\n</code></pre> <p>Tokenize .loom files in data_directory and save as tokenized .dataset in output_directory.</p> <p>Parameters:</p> <p>data_directory : Path     | Path to directory containing loom files or anndata files output_directory : Path     | Path to directory where tokenized data will be saved as .dataset output_prefix : str     | Prefix for output .dataset file_format : str     | Format of input files. Can be \"loom\" or \"h5ad\". use_generator : bool     | Whether to use generator or dict for tokenization.</p>"},{"location":"geniml/api-reference/io/","title":"I/O Module","text":""},{"location":"geniml/api-reference/io/#geniml.io","title":"io","text":""},{"location":"geniml/api-reference/io/#geniml.io-classes","title":"Classes","text":""},{"location":"geniml/api-reference/io/#geniml.io.BedSet","title":"BedSet","text":"<pre><code>BedSet(region_sets=None, file_path=None, identifier=None)\n</code></pre> <p>BedSet object</p> <p>Initialize a BedSet object.</p> <p>Args:     region_sets: list of BED file paths, RegionSet, or 2-dimension list of Region. Defaults to None (empty BedSet).     file_path (str): path to the .txt file with identifier of all BED files in it     identifier (str): the identifier of the BED set</p>"},{"location":"geniml/api-reference/io/#geniml.io.BedSet-functions","title":"Functions","text":""},{"location":"geniml/api-reference/io/#geniml.io.BedSet.add","title":"add","text":"<pre><code>add(bedfile)\n</code></pre> <p>Add a BED file to the BED set.</p> <p>Warning: if new bedfile will be added, bedSet identifier will be changed!</p> <p>Args:     bedfile (RegionSet): RegionSet instance, that should be added to the bedSet</p>"},{"location":"geniml/api-reference/io/#geniml.io.BedSet.compute_bedset_identifier","title":"compute_bedset_identifier","text":"<pre><code>compute_bedset_identifier()\n</code></pre> <p>Return the identifier. If it is not set, compute one.</p> <p>Returns:     str: the identifier of BED set</p>"},{"location":"geniml/api-reference/io/#geniml.io.RegionSet","title":"RegionSet","text":"<pre><code>RegionSet(regions, backed=False)\n</code></pre> <p>Instantiate a RegionSet object. This can be backed or not backed. It represents a set of genomic regions.</p> <p>If you specify <code>backed</code> as True, then the bed file will not be loaded into memory. This is useful for large bed files. You can still iterate over the regions, but you cannot index into them.</p> <p>Args:     regions: path, or url to bed file or list of Region objects     backed (bool): whether to load the bed file into memory or not. Defaults to False.</p>"},{"location":"geniml/api-reference/io/#geniml.io.RegionSet-functions","title":"Functions","text":""},{"location":"geniml/api-reference/io/#geniml.io.RegionSet.to_granges","title":"to_granges","text":"<pre><code>to_granges()\n</code></pre> <p>Return GenomicRanges contained in this BED file.</p> <p>Returns:     genomicranges.GenomicRanges: GenomicRanges object</p>"},{"location":"geniml/api-reference/io/#geniml.io.RegionSet.compute_bed_identifier","title":"compute_bed_identifier","text":"<pre><code>compute_bed_identifier()\n</code></pre> <p>Return bed file identifier. If it is not set, compute one.</p> <p>Returns:     str: the identifier of BED file</p>"},{"location":"geniml/api-reference/io/#geniml.io.Region","title":"Region","text":"<pre><code>Region(chr, start, stop)\n</code></pre> <p>Instantiate a Region object.</p> <p>Args:     chr (str): chromosome     start (int): start position     stop (int): stop position</p>"},{"location":"geniml/api-reference/io/#geniml.io.Region-functions","title":"Functions","text":""},{"location":"geniml/api-reference/likelihood/","title":"Likelihood Module","text":""},{"location":"geniml/api-reference/likelihood/#geniml.likelihood","title":"likelihood","text":""},{"location":"geniml/api-reference/likelihood/#geniml.likelihood-functions","title":"Functions","text":""},{"location":"geniml/api-reference/models/","title":"Models Module","text":""},{"location":"geniml/api-reference/models/#geniml.models","title":"models","text":""},{"location":"geniml/api-reference/models/#geniml.models-classes","title":"Classes","text":""},{"location":"geniml/api-reference/region2vec/","title":"Region2Vec Module","text":""},{"location":"geniml/api-reference/region2vec/#geniml.region2vec","title":"region2vec","text":""},{"location":"geniml/api-reference/scembed/","title":"scEmbed Module","text":""},{"location":"geniml/api-reference/scembed/#geniml.scembed","title":"scembed","text":""},{"location":"geniml/api-reference/search/","title":"Search Module","text":""},{"location":"geniml/api-reference/search/#geniml.search","title":"search","text":""},{"location":"geniml/api-reference/search/#geniml.search-classes","title":"Classes","text":""},{"location":"geniml/api-reference/search/#geniml.search-functions","title":"Functions","text":""},{"location":"geniml/api-reference/text2bednn/","title":"Text2BedNN Module","text":""},{"location":"geniml/api-reference/text2bednn/#geniml.text2bednn","title":"text2bednn","text":""},{"location":"geniml/api-reference/text2bednn/#geniml.text2bednn-classes","title":"Classes","text":""},{"location":"geniml/api-reference/text2bednn/#geniml.text2bednn-functions","title":"Functions","text":""},{"location":"geniml/api-reference/tokenization/","title":"Tokenization Module","text":""},{"location":"geniml/api-reference/tokenization/#geniml.tokenization","title":"tokenization","text":""},{"location":"geniml/api-reference/universe/","title":"Universe Module","text":""},{"location":"geniml/api-reference/universe/#geniml.universe","title":"universe","text":""},{"location":"geniml/api-reference/universe/#geniml.universe-functions","title":"Functions","text":""},{"location":"geniml/api-reference/utils/","title":"Utils Module","text":""},{"location":"geniml/api-reference/utils/#geniml.utils","title":"utils","text":""},{"location":"geniml/api-reference/utils/#geniml.utils-functions","title":"Functions","text":""},{"location":"geniml/api-reference/utils/#geniml.utils.find_path","title":"find_path","text":"<pre><code>find_path(hierarchy, path, cell_type)\n</code></pre> <p>Find the path from the root to a given cell type in a hierarchy.</p> <p>Args:     hierarchy: A dictionary representing the hierarchy.     path: The current path.     cell_type: The cell type to find.</p> <p>Returns:     The path from the root to the cell type (a list of strings or None).</p>"},{"location":"geniml/api-reference/utils/#geniml.utils.find_lca","title":"find_lca","text":"<pre><code>find_lca(path1, path2)\n</code></pre> <p>Find the lowest common ancestor (LCA) of two paths.</p> <p>Args:     path1: The first path.     path2: The second path.</p> <p>Returns:     int: The index of the lowest common ancestor in the paths.</p>"},{"location":"geniml/api-reference/utils/#geniml.utils.compute_cell_hierarchy_distance","title":"compute_cell_hierarchy_distance","text":"<pre><code>compute_cell_hierarchy_distance(hierarchy, cell1, cell2)\n</code></pre> <p>Compute the distance between two cell types in a hierarchy.</p> <p>The distance is the number of edges between the two cells in the hierarchy.</p> <p>Args:     hierarchy: A dictionary representing the hierarchy.     cell1: The first cell type.     cell2: The second cell type.</p> <p>Returns:     The distance between the two cell types (an integer or None).</p>"},{"location":"geniml/bbclient/bbclient/","title":"BBClient","text":"<p>The BEDbase client command <code>bbclient</code> downloads, processes, and caches BED files and BED sets from the BEDbase API and converts them into RegionSet, GenomicRanges or GenomicRangesList objects. It provides various commands to interact with BED files, including downloading individual files, downloading BEDsets, processing local BED files, and processing BED file identifiers.</p> <p>This document provides tutorials for using <code>bbclient</code> via either:</p> <ol> <li>the \ud83d\udc0d Python interface,</li> <li>the \ud83d\udda5\ufe0f command-line interface, or </li> <li>the \ud83e\udd80 Rust interface.</li> </ol>"},{"location":"geniml/bbclient/bbclient/#python-interface","title":"\ud83d\udc0d Python interface","text":""},{"location":"geniml/bbclient/bbclient/#instance-of-bbclient","title":"Instance of BBClient:","text":"<p>First, we need to create an instance of the <code>BBClient</code> class. Optionally, you can specify a custom cache folder to store downloaded BED files. If not specified, it defaults to <code>./.bb_cache</code>. <pre><code>from geniml.bbclient import BBClient\n\nbbclient = BBClient(cache_folder=\"cache\")\n</code></pre></p>"},{"location":"geniml/bbclient/bbclient/#load-a-bed-file-from-cache-or-download-and-cache-a-remote-bed-file-from-bedbase","title":"Load a BED file from cache, or download and cache a remote BED file from BEDbase","text":"<p>THis function will try to find the BED file in the cache first. If not found, it will download it from BEDbase, process it, cache it, and return a <code>RegionSet</code> object. <pre><code>bedfile_id = \"....\"  # find interesting bedfile on bedbase e.g. \"3f0aaadf6e8854282f21ea19ab5c061a\"\nbedfile = bbclient.load_bed(bedfile_id)  # download, cache and return a RegionSet object\n</code></pre></p>"},{"location":"geniml/bbclient/bbclient/#cache-a-local-bed-file","title":"Cache a local BED file","text":"<p>You can just provide a URL, Path or RegionSet object and it will add to cache for you: <pre><code>from gtars.models import RegionSet\n\nbedfile = RegionSet(\"path/to/bedfile\")\n\n# Alternatively, you can provide a URL:\n# bedfile = \"https://example.com/path/to/bedfile.bed.gz\"\n# Or a local file path:\n# bedfile = \"path/to/bedfile\"\nbedfile_id = bbclient.add_bed_to_cache(bedfile)\n</code></pre></p>"},{"location":"geniml/bbclient/bbclient/#download-and-cache-a-bedset-from-bedbase","title":"Download and cache a BEDset from BEDbase","text":"<pre><code>bedset_identifier = \"xyz\" # find some interesting bedset on bedbase.org\nbedset = bbclient.load_bedset(bedset_identifier)  # download, cache and return a BedSet object\ngrl = bedset.to_granges_list()  # return a GenomicRangesList object\n</code></pre>"},{"location":"geniml/bbclient/bbclient/#cache-a-local-bedset","title":"Cache a local BEDset","text":"<pre><code>from geniml.io import BedSet\n\nbedset_folder = \"path/to/bed/files/folder\"\nbedset = BedSet(\n    [os.path.join(bedset-folder, file_name) for file_name in os.listdir(bedset_folder)]\n)\nbedset_id = bbclient.add_bedset_to_cache(bedset)\n</code></pre>"},{"location":"geniml/bbclient/bbclient/#command-line-interface","title":"\ud83d\udda5\ufe0f Command line interface","text":""},{"location":"geniml/bbclient/bbclient/#cache-bed-file","title":"Cache BED file","text":"<p><pre><code>geniml bbclient cache-bed &lt;BED_file_or_identifier_or_url&gt;\n</code></pre> The <code>&lt;BED_file_or_identifier_or_url&gt;</code> variable can be one of 3 things:</p> <ol> <li>a path to a local BED file;</li> <li>a BED record identifier from BEDbase; or,</li> <li>a URL to a BED file hosted anywhere.</li> </ol>"},{"location":"geniml/bbclient/bbclient/#cache-bedset","title":"Cache BEDset","text":"<p><pre><code>geniml bbclient cache-bedset &lt;BED_files_folder_or_identifier&gt;\n</code></pre> The <code>&lt;BED_files_folder_or_identifier&gt;</code> variable may be:</p> <ol> <li>local path to a folder containing BED files; or,</li> <li>a BEDbase BEDset identifier</li> </ol>"},{"location":"geniml/bbclient/bbclient/#seek-the-path-of-a-bed-file-or-bedset-in-cache-folder","title":"Seek the path of a BED file or BEDset in cache folder","text":"<p>To retrieve the local file path to a BED file stored locally,</p> <p><pre><code>geniml bbclient seek &lt;identifier&gt;\n</code></pre> where  is the BED file or BEDset identifier."},{"location":"geniml/bbclient/bbclient/#count-bed-files-that-are-cached","title":"Count BED files that are cached","text":"<pre><code>geniml bbclient inspect-bedfiles\n</code></pre>"},{"location":"geniml/bbclient/bbclient/#count-bedsets-that-are-cached","title":"Count BEDsets that are cached","text":"<pre><code>geniml bbclient inspect-bedsets\n</code></pre>"},{"location":"geniml/bbclient/bbclient/#remove-a-bed-file-or-bedset-from-the-cache-folder","title":"Remove a BED file or BEDset from the cache folder","text":"<p><pre><code>geniml bbclient rm &lt;identifier&gt;\n</code></pre> where  is the BED file or BEDset identifier."},{"location":"geniml/bbclient/bbclient/#rust-interface","title":"\ud83e\udd80 Rust interface","text":""},{"location":"geniml/bbclient/bbclient/#install-gtars","title":"Install gtars","text":"<pre><code>cargo install gtars\n</code></pre>"},{"location":"geniml/bbclient/bbclient/#cache-a-bed-file","title":"Cache a BED file","text":"<pre><code>use gtars::bbclient::BBClient;\n\nlet mut bbc = BBClient::new(Some(cache_folder.clone()), None).expect(\"Failed to create BBClient\");\n</code></pre> <p>where first argument is an optional cache folder path, and the second argument is an optional BEDbase API URL.</p>"},{"location":"geniml/bbclient/bbclient/#add-local-bed-file-to-cache","title":"Add local BED file to cache","text":"<pre><code>let bed_id: String = bbc\n            .add_local_bed_to_cache(PathBuf::from(_path/to.bed.gz), None)\n            .unwrap();\n</code></pre>"},{"location":"geniml/bbclient/bbclient/#add-regionset-object-to-cache","title":"Add RegionSet object to cache","text":"<pre><code>use gtars::models::RegionSet;\nlet rs: RegionSet = RegionSet::try_from(\"path/to/bedfile\").unwrap();\nlet bed_id: String = bbc\n            .add_regionset_to_cache(rs, None)\n            .unwrap();\n</code></pre>"},{"location":"geniml/bbclient/bbclient/#delete-a-bed-file-from-cache","title":"Delete a BED file from cache","text":"<pre><code>let result: Result&lt;()&gt; = bbc.remove(\"bed_id\")\n            .expect(\"Failed to remove bedset file and its bed files\");\n</code></pre>"},{"location":"geniml/bbclient/bbclient/#cache-folder","title":"\ud83d\udcc2 Cache Folder","text":"<p>By default, the downloaded and cached BED files are cached in the <code>./.bbcache</code> folder.  You can specify a different cache folder using the --cache-folder argument, or set the environment variable <code>BBCLIENT_CACHE</code>.</p> <pre><code>export BBCLIENT_CACHE=\"/path/to/cache/folder\"\n</code></pre>"},{"location":"geniml/bbclient/bbclient/#bedbase-api","title":"\ud83d\udd17 BEDbase API","text":"<p>By default the <code>bbclient</code> connects to the public BEDbase instance at <code>https://api.bedbase.org</code>. To set a different BEDbase instance set the environment variable <code>BEDBASE_API</code>: <pre><code>export BEDBASE_API=\"https://your.bedbase.instance/api\"\n</code></pre></p>"},{"location":"geniml/bbclient/pyBiocFileCache/","title":"How to use bbclient cache in R","text":"<p>The bbclient caching system is supported in both Python and R.  Both of these caching systems are compatible with <code>BiocFileCache</code> for BED files and <code>Zarr</code> with BED tokens.</p>"},{"location":"geniml/bbclient/pyBiocFileCache/#bed-caching","title":"Bed caching","text":""},{"location":"geniml/bbclient/pyBiocFileCache/#step-1-cache-bed-file-using-bbclient-cli","title":"Step 1: Cache BED File Using bbclient CLI","text":"<pre><code>geniml bbclient cache-bed bbad85f21962bb8d972444f7f9a3a932\n</code></pre> <p>The cached file is saved in the default bbclient cache folder, or a user-provided cache folder.  Since we didn't provide a caching folder, the default folder will be in our home directory.</p>"},{"location":"geniml/bbclient/pyBiocFileCache/#step-2-get-path-to-the-bed-file-using-python","title":"Step 2: Get Path to the BED File Using Python","text":""},{"location":"geniml/bbclient/pyBiocFileCache/#a-using-bbclient-in-python","title":"a. Using bbclient in Python","text":"<p>Here's an example of how you can retrieve the path to the BED file in Python: <pre><code>from geniml.bbclient import BBClient\n\nbbc = BBClient()\nprint(bbc.seek(\"bbad85f21962bb8d972444f7f9a3a932\"))\n</code></pre></p> <p>And it will print path to our file: <pre><code>'/home/bnt4me/.bbcache/bedfiles/b/b/bbad85f21962bb8d972444f7f9a3a932.bed.gz'\n</code></pre></p> <p>b. Using pyBiocFileCache</p> <p><pre><code>import os\n\nfrom pybiocfilecache import BiocFileCache\n\n# get cache folder\nbbcache_folder = os.path.join(os.path.expanduser(\"~\"), \".bbcache\")\n\n# get bedfile cache folder \nbedfile_cache_folder = os.path.join(bbcache_folder, \"bedfiles\")\n\nbio_cache = BiocFileCache(bedfile_cache_folder)\n\nbed_cache_obj = bio_cache.get(\"bbad85f21962bb8d972444f7f9a3a932\")\n\nprint(bed_cache_obj.fpath)\n</code></pre> And it will print path to our file: <pre><code>'/home/bnt4me/.bbcache/bedfiles/b/b/bbad85f21962bb8d972444f7f9a3a932.bed.gz'\n</code></pre></p> <ol> <li>Get path using R caching system - BiocFileCache</li> </ol> <p><pre><code># Install necessary package\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n  install.packages(\"BiocManager\")\n}\nBiocManager::install(\"BiocFileCache\")\n\n# Load necessary libraries\nlibrary(BiocFileCache)\n\n# Get cache folder\nbbcache_folder &lt;- file.path(Sys.getenv(\"HOME\"), \".bbcache\")\n\n# Get bedfile cache folder\nbedfile_cache_folder &lt;- file.path(bbcache_folder, \"bedfiles\")\n\n# Create a BiocFileCache object\nbio_cache &lt;- BiocFileCache(bedfile_cache_folder)\n\n# Get the cached file using the identifier\nbed_cache_obj &lt;- bfcrpath(bio_cache, \"bbad85f21962bb8d972444f7f9a3a932\")\n\n# Print the file path\nprint(bed_cache_obj)\n</code></pre> And we will get this message printed: <pre><code>[1] \"/home/bnt4me/.bbcache/bedfiles/b/b/bbad85f21962bb8d972444f7f9a3a932.bed.gz\"\n</code></pre></p> <p>This R script will perform the same tasks as the Python script: handling the cache and retrieving the specified BED file from the cache.</p> <p>By following these steps, you can efficiently manage and retrieve cached BED files using the bbclient caching system in both Python and R.</p> <p>P.S. All links will be different on your machine, as they are generated based on your local home directory.</p>"},{"location":"geniml/bbclient/pyBiocFileCache/#caching-of-tokenized-bed-files","title":"Caching of tokenized bed files","text":"<p>To store tokenized BED files, we use the Zarr format.  BBClient saves tokenized files in the zarr folder within the bbcache folder (which is located in our home directory if the bbcache folder is not specified).</p> <p>Here's an example of how you can download and cache tokenized bed file using bbclient: <pre><code>from geniml.bbclient import BBClient\n\nbbc = BBClient()\nbbc.add_bed_tokens_to_cache( bed_id= '0dcdf8986a72a3d85805bbc9493a1302', universe_id= '58dee1672b7e581c8e1312bd4ca6b3c7')\n</code></pre> If user didn't get any error, the tokenized file is saved in the default bbclient cache folder, or a user-provided cache folder.</p>"},{"location":"geniml/bbclient/pyBiocFileCache/#step-1-get-zarr-tokenized-bed-file-using-bbclient-in-python","title":"Step 1: Get zarr tokenized bed file using bbclient in Python","text":"<p><pre><code>from geniml.bbclient import BBClient\n\nbbc = BBClient()\ntokens_arr = bbc.load_bed_tokens(bed_id= '0dcdf8986a72a3d85805bbc9493a1302', universe_id= '58dee1672b7e581c8e1312bd4ca6b3c7')\n\nprint(tokens_arr)\n</code></pre> Result is a zarr array object: <pre><code>&lt;zarr.core.Array '/58dee1672b7e581c8e1312bd4ca6b3c7/0dcdf8986a72a3d85805bbc9493a1302' (29438,) int64&gt;\n</code></pre></p>"},{"location":"geniml/bbclient/pyBiocFileCache/#step-2-get-zarr-tokenized-bed-file-using-python-zarr-library","title":"Step 2: Get zarr tokenized bed file using Python zarr library","text":"<p><pre><code>import os \nimport zarr\n\n\nbbcache_folder = os.path.join(os.path.expanduser(\"~\"), \".bbcache\")\n\n# get zarr cache folder \nbedfile_cache_folder = os.path.join(bbcache_folder, \"tokens.zarr\")\n\nzarr_cache = zarr.group(bedfile_cache_folder)\n\nuniverse_id = \"58dee1672b7e581c8e1312bd4ca6b3c7\"\nbed_id = \"0dcdf8986a72a3d85805bbc9493a1302\"\n\ntokens_arr = zarr_cache[universe_id][bed_id]\nprint(tokens_arr)\n</code></pre> Result is a zarr array object: <pre><code>&lt;zarr.core.Array '/58dee1672b7e581c8e1312bd4ca6b3c7/0dcdf8986a72a3d85805bbc9493a1302' (29438,) int64&gt;\n</code></pre></p>"},{"location":"geniml/bbclient/pyBiocFileCache/#step-3-get-zarr-tokenized-bed-file-using-r-zarr-library","title":"Step 3: Get zarr tokenized bed file using R zarr library","text":"<p><pre><code>## we need BiocManager to perform the installation\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n## install Rarr\nBiocManager::install(\"Rarr\")\n\nlibrary(Rarr)\n</code></pre> \ud83d\udea7 Use <code>Rarr</code> library to open zarr file: https://github.com/grimbough/Rarr?tab=readme-ov-file</p>"},{"location":"geniml/notebooks/assess-universe/","title":"Assess universe fit","text":"In\u00a0[1]: Copied! <pre>from geniml.assess.assess import get_rbs_from_assessment_file, get_f_10_score_from_assessment_file\nimport pandas as pd\n\nassessment_file_path = \"test_assess_data.csv\"\ndf = pd.read_csv(assessment_file_path)\ndf.head()\n</pre> from geniml.assess.assess import get_rbs_from_assessment_file, get_f_10_score_from_assessment_file import pandas as pd  assessment_file_path = \"test_assess_data.csv\" df = pd.read_csv(assessment_file_path) df.head() Out[1]: file univers/file file/universe universe&amp;file median_dist_file_to_universe median_dist_file_to_universe_flex median_dist_universe_to_file median_dist_universe_to_file_flex 0 test_1.bed 2506 403 3630 27.0 0.0 76.5 0.0 1 test_2.bed 1803 146 4333 27.0 0.0 70.0 7.5 2 test_3.bed 2949 0 3187 28.0 0.0 225.0 224.5 3 test_4.bed 2071 546 4065 27.0 0.0 116.5 105.5 In\u00a0[2]: Copied! <pre>rbs = get_rbs_from_assessment_file(assessment_file_path)\nf_10 = get_f_10_score_from_assessment_file(assessment_file_path)\nrbs_flex = get_rbs_from_assessment_file(assessment_file_path, flexible=True)\nprint(f\"Universe \\nF10: {f_10:.2f}\\nRBS: {rbs:.2f}\\nflexible RBS: {rbs_flex:.2f}\")\n</pre> rbs = get_rbs_from_assessment_file(assessment_file_path) f_10 = get_f_10_score_from_assessment_file(assessment_file_path) rbs_flex = get_rbs_from_assessment_file(assessment_file_path, flexible=True) print(f\"Universe \\nF10: {f_10:.2f}\\nRBS: {rbs:.2f}\\nflexible RBS: {rbs_flex:.2f}\") <pre>Universe \nF10: 0.93\nRBS: 0.77\nflexible RBS: 0.98\n</pre> <p>Or all of this metrics can be directly calculated from the universe and raw files including a likelihood score (LH):</p> In\u00a0[\u00a0]: Copied! <pre>from geniml.assess.assess import get_f_10_score\n\nf10 = get_f_10_score(\n    \"raw/\",\n    'file_list.txt',\n    \"universe_hmm.bed\",\n    1)\n\nf\"Universe F10: {f10:.2f}\"\n</pre> from geniml.assess.assess import get_f_10_score  f10 = get_f_10_score(     \"raw/\",     'file_list.txt',     \"universe_hmm.bed\",     1)  f\"Universe F10: {f10:.2f}\" Out[\u00a0]: <pre>'Universe F10: 0.93'</pre> In\u00a0[\u00a0]: Copied! <pre>from geniml.assess.assess import get_mean_rbs\nrbs = get_mean_rbs(\"raw/\",\n    'file_list.txt',\n    \"universe_hmm.bed\", 1)\nf\"Universe RBS: {rbs:.2f}\"\n</pre> from geniml.assess.assess import get_mean_rbs rbs = get_mean_rbs(\"raw/\",     'file_list.txt',     \"universe_hmm.bed\", 1) f\"Universe RBS: {rbs:.2f}\" Out[\u00a0]: <pre>'Universe RBS: 0.77'</pre> In\u00a0[\u00a0]: Copied! <pre>from geniml.assess.assess import get_likelihood\nlh = get_likelihood(\n    \"model.tar\",\n    \"universe_hmm.bed\",\n    \"coverage/\"\n)\nf\"Universe LH: {lh:.2f}\" \n</pre> from geniml.assess.assess import get_likelihood lh = get_likelihood(     \"model.tar\",     \"universe_hmm.bed\",     \"coverage/\" ) f\"Universe LH: {lh:.2f}\"  Out[\u00a0]: <pre>'Universe LH: -127156.87'</pre> <p>Both region boundary score and likelihood can be also calculated taking into account universe flexibility:</p> In\u00a0[\u00a0]: Copied! <pre>from geniml.assess.assess import get_mean_rbs\nrbs_flex = get_mean_rbs(\n    \"raw/\",\n    'file_list.txt',\n    \"universe_hmm.bed\",\n    1,\n    flexible=True)\nf\"Universe flexible RBS: {rbs_flex:.2f}\"\n</pre> from geniml.assess.assess import get_mean_rbs rbs_flex = get_mean_rbs(     \"raw/\",     'file_list.txt',     \"universe_hmm.bed\",     1,     flexible=True) f\"Universe flexible RBS: {rbs_flex:.2f}\" Out[\u00a0]: <pre>'Universe flexible RBS: 0.98'</pre> In\u00a0[\u00a0]: Copied! <pre>lh_flex = get_likelihood(\n    \"model.tar\",\n    \"universe_hmm.bed\",\n    \"coverage/\"\n)\nf\"Universe flexible LH: {lh_flex:.2f}\" \n</pre> lh_flex = get_likelihood(     \"model.tar\",     \"universe_hmm.bed\",     \"coverage/\" ) f\"Universe flexible LH: {lh_flex:.2f}\"  Out[\u00a0]: <pre>'Universe flexible LH: -127156.87'</pre>"},{"location":"geniml/notebooks/assess-universe/#how-to-assess-universe-fit-to-collection-of-bed-files","title":"How to assess universe fit to collection of BED files\u00b6","text":""},{"location":"geniml/notebooks/assess-universe/#introduction","title":"Introduction\u00b6","text":"<p>In this tutorial, you will see how to assess a fit of a given universe to a collection of files. (Tutorial on creating different universes from files can be found here and here.) Choosing, which universe represents data the best can be challenging. To help with this decision we created three different metrics for assessing universe fit to the region collections: a base-level overlap score, a region boundary score, and a likelihood score. Fit of a universe can be assessed both using CLI and python functions depending on use case. With CLI you can create a file with values of universe assessment methods for each file within the collection, while with python functions you can get measures of universe fit to the whole collection.</p>"},{"location":"geniml/notebooks/assess-universe/#cli","title":"CLI\u00b6","text":"<p>Using CLI you can calculate both base-level overlap score and region boundary score separately for each file in the collections and than summarized. To calculate them you need raw files as well as the analyzed universe. It is also necessary to choose at least one assessment metric to be calculated:</p> <ul> <li><code>--overlap</code> - to calculate base pair overlap between universe and regions in the file, number of base pair only in the universe, number of base pair only in the file, which can be used to calculate F10 score;</li> <li><code>--distance</code> - to calculate median of distance form regions in the raw file to the universe;</li> <li><code>--distance-universe-to-file</code> - to calculate median of distance form the universe to regions in the raw file;</li> <li><code>--distance-flexible</code> - to calculate median of distance form regions in the raw file to the universe taking into account universe flexibility;</li> <li><code>--distance-flexible-universe-to-file</code> - - to calculate median of distance form the universe to regions in the raw file taking into account universe flexibility.</li> </ul> <p>Here we present an example, which calculates all possible metrics for HMM universe:</p> <pre><code> geniml assess-universe --raw-data-folder raw/ \\\n --file-list file_list.txt \\\n --universe universe_hmm.bed \\\n --folder-out . \\\n --pref test_assess \\\n --overlap \\\n --distance \\\n --distance-universe-to-file \\\n --distance-flexible \\\n --distance-flexible-universe-to-file\n</code></pre> <p>The resulting file is called test_assess_data.csv, and contains columns with the raw calculated metrics for each file: file, univers/file, file/universe, universe&amp;file, median_dist_file_to_universe, median_dist_file_to_universe_flex, median_dist_universe_to_file, median_dist_universe_to_file_flex.</p>"},{"location":"geniml/notebooks/assess-universe/#python-functions","title":"Python functions\u00b6","text":"<p>The file created with CLI can be further summarized into specific metrics assessing the fit of a universe to a whole collection such as: a base-level overlap score (F10), a region boundary distance score (RBD).</p>"},{"location":"geniml/notebooks/bedspace-analysis/","title":"BEDspace results notebook","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport glob\nimport numpy as np\nimport re\n</pre> import pandas as pd import glob import numpy as np import re In\u00a0[2]: Copied! <pre>import matplotlib\nmatplotlib.rcParams[\"svg.fonttype\"] = \"none\"\nmatplotlib.rcParams[\"text.usetex\"] = False\nimport matplotlib.pyplot as plt\n</pre> import matplotlib matplotlib.rcParams[\"svg.fonttype\"] = \"none\" matplotlib.rcParams[\"text.usetex\"] = False import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre># Path to the pre-calculated distance file between label embedding and region set embeddings\npath_simfile = './distance_l2r.csv'\ndistance = pd.read_csv(path_simfile)\ndistance.file_label = distance.file_label.str.lower()\ndistance.search_term = distance.search_term.str.lower()\ndistance = distance.drop_duplicates()\n</pre> # Path to the pre-calculated distance file between label embedding and region set embeddings path_simfile = './distance_l2r.csv' distance = pd.read_csv(path_simfile) distance.file_label = distance.file_label.str.lower() distance.search_term = distance.search_term.str.lower() distance = distance.drop_duplicates() In\u00a0[4]: Copied! <pre># Print the search terms (labels)\nprint(distance.search_term.unique())\n</pre> # Print the search terms (labels) print(distance.search_term.unique()) <pre>['h3k4me3' 'h3k27me3' 'h3k27ac' 'h3k4me1' 'h3k9me3' 'h3k4me2' 'h3k9ac'\n 'h3k79me2' 'h4k20me1' 'h3k9me2' 'h3k9me1']\n</pre> In\u00a0[5]: Copied! <pre>def S1(searchterm, distance):\n    nof = len(distance[distance.file_label.str.contains(searchterm)])\n    df = distance[distance.search_term == searchterm].sort_values(by=['score'], ascending = False)[0:10]\n    df = df.sort_values(by=['score'], ascending=True)\n    df['color']='gray'\n    df.loc[df.file_label.str.contains(searchterm), 'color'] = 'green'\n    if(len(df[df.color == 'green']) == nof):\n        df.loc[(df.color!='green'), 'color'] = 'gray'\n        \n    plt= df.plot.barh(x='filename', y='score', figsize=(10,7), fontsize=16, color=list(df['color']))\n    plt.set_xlabel('Search term:' + searchterm, fontsize=15)\n\n    plt.axis(xmin=0.5, xmax=1.01)\n</pre> def S1(searchterm, distance):     nof = len(distance[distance.file_label.str.contains(searchterm)])     df = distance[distance.search_term == searchterm].sort_values(by=['score'], ascending = False)[0:10]     df = df.sort_values(by=['score'], ascending=True)     df['color']='gray'     df.loc[df.file_label.str.contains(searchterm), 'color'] = 'green'     if(len(df[df.color == 'green']) == nof):         df.loc[(df.color!='green'), 'color'] = 'gray'              plt= df.plot.barh(x='filename', y='score', figsize=(10,7), fontsize=16, color=list(df['color']))     plt.set_xlabel('Search term:' + searchterm, fontsize=15)      plt.axis(xmin=0.5, xmax=1.01)  In\u00a0[6]: Copied! <pre>S1('h3k4me2', distance)\n</pre> S1('h3k4me2', distance) In\u00a0[7]: Copied! <pre># Print a sample of filenames\n\nprint(list(set(distance.filename))[0:4])\n</pre> # Print a sample of filenames  print(list(set(distance.filename))[0:4]) <pre>['gse124683/gsm3540312_k562_h3k4me2_rep1_20181202_icell8_21.bed.gz', 'gse175750/gsm5345532_kdm4a-oe.h3k9ac.ls.rep2.bed.gz', 'gse161624/gsm4911338_etv6_ncoa2_tm_hcd34_h3k27ac_vs_total_peaks.narrowpeak.gz', 'gse124690/gsm3540920_k562_h3k4me2_rep2_20181202_icell8_215.bed.gz']\n</pre> In\u00a0[8]: Copied! <pre>def S2(file, distance):\n    df = distance[distance.filename == file].sort_values(by=['score'], ascending = False)[0:10]\n    df= df.sort_values(by=['score'], ascending = True)\n    df['color']='green'\n    plt= df.plot.barh(x='search_term', y='score', figsize=(8,5), fontsize=16, color=list(df['color']))\n    plt.set_xticks(np.arange(0.5,1.1, 0.1))\n    plt.set_ylabel('Similarity', fontsize=15)\n    plt.set_xlabel(file, fontsize=15)\n    \n</pre> def S2(file, distance):     df = distance[distance.filename == file].sort_values(by=['score'], ascending = False)[0:10]     df= df.sort_values(by=['score'], ascending = True)     df['color']='green'     plt= df.plot.barh(x='search_term', y='score', figsize=(8,5), fontsize=16, color=list(df['color']))     plt.set_xticks(np.arange(0.5,1.1, 0.1))     plt.set_ylabel('Similarity', fontsize=15)     plt.set_xlabel(file, fontsize=15)      In\u00a0[9]: Copied! <pre>S2('gse156613/gsm4743940_t52-h3k27ac_peaks.bed.gz', distance)\n</pre> S2('gse156613/gsm4743940_t52-h3k27ac_peaks.bed.gz', distance) In\u00a0[11]: Copied! <pre>file_name = './distance_r2r.csv'\ndistance_s3 = pd.read_csv(file_name)\ndistance_s3.score = 1 - distance_s3.score\n</pre> file_name = './distance_r2r.csv' distance_s3 = pd.read_csv(file_name) distance_s3.score = 1 - distance_s3.score In\u00a0[12]: Copied! <pre># print sample query region set\nlist(set(distance_s3.test_file))[0:10]\n</pre> # print sample query region set list(set(distance_s3.test_file))[0:10] Out[12]: <pre>['ENCFF538OAF.bed.gz,h3k4me1',\n 'ENCFF494ASP.bed.gz,h3k27me3',\n 'ENCFF292YSJ.bed.gz,h3k4me3',\n 'ENCFF001WUM.bed.gz,h3k4me3',\n 'ENCFF701ENX.bed.gz,h3k27ac',\n 'ENCFF727GSV.bed.gz,h3k27ac',\n 'ENCFF787BOJ.bed.gz,h3k27me3',\n 'ENCFF526BJR.bed.gz,h3k27ac',\n 'ENCFF908TKC.bed.gz,h3k9me3',\n 'ENCFF099WAJ.bed.gz,h4k20me1']</pre> In\u00a0[13]: Copied! <pre>def S3(query_file, distance_s3):\n    \n    df = distance_s3[distance_s3.test_file==query_file].sort_values(by ='score', ascending = False)[['test_file', 'train_file', 'score']]\n    df['label_test'] = df.test_file.str.split(',', expand = True)[1]\n    df['label_train'] = df.train_file.str.split(',', expand = True)[1]\n    \n    nof = len(df[df.label_test==df.label_train])\n    df = df[0:10]\n    \n\n    df=df.sort_values(by=['score'])\n    df['color']='gray'\n    df.loc[df.label_test==df.label_train, 'color'] = 'green'\n    \n    if(len(df[df.color=='green']) ==nof):\n\n        df.loc[(df.color!='green'), 'color'] = 'gray'\n  \n    plt= df.plot.barh(x='train_file', y='score', figsize=(10,7), fontsize=16, color=list(df['color']))\n    plt.axis(xmin=0.7,xmax=1.01)\n    \n    plt.set_ylabel('Similarity', fontsize=15)\n    plt.set_xlabel(query_file, fontsize=15)\n</pre> def S3(query_file, distance_s3):          df = distance_s3[distance_s3.test_file==query_file].sort_values(by ='score', ascending = False)[['test_file', 'train_file', 'score']]     df['label_test'] = df.test_file.str.split(',', expand = True)[1]     df['label_train'] = df.train_file.str.split(',', expand = True)[1]          nof = len(df[df.label_test==df.label_train])     df = df[0:10]           df=df.sort_values(by=['score'])     df['color']='gray'     df.loc[df.label_test==df.label_train, 'color'] = 'green'          if(len(df[df.color=='green']) ==nof):          df.loc[(df.color!='green'), 'color'] = 'gray'        plt= df.plot.barh(x='train_file', y='score', figsize=(10,7), fontsize=16, color=list(df['color']))     plt.axis(xmin=0.7,xmax=1.01)          plt.set_ylabel('Similarity', fontsize=15)     plt.set_xlabel(query_file, fontsize=15) In\u00a0[16]: Copied! <pre>S3('ENCFF168GCU.bed.gz,h3k4me1', distance_s3)\n</pre> S3('ENCFF168GCU.bed.gz,h3k4me1', distance_s3)"},{"location":"geniml/notebooks/bedspace-analysis/#scenario-1-return-a-list-most-similar-region-sets-and-their-similarity-score-to-a-query-metadata-label-l2r","title":"Scenario 1: return a list most similar region sets and their similarity score to a query metadata label (l2r)\u00b6","text":""},{"location":"geniml/notebooks/bedspace-analysis/#scenario-2-return-a-list-of-most-similar-labels-and-their-similarity-score-to-a-query-region-set-r2l","title":"Scenario 2: return a list of most similar labels and their similarity score to a query region set (r2l)\u00b6","text":""},{"location":"geniml/notebooks/bedspace-analysis/#scenario-3-return-a-list-of-most-similar-region-sets-and-their-similarity-scores-to-a-query-region-set-r2r","title":"Scenario 3: return a list of most similar region sets and their similarity scores to a query region set (r2r)\u00b6","text":""},{"location":"geniml/notebooks/create-consensus-peaks-python/","title":"Create consensus peaks with Python","text":"In\u00a0[1]: Copied! <pre>from geniml.universe.cc_universe import cc_universe\ncc_universe(\"coverage/\", file_out=\"universe_cc.bed\")\n</pre> from geniml.universe.cc_universe import cc_universe cc_universe(\"coverage/\", file_out=\"universe_cc.bed\") <p>Depending on the task the universe can be smooth by setting <code>merge</code> option with the distance below witch peaks should be merged together and <code>filter_size</code> with minimum size of peak that should be part of the universe. Instead of using maximum likelihood cutoff one can also defined cutoff with <code>cutoff</code> option. If it is set to 1 the result is union universe, and when to number of files it will produce intersection universe:</p> In\u00a0[2]: Copied! <pre>cc_universe(\"coverage/\", file_out=\"universe_union.bed\", cutoff=1)\ncc_universe(\"coverage/\", file_out=\"universe_intersection.bed\", cutoff=4)\n</pre> cc_universe(\"coverage/\", file_out=\"universe_union.bed\", cutoff=1) cc_universe(\"coverage/\", file_out=\"universe_intersection.bed\", cutoff=4) In\u00a0[3]: Copied! <pre>from geniml.universe.ccf_universe import ccf_universe\n\nccf_universe(\"coverage/\", file_out=\"universe_ccf.bed\")\n</pre> from geniml.universe.ccf_universe import ccf_universe  ccf_universe(\"coverage/\", file_out=\"universe_ccf.bed\") In\u00a0[4]: Copied! <pre>from geniml.likelihood.build_model import main\n\nmain(\"model.tar\", \"coverage/\",\n     \"all\",\n     file_no=4)\n</pre> from geniml.likelihood.build_model import main  main(\"model.tar\", \"coverage/\",      \"all\",      file_no=4) <pre>Function 'main' executed in 0.0001min\n</pre> <p>The resulting tar archiver contains LH model. This model can be used as a scoring function that assigns to each position probability of it being a start, core or end of a region. It can be both used for universe assessment and universe building. Combination of LH model and optimization algorithm for building flexible universes results in maximum likelihood universe (ML):</p> In\u00a0[5]: Copied! <pre>from geniml.universe.ml_universe import ml_universe\n\nml_universe(\"model.tar\",\n     \"coverage\",\n     \"all\",\n     \"universe_ml.bed\")\n</pre> from geniml.universe.ml_universe import ml_universe  ml_universe(\"model.tar\",      \"coverage\",      \"all\",      \"universe_ml.bed\") In\u00a0[6]: Copied! <pre>from geniml.universe.hmm_universe import hmm_universe\n\nhmm_universe(\"coverage/\",\n             \"universe_hmm.bed\")\n</pre> from geniml.universe.hmm_universe import hmm_universe  hmm_universe(\"coverage/\",              \"universe_hmm.bed\")"},{"location":"geniml/notebooks/create-consensus-peaks-python/#how-to-build-a-new-universe","title":"How to build a new universe?\u00b6","text":""},{"location":"geniml/notebooks/create-consensus-peaks-python/#data-preprocessing","title":"Data preprocessing\u00b6","text":"<p>This is a jupyter version of CLI tutorial that can be found here. You will use here python functions instead of CLI to build and assess different universes. Files that you will use here can be downloaded from XXX. In there you will find a compressed folder:</p> <pre><code>consensus:\n    - raw\n        test_1.bed\n        test_2.bed\n        test_3.bed\n        test_4.bed\n    file_list.txt\n    chrom.sizes\n</code></pre> <p>In the raw folder there are example BED files used in this tutorial and in file_list.txt are names of files you will analyze. Additionally there is a file with chromosome sizes, which you will use to preprocess the data.</p> <p>Here we assume that you already have files of the genome coverage by the analyzed collection. The example of how to create them can be found here.</p>"},{"location":"geniml/notebooks/create-consensus-peaks-python/#coverage-cutoff-universe","title":"Coverage cutoff universe\u00b6","text":"<p>First, you will create a coverage cutoff universe (CC). This is the simplest type of a universe that only includes genomic positions with coverage greater or equal to cutoff x. This cutoff by default is calculated using simple likelihood model that calculates the probability of appearing in a collection. The universe can be build just based on genome coverage:</p>"},{"location":"geniml/notebooks/create-consensus-peaks-python/#coverage-cutoff-flexible-universe","title":"Coverage cutoff flexible universe\u00b6","text":"<p>A more complex version of coverage cutoff universe is coverage cutoff flexible universe (CCF). In contrast to its' fixed version it produces flexible universes. It uses two cutoffs calculated based on maximum likelihood cutoff, making a confidence interval around the optimal cutoff value. Despite the fact that the CFF universe is more complex it is build using the same input as the CC universe:</p>"},{"location":"geniml/notebooks/create-consensus-peaks-python/#maximum-likelihood-universe","title":"Maximum likelihood universe\u00b6","text":"<p>In the previous examples both CC anf CCF universes used simple likelihood model to calculate the cutoff. However, we also developed more complex likelihood model that takes into account the positions of starts and ends of the regions in the collection. This LH model can build based on coverage files:</p>"},{"location":"geniml/notebooks/create-consensus-peaks-python/#hmm","title":"HMM\u00b6","text":"<p>The forth presented method of creating universes utilizes Hidden Markov Models. In this approach the parts of flexible regions are hidden states of the model, while genome coverage by the collections are emissions. The resulting universe is called Hidden Markov Model universe. It can be build only based on the genome coverage by the collection:</p>"},{"location":"geniml/notebooks/create-consensus-peaks-python/#how-to-assess-new-universe","title":"How to assess new universe?\u00b6","text":"<p>So far you used many different methods for creating new universes. But choosing, which universe represents data the best can be challenging. To help with this we created a tutorial that can be found here, which presents different  methods that assess universe fit to the collection of files.</p>"},{"location":"geniml/tutorials/assess-universe/","title":"How to assess universe fit to collection of BED files","text":""},{"location":"geniml/tutorials/assess-universe/#introduction","title":"Introduction","text":"<p>If you have a potential universe file, and a collection of BED files, this module will help you assess the fit of the proposed universe to your collection of BED files. We can assess fit either from CLI, or from within Python.</p>"},{"location":"geniml/tutorials/assess-universe/#command-line-usage","title":"Command-line usage","text":"<p>Both overlap and distance based assessments can be run using: <code>geniml assess ...</code> with appropriate flags.</p> <p><pre><code> geniml assess --assessment-method1 \\\n             --assessment-method2 \\\n             --...\n             --raw-data-folder tests/consesnus/raw/ \\\n             --file-list tests/consesnus/file_list.txt \\\n             --universe tests/consenus/universe/universe.bed \\\n             --save-to-file \\\n             --folder-out tests/consesnus/results/intersection/ \\\n             --pref test \\\n             --no-workers 1\n</code></pre> Where:</p> <ul> <li><code>--raw-data-folder</code>, takes the path to folder with files from the collection</li> <li><code>--file-list</code>, takes the path to file with list of files</li> <li><code>--universe</code>, takes the path to file with the assessed universe</li> <li><code>--save-to-file</code>,  is a flag that specifies whether to out put table with each row  containing file name and results of chosen metrics</li> <li><code>--folder-out</code>, takes the path to folder in which put the output file</li> <li><code>--pref</code>, takes a prefix of output file name</li> <li><code>--no-workers</code>, takes the number of workers that should be used</li> <li><code>--save-each</code>, is a flag that specifies whether to save between the closest peaks to file</li> </ul>"},{"location":"geniml/tutorials/assess-universe/#base-level-overlap-measure","title":"Base-level overlap measure","text":"<p>First test checks how much of our file is present in the universe and how much additional information is present in the universe. We can check that by adding <code>--overlap</code> to <code>geniml assess ...</code>. In the result files it will output columns with: number of bp in universe but not in file, number of bp in file but not the universe, and number of bp both in universe and file.</p> <p>We can also use it directly from Python like this:</p> <pre><code>from geniml.assess.intersection import run_intersection\n\nrun_intersection(\"test/consensus/raw/\",\n                        \"tests/consensus/file_list.txt\",\n                        \"tests/consensus/universe/universe.bed\",\n                        no_workers=1)\n</code></pre> <p>Or, we can calculate F10 score of the universe using:</p> <pre><code>from geniml.assess.intersection import get_f_10_score\n\nget_f_10_score(\"test/consensus/raw/\",\n               \"tests/consensus/file_list.txt\",\n               \"tests/consensus/universe/universe.bed\",\n               no_workers=1)\n</code></pre>"},{"location":"geniml/tutorials/assess-universe/#region-boundary-distance-measure","title":"Region boundary distance measure","text":"<p>Next, we can calculate the distance between query and universe. To do that we can choose from :  - <code>distance</code> - calculates distance from region in query to the nearest region in the universe  - <code>distance-universe-to-file</code>- calculates distance from region in query to the nearest region in the universe accounting for universe flexibility  - <code>distance-flexible</code> - calculates distance from region in universe to the nearest region in the query  - <code>distance-flexible-universe-to-file</code> - calculates distance from region in universe to the nearest region in the query accounting for universe flexibility</p> <p>All presented distance measures can be done using python, which will result in matrix where first column is file names and the second one is median of distances. </p> <p><pre><code>from geniml.assess.distance import run_distance\n\nd_median = run_distance(\"tests/consensus/raw\",\n                  \"tests/consensus/file_list.txt\",\n                  \"tests/consensus/universe/universe.bed\",\n                  npool=2)\n</code></pre> Additionally, we can directly calculate the closeness score using:</p> <pre><code>from geniml.assess.distance import get_closeness_score\n\ncloseness_score = get_closeness_score(\"tests/consensus/raw\",\n                                      \"tests/consensus/file_list.txt\",\n                                      \"tests/consensus/universe/universe.bed\",\n                                      no_workers=2)\n</code></pre>"},{"location":"geniml/tutorials/assess-universe/#universe-likelihood","title":"Universe likelihood","text":"<p>We can also calculate the likelihood of universe given collection of file. For that we will need likelihood model. We can do it either for hard universe:</p> <pre><code>from geniml.assess.likelihood import hard_universe_likelihood\n\nlh_hard = hard_universe_likelihood(\"tests/consensus/lh_model.tar\",\n                         \"tests/consensus/universe/universe.bed\",\n                         \"tests/consensus/coverage\", \"all\")\n</code></pre> <p>or with taking into account universe flexibility:</p> <pre><code>from geniml.assess.likelihood import likelihood_flexible_universe\n\nlh_flexible = likelihood_flexible_universe(\"tests/consensus/lh_model.tar\",\n                         \"tests/consensus/universe/universe.bed\",\n                         \"tests/consensus/coverage\", \"all\")\n</code></pre>"},{"location":"geniml/tutorials/bedshift-evaluation-guide/","title":"Using bedshift to create an evaluation dataset for similarity scores","text":""},{"location":"geniml/tutorials/bedshift-evaluation-guide/#generate-different-files","title":"Generate different files","text":"<p>Bedshift perturbations include add, drop, shift, cut, and merge. Using any of these perturbations, or combinations of them, you can generate a set of files that are slightly perturbed from the original file. Assuming that the original file is called <code>original.bed</code>, and you want 100 files of added regions and 100 files of dropped regions:</p> <pre><code>geniml bedshift -l hg38.chrom.sizes -b original.bed -a 0.1 -r 100\ngeniml bedshift -l hg38.chrom.sizes -b original.bed -d 0.3 -r 100\n</code></pre> <p>Don't forget the add and shift operations require a chrom.sizes file. The output file will be in <code>bedshifted_original.bed</code>.</p>"},{"location":"geniml/tutorials/bedshift-evaluation-guide/#evaluating-a-similarity-score","title":"Evaluating a similarity score","text":"<p>This is when the bedshifted file will be put to use. The 100 repetitions of add and drop will be compared against the original file using the similarity score of your choice. The output of the similarity score should reflect the degree of change specified to bedshift. In very general terms, the pseudocode should be like this:</p> <pre><code>for each bedshift_file in folder:\n    score = SimilarityScore(bedshift_file, original_file, ...)\n    add score to score_list\navg_similarity_score = mean(score_list)\n</code></pre> <p>You can repeat this for each of the similarity scores and each of the perturbation combinations, and then compare the results. This way, you can get an accurate understanding of whether your similarity score reflects added regions, dropped regions, and more.</p>"},{"location":"geniml/tutorials/bedshift-evaluation-guide/#using-a-pep-to-quickly-submit-multiple-bedshift-jobs","title":"Using a PEP to quickly submit multiple bedshift jobs","text":"<p>Using a Portable Encapsulated Project (PEP), creating multiple combinations of bedshift files becomes faster and more organized. The PEP consists of a sample table containing the perturbation parameters and a config file. Here is what the <code>sample_table.csv</code> may look like. Each row specifies the arguments for a bedshift command.</p> sample_name add drop shift cut merge add1 0.1 0.0 0.0 0.0 0.0 add2 0.2 0.0 0.0 0.0 0.0 add3 0.3 0.0 0.0 0.0 0.0 drop-shift1 0.0 0.1 0.2 0.0 0.0 drop-shift2 0.0 0.2 0.2 0.0 0.0 drop-cut 0.0 0.3 0.0 0.4 0.0 shift-merge 0.0 0.0 0.4 0.0 0.4 <p>And here is what the <code>project_config.yaml</code> file looks like:</p> <pre><code>pep_version: 2.0.0\nsample_table: \"sample_table.csv\"\nsample_modifiers:\n  append:\n    file: \"original.bed\"\n    repeat: 100\n</code></pre> <p>Now the project is described neatly in two files. The <code>sample_modifiers</code> in the config file just adds extra columns to the sample table in post-processing and makes the project more configurable, instead of having to repeat the same parameter in the <code>sample_table.csv</code>. In this example, the <code>sample_modifiers</code> append two columns with the file which bedshift is to be performed on, and the number of repetitions that bedshift should create.</p> <p>The PEP describes the project, but the tool that submits the project jobs is called looper. In one line of code, it will interpret the PEP and form commands to be submitted to your processor or computing cluster. To use looper, you will need to add a few lines to your <code>project_config.yaml</code>:</p> <pre><code>pep_version: 2.0.0\nsample_table: \"sample_table.csv\"\nlooper:\n  output_dir: \"looper_output/\"\nsample_modifiers:\n  append:\n    pipeline_interfaces: \"pipeline_interface.yaml\"\n    file: \"original.bed\"\n    repeat: 100\n</code></pre> <p>You will also need to create a <code>pipeline_interface.yaml</code> that describes how to form commands:</p> <pre><code>pipeline_name: bedshift_run\npipeline_type: sample\ncommand_template: &gt;\n    bedshift -b {sample.file} -l hg38.chrom.sizes -a {sample.add} -d {sample.drop} -s {sample.shift} -c {sample.cut} -m {sample.merge} -r {sample.repeat} -o {sample.sample_name}.bed\ncompute:\n  mem: 4000\n  cores: 1\n  time: \"00:10:00\"\n</code></pre> <p>After all of this, the command to run looper and submit the jobs is:</p> <pre><code>looper run project_config.yaml\n</code></pre> <p>Soon, you should see bedshift files appear in the <code>looper_output</code> folder. The BED file names will correspond to the sample names from <code>sample_table.csv</code>.</p>"},{"location":"geniml/tutorials/bedshift/","title":"Randomizing BED files with bedshift","text":"<p>Bedshift is a tool for randomly perturbing BED file regions. The perturbations supported on regions are shift, drop, add, cut, and merge. This tool is particularly useful for creating test datasets for various tasks, since there often is no ground truth dataset to compare to. By perturbing a file, a pipeline or analysis can be run on both the perturbed file and the original file, then be compared.</p>"},{"location":"geniml/tutorials/bedshift/#installing","title":"Installing","text":"<p>Bedshift is part of the <code>geniml</code> package distributed on PyPI.</p> <pre><code>pip install geniml\n</code></pre>"},{"location":"geniml/tutorials/bedshift/#quickstart","title":"Quickstart","text":"<p>The package is available for use both as a command line interface and a python package. To get started, type on the command line:</p> <pre><code>geniml bedshift -h\n</code></pre> <p>The following examples will shift 10% of the regions and add 10% new regions in <code>examples/test.bed</code>. The -l argument is the file in which chromosome sizes are located, and is only required for adding and/or shifting regions. The output is located at <code>bedshifted_test.bed</code>.</p> <p>CLI:</p> <pre><code>geniml bedshift -l hg38.chrom.sizes -b tests/test.bed -s 0.1 -a 0.1\n</code></pre> <p>Python:</p> <pre><code>from geniml.bedshift import bedshift\n\nbedshifter = bedshift.Bedshift('tests/test.bed', 'hg38.chrom.sizes')\nbedshifter.shift(shiftrate=0.1, shiftmean=0.0, shiftstdev=120.0)\nbedshifter.add(addrate=0.1, addmean=320.0, addstdev=20.0)\nbedshifter.to_bed('tests/test_output.bed')\n</code></pre>"},{"location":"geniml/tutorials/bedshift/#example-repository","title":"Example Repository","text":"<p>If you're looking to use Bedshift in your own experiment, we created an example repository containing working code to:</p> <ol> <li>Produce a large dataset of Bedshift files</li> <li>Run a pipeline on the dataset and obtain results</li> <li>Aggregate and visualize the results</li> </ol> <p>It integrates the PEP and looper workflow allowing you to easily run the project out of the box.</p>"},{"location":"geniml/tutorials/bedshift/#generate-a-random-bed-file","title":"Generate a random BED file","text":"<p>To generate a random BED file, you need to start with is a BED file with one region, which you will delete later. (It will appear at the top of the BED files, so it will be easier to delete later.) On the command line:</p> <pre><code>echo -e \"chr1\\t1\\t1000\" &gt; random.bed\n</code></pre> <p>The next step is to construct a bedshift command to add regions to this file. We will need to specify the rate of add, which in this case is going to be the number of the new regions we want. Let's try generating 1,000 new regions. Don't forget to specify a chromosome sizes file, which is required for adding regions.</p> <pre><code>geniml bedshift -b random.bed -l hg38.chrom.sizes -a 1000\n</code></pre> <p>The printed output should say that 1000 regions changed. Finally, go into the output at bedshifted_random.bed and delete the original region. If you specified repeats and have many output files that need to have the original region deleted, here is a handy command to delete the first line of every BED file. (Warning: make sure there are no other BED files in the folder before using this command.)</p> <pre><code>find . -name \"*.bed\" -exec sed -i.bak '1d' {} \\;\n# if your system is macOS\n# find . -name \"*.bed\" -exec sed -i .bak '1d' {} \\;\n</code></pre> <p>This <code>find</code> command will find all BED files and execute a <code>sed</code> command to remove the first line. The <code>sed</code> command will operate in place and create <code>.bak</code> backup files, which can be removed later.</p>"},{"location":"geniml/tutorials/bedshift/#shift-add-and-drop-from-file","title":"Shift, Add, and Drop From File","text":"<p>\"From file\" means that the regions selected to shift, add, or drop are specified from a provided file. These features provide the ability to finely control what regions are perturbed. For example, if you have a BED file specifying exon regions and you want to add only exons, you can use <code>--addfile</code>.</p>"},{"location":"geniml/tutorials/bedshift/#add-from-file-example","title":"Add from file example","text":"<pre><code>geniml bedshift -b mydata.bed -a 0.07 --addfile exons.bed\n</code></pre> <p>Specifying <code>--addfile</code> with <code>-a</code> add rate will increase the size of <code>mydata.bed</code> by 7% with new regions selected from <code>exons.bed</code>.</p>"},{"location":"geniml/tutorials/bedshift/#shift-from-file-example","title":"Shift from file example","text":"<p>Shift from file first calculates which regions overlap between the specified <code>--shiftfile</code> and <code>--bedfile</code>, then selects which regions to shift among those overlaps.</p> <pre><code>geniml bedshift -b mydata.bed -l hg38.chrom.sizes -s 0.42 --shiftmean 5 --shiftstdev 5 --shiftfile snp.bed\n</code></pre> <p>In this example, we only want to shift regions that are SNPs. The number of shifted regions is 42% of the total regions in <code>mydata.bed</code>. Notice here that unlike <code>--addfile</code>, we still have to specify the shift mean and standard deviation. This is because <code>--shiftfile</code> tells which regions to shift, but not by how much.</p>"},{"location":"geniml/tutorials/bedshift/#drop-from-file-example","title":"Drop from file example","text":"<p>Drop from file, like shift from file, calculates overlaps between the specified <code>--dropfile</code> and <code>--bedfile</code>, then selects regions from those overlaps to drop.</p> <pre><code>geniml bedshift -b mydata.bed -d 0.4 --dropfile snp.bed\n</code></pre> <p>This command will drop regions that overlap with SNPs. The number of dropped regions is 40% of the total regions in <code>mydata.bed</code>.</p>"},{"location":"geniml/tutorials/bedshift/#use-a-yaml-file-to-specify-perturbations","title":"Use a YAML File to Specify Perturbations","text":"<p>Sometimes the default settings of bedshift does not allow enough control over perturbations. For example, the order of perturbations is fixed as shift, add, cut, merge, drop, so if you wanted to change the order you would have to specify multiple commands. The same problem arises when you want to run multiple \"add from file\" commands - there is just no way to do it using a single command.</p> <p>This is why we created the YAML config file perturbation option. In the YAML file, users can specify as many perturbations as they want, along with the parameters specific to each perturbation. An example of a YAML config file follows:</p> <pre><code>bedshift_operations:\n  - add_from_file:\n    file: exons.bed\n    rate: 0.2\n  - add_from_file:\n    file: snp.bed\n    rate: 0.05\n  - shift_from_file:\n    file: exons.bed\n    rate: 0.4\n    mean: 100\n    stdev: 85\n  - shift_from_file:\n    file: snp.bed\n    rate: 0.4\n    mean: 2\n    stdev: 1\n  - merge:\n    rate: 0.15\n</code></pre> <p>The order of perturbations is run in the same order they are specified. So in this example, we add from two different files, then also shift those regions that were just added. Finally we perform a merge at 15% rate.</p>"},{"location":"geniml/tutorials/bedshift/#how-to-bedshift-all-files-in-a-directory","title":"How to bedshift all files in a directory","text":""},{"location":"geniml/tutorials/bedshift/#using-shell","title":"Using shell","text":"<p>Assuming you are bedshifting all files in the current working directory using the same parameter, use the following shell script (changing parameters as needed), which iterates over files in the directory and applies bedshift:</p> <pre><code>#!/bin/bash\nfor filename in *.bed; do\n    CHROM_LENGTHS=hg38.chrom.sizes\n    BEDFILE=$filename\n    DROP_RATE=0.3\n\n    ADD_RATE=0.2\n    ADD_MEAN=320.0\n    ADD_STDEV=30.0\n\n    SHIFT_RATE=0.2\n    SHIFT_MEAN=0.0\n    SHIFT_STDEV=150.0\n\n    CUT_RATE=0.0\n    MERGE_RATE=0.0\n\n    geniml bedshift --bedfile $BEDFILE --chrom-lengths $CHROM_LENGTHS --droprate $DROP_RATE --addrate $ADD_RATE --addmean $ADD_MEAN --addstdev $ADD_STDEV --shiftrate $SHIFT_RATE --shiftmean $SHIFT_MEAN --shiftstdev $SHIFT_STDEV --cutrate $CUT_RATE --mergerate $MERGE_RATE\ndone\n</code></pre>"},{"location":"geniml/tutorials/bedshift/#using-python","title":"Using Python","text":"<p>In Python, you need the <code>os</code> library to get the filenames in a directory. Then you loop through the filenames and apply bedshift.</p> <pre><code>from geniml.bedshift import bedshift\nimport os\n\nfiles = os.listdir('/path/to/data/')\nfor file in files:\n    if file.endswith('.bed'):\n        # you may also pass in a chrom.sizes file as the \n        # second argument if you are adding or shifting regions\n        b = bedshift.Bedshift(file)\n        b.all_perturbations(cutrate=0.3, droprate=0.2)\n        b.to_bed('bedshifted_' + file)\n</code></pre>"},{"location":"geniml/tutorials/bedshift/#add-random-regions-only-in-valid-regions","title":"Add Random Regions Only in Valid Regions","text":"<p>Using the basic <code>--add</code> option, regions are added randomly onto any chromosome at any location, without any regard for non-coding regions. For use cases of Bedshift more rooted in biology, this effect is not desirable. The <code>--add-valid</code> option gives the user the ability to specify a BED file indicating areas where it is valid to add regions. Thus, if an <code>--add-valid</code> file has only coding regions, then regions will be randomly added only in those areas. Here is an example:</p> <pre><code>geniml bedshift -b mydata.bed -a 0.5 --add-valid coding.bed --addmean 500 --addstdev 200\n</code></pre> <p><code>coding.bed</code> contains large regions of the genome which are coding. Added regions can be anywhere inside of those regions. In addition, the method considers the size of the valid regions in deciding where the new regions will be added, so the smaller valid regions will contain proportionally less new regions than the larger valid regions.</p>"},{"location":"geniml/tutorials/bedspace/","title":"How to use BEDSpace to jointly embed regions and metadata","text":""},{"location":"geniml/tutorials/bedspace/#introduction","title":"Introduction","text":"<p>BEDspace is an application of the StarSpace model to genomic interval data, described in Gharavi et al. 2023. It allows us to train numerical embeddings for a collection of region sets simultaneously with their metadata labels, capturing similarity between region sets and their metadata in a low-dimensional space. Using these learned co-embeddings, BEDspace solves three related information retrieval tasks using embedding distance computations: retrieving region sets related to a user query string; suggesting new labels for database region sets; and retrieving database region sets similar to a query region set.</p>"},{"location":"geniml/tutorials/bedspace/#installation","title":"Installation","text":"<p>The <code>bedspace</code> module is installed with <code>geniml</code>. To ensure that everything is working correctly, run: <code>python -c \"from geniml import bedspace\"</code>. </p>"},{"location":"geniml/tutorials/bedspace/#bedspace-operations","title":"BEDspace operations","text":"<p>There are four main commands in <code>bedspace</code>:</p> <ol> <li><code>bedspace preprocess</code>: preprocesses a set of genomic interval regions and their associated metadata into a format that can be used by <code>bedspace train</code>.</li> <li><code>bedspace train</code>: trains a StarSpace model on the preprocessed data.</li> <li><code>bedspace distances</code>: computes distances between region sets in the trained model and metadata labels.</li> <li><code>bedspace search</code>: searches for the most similar region sets and metadata labels to a given query. Three scenarios for this command are described in the details.</li> </ol> <p>These commands are accessed via the command line with <code>genimtools bedspace &lt;command&gt;</code>.</p>"},{"location":"geniml/tutorials/bedspace/#bedspace-preprocess","title":"<code>bedspace preprocess</code>","text":"<p>The <code>preprocess</code> command will prepare a set of region sets and metadata labels for training. This includes things like adding the <code>__label__</code> prefix to metadata labels, and converting the region sets into a format that can be used by StarSpace. The command takes in a set of region sets and metadata labels, and outputs a set of preprocessed region sets and metadata labels. The command can be run as follows:</p> <pre><code>geniml bedspace preprocess \\\n    --input &lt;path to input region sets&gt; \\\n    --metadata &lt;path to input metadata labels&gt; \\\n    --universe &lt;path to universe file&gt; \\\n    --labels &lt;path to the labels file&gt; \\\n    --output &lt;path to output preprocessed region sets&gt;\n</code></pre> <p>Input Description:</p> <p><code>--input</code>: Specifies the path to the folder containing the region sets. <code>--metadata</code>: Specifies the path to the metadata file in CSV format. The CSV file should include a column for file_name and separate columns for each label. The file_name column contains the names of the region set files, and the label columns contain the corresponding labels for each region set. <code>--universe</code>: Specifies the path to the universe file. The universe file contains the chromosome, start position, and end position for each region for region set tokenization. <code>--labels</code>: Specifies the target labels as a single string containing labels separated by commas.  </p>"},{"location":"geniml/tutorials/bedspace/#bedspace-train","title":"<code>bedspace train</code>","text":"<p>The <code>train</code> command will train a StarSpace model on the preprocessed region sets and metadata labels. It requires that you have run the <code>preprocess</code> command first. The <code>train</code> command takes in a set of preprocessed region sets and metadata labels, and outputs a trained StarSpace model. The command can be run as follows:</p> <pre><code>geniml bedspace train \\\n    --path-to-starspace &lt;path to StarSpace executable&gt; \\\n    --input &lt;path to preprocessed region sets&gt; \\\n    --output &lt;path to output trained model&gt; \\\n    --dim &lt;dimension of embedding space&gt; \\\n    --epochs &lt;number of epochs to train for&gt; \\\n    --lr &lt;learning rate&gt;\n</code></pre> <p>Input Description:</p> <p><code>--path-to-starspace</code>: Specifies the path to the StarSpace executable. <code>--input</code>: Specifies the path to the preprocessed region sets file generated from the preprocess function. The file should be in TXT format. <code>--output</code>: Specifies the path where the trained model will be saved.  <code>--dim</code>: Sets the dimension of the vector for the region set and label embedding from the StarSpace model. <code>--epochs</code>: Specifies the number of epochs to train the StartSpace model. <code>--lr</code>: Sets the learning rate for the training process.  </p>"},{"location":"geniml/tutorials/bedspace/#bedspace-distances","title":"<code>bedspace distances</code>","text":"<p>The <code>distances</code> command will compute the distances between all of the region sets and metadata labels in the trained model. It requires that you have ran the <code>train</code> command first. The <code>distances</code> command takes in a trained StarSpace model, and outputs a set of distances between all of the region sets and metadata labels in the model. The command can be run as follows:</p> <pre><code>geniml bedspace distances \\\n    --input &lt;path to trained model&gt; \\\n    --metadata &lt;path to input metadata labels&gt; \\\n    --universe &lt;path to universe file&gt; \\\n    --labels &lt;path to labels file&gt; \\\n    --files &lt;path to region sets&gt; \\\n    --output &lt;path to output distances&gt;\n</code></pre> <p>Input Description:</p> <p><code>--input</code>: Specifies the path to the trained model generated by the bedspace train command. <code>--metadata</code>: Specifies the path to the input metadata labels. <code>--universe</code>: Specifies the path to the universe file used for test file tokenization. <code>--labels</code>: Specifies the target labels as a single string containing labels separated by commas. <code>--files</code>: Specifies the path to the new region sets. <code>--output</code>: Specifies the path where the distances file between labels and files, as well as database files and new files, will be saved.  </p>"},{"location":"geniml/tutorials/bedspace/#bedspace-search","title":"<code>bedspace search</code>","text":"<p>The <code>search</code> command requires that you have previously run the <code>distances</code> command.  It also requires a query. To search, you must specify one of 3 scenarios when using the <code>search</code> command:</p> <ol> <li><code>r2l</code> (region-to-label): You have a query region set and want to find the most similar metadata labels,</li> <li><code>l2r</code> (label-to-region): You have a query metadata label and want to find the most similar region sets, and</li> <li><code>r2f</code> (region-to-region): You have a query region set and want to find the most similar region sets.</li> </ol> <p>Example usage for each type are given below:</p>"},{"location":"geniml/tutorials/bedspace/#r2l","title":"<code>r2l</code>","text":"<pre><code>geniml bedspace search \\\n    -t lr2\n    -d &lt;path to distances&gt; \\\n    -n &lt;number of results to return&gt; \\\n    path/to/regions.bed\n</code></pre>"},{"location":"geniml/tutorials/bedspace/#l2r","title":"<code>l2r</code>","text":"<pre><code>geniml bedspace search \\\n    -t rl2\n    -d &lt;path to distances&gt; \\\n    -n &lt;number of results to return&gt; \\\n    K562\n</code></pre>"},{"location":"geniml/tutorials/bedspace/#r2r","title":"<code>r2r</code>","text":"<pre><code>geniml bedspace search \\\n    -t r2r\n    -d &lt;path to distances&gt; \\\n    -n &lt;number of results to return&gt; \\\n    path/to/regions.bed\n</code></pre> <p>Input Description:</p> <p><code>-t</code>: Specifies the search type. <code>-d</code>: Specifies the path to the distances file generated by the bedsapce distances command. <code>-n</code>: Specifies the number of top results to return.  </p>"},{"location":"geniml/tutorials/bivector-search-interface/","title":"Region set (BED) search combining both metadata and genomic regions","text":""},{"location":"geniml/tutorials/bivector-search-interface/#metadata-embedding-vector-backend","title":"Metadata embedding vector backend","text":"<p>This vector beckend stored the embedding vectors of region set metadata annotations, which are encoded by  open-source text model (<code>SentenceTransformers</code>, etc.). The payload of each metadata annotation vector must contain the storage ids of region set that the annotation matches.</p>"},{"location":"geniml/tutorials/bivector-search-interface/#example-code","title":"Example code","text":"<pre><code>from geniml.search.backends import BiVectorBackend, QdrantBackend\nfrom geniml.search.interfaces import BiVectorSearchInterface\n\n# 2 required backends\ntext_backend = QdrantBackend(dim=384)\nbed_backend = QdrantBackend()\n\n# load vectors and payloads\nbed_backend.load(vectors=np.array(bed_vecs), payloads=bed_payloads)\ntext_backend.load(vectors=np.array(text_embeddings), payloads=text_payloads)\n\n# the search backend\nsearch_backend = BiVectorBackend(text_backend, bed_backend)\n\n\n# the search interface\nsearch_interface = BiVectorSearchInterface(\n    backend=search_backend, query2vec=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n\n\n# actual search\nresult = search_interface.query_search(\n    query=\"lung cancer cell lines\",\n    limit=10,\n    with_payload=True,\n    with_vectors=False,\n    p=1.0,\n    q=1.0,\n    distance = False # QdrantBackend returns similarity as the score, not distance\n)\n</code></pre>"},{"location":"geniml/tutorials/cell-type-annotation-with-knn/","title":"How to annotate cell-types with KNN","text":"<p>In the previous tutorial, we loaded a vector database with cell embeddings. In this tutorial, we will show how to use this vector database to query cell embeddings and annotate cells with cell-type labels using a KNN classification algorithm.</p> <p>If you have not completed the previous tutorial, you should ensure you have a vector database with cell embeddings.</p>"},{"location":"geniml/tutorials/cell-type-annotation-with-knn/#what-is-k-nearest-neighbors-knn-classification","title":"What is K-nearest-neighbors (KNN) classification?","text":"<p>According to IBM, K-nearest-neighbors classification is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. Point more simply: KNN is a classification algorithm that uses the distance between an unlabeled data point and its labeled neighbors to classify the new data point.</p> <p>Assuming we have a vector-space of well-annotated cell embeddings, we can use KNN to classify new cell embeddings based on their proximity to the labeled cell embeddings.</p>"},{"location":"geniml/tutorials/cell-type-annotation-with-knn/#querying-the-vector-database","title":"Querying the vector database","text":"<p>First, we need to generate new cell embeddings for the cells we want to annotate. Note: it is imperative that the new cell embeddings are generated using the same model as the cell embeddings in the vector database. The previous tutorial used <code>databio/r2v-luecken2021-hg38-v2</code> to generate cell embeddings. We will use the same model to generate new cell embeddings.</p> <pre><code>import scanpy as sc\n\nfrom geniml.scembed import ScEmbed\n\nadata = sc.read_h5ad(\"path/to/adata_unlabeled.h5ad\")\n\nmodel = ScEmbed(\"databio/r2v-luecken2021-hg38-v2\")\n</code></pre> <p>We can get embeddings of the dataset using the pre-trained model:</p> <pre><code>embeddings = model.encode(adata)\n\nadata.obsm['scembed_X'] = np.array(embeddings)\n</code></pre> <p>Now that we have the new cell embeddings, we can query the vector database to find the K-nearest-neighbors of each cell embedding.</p> <pre><code>from collections import Counter\nfrom qdrant_client import QdrantClient\n\nclient = QdrantClient(\"localhost\", port=6333)\n\n# Query the vector database\nk = 5 # set to whatever value you want, this is a hyperparameter\n\nfor i, embedding in enumerate(embeddings):\n    neighbors = client.search(\n        collection_name=\"luecken2021\", \n        query_vector=embedding.tolist(), \n        limit=k, \n        with_payload=True\n    )\n    cell_types = [neighbor.payload[\"cell_type\"] for neighbor in neighbors]\n\n    # get majority\n    cell_type = Counter(cell_types).most_common(1)[0][0]\n    adata.obs['cell_type'][i] = cell_type\n</code></pre> <p>And just like that, we've annotated our cells with cell-type labels using KNN classification. We can improve this methodology by first clustering the unlabeled cells and then using the cluster centroids to query the vector database. This will reduce the number of queries and improve the speed of the annotation process. Another approach would be to do a secondary consensus vote on each cluster and assign one label per cluster.</p>"},{"location":"geniml/tutorials/cli-tokenization/","title":"How to tokenize a BED file on the command line","text":"<p>For hard tokenization, run</p> <pre><code>from geniml.tokenization import hard_tokenization\n\nsrc_folder = '/path/to/raw/bed/files/'\ndst_folder = '/path/to/tokenized_files/'\nuniverse_file = '/path/to/universe_file.bed'\nhard_tokenization(src_folder, dst_folder, universe_file, 1e-9)\n</code></pre> <p>We use the <code>intersect</code> function of <code>bedtools</code> to do tokenization. If you want to switch to different tools, you can override the <code>bedtools_tokenization</code> function in <code>hard_tokenization_batch.py</code> and provide the path to your tool by specifying the input argument <code>bedtools_path</code>. The <code>fraction</code> argument specifies the minimum overlap required as a fraction of some region in the universe (default: 1E-9,i.e. 1bp; maximum 1.0). A raw region will be mapped into a universe region when an overlap is above the threshold.</p> <p>By default, the code assumes the binary <code>bedtools</code> exists and can be called via command line. If <code>bedtools</code> does not exists, the code will raise an exception. To solve this, please specify <code>bedtools_path</code> which points to a bedtools binary.</p> <p>Command line usage <pre><code>geniml tokenize --data-folder /folder/with/raw/BED/files --token-folder ./tokens --universe /universe/file --bedtools-path bedtools\n</code></pre></p> <p>For more details, type <code>geniml tokenize --help</code>.</p>"},{"location":"geniml/tutorials/create-consensus-peaks/","title":"How to build a new universe?","text":""},{"location":"geniml/tutorials/create-consensus-peaks/#data-preprocessing","title":"Data preprocessing","text":"<p>In this tutorial, you will use CLI of geniml package to build different types of universes from example files, which can be downloaded from XXX. In there you will find a compressed folder:</p> <pre><code>consensus:\n    - raw\n        test_1.bed\n        test_2.bed\n        test_3.bed\n        test_4.bed\n    file_list.txt\n    chrom.sizes\n</code></pre> <p>In the raw folder there are example BED files used in this tutorial and in file_list.txt are names of files you will analyze. Additionally there is a file with chromosome sizes, which you will use to preprocess the data. </p> <p>To build any kind of a universe you need bigWig files with genome coverage by the analyzed collection, which can be made it using uniwig. First we have to combine all the analyzed files into one BED file:</p> <pre><code>cat raw/* &gt; raw/combined_files.bed\n</code></pre> <p>This combined file can next be used to prepare the genome coverage tracks, with window size for smoothing of breakpoints set to 25:</p> <pre><code>$UNIWIG_PATH/bin/uniwig -m 25 raw/combined_files.bed chrom.sizes coverage/all\n</code></pre> <p>This will create three files: <code>coverage/all_start.bw</code>, <code>coverage/all_core.bw</code>, <code>coverage/all_end.bw</code>, with coverage of the genome by regions' starts, regions and regions' ends respectively. Those files can be loaded into Genomic Viewer for visualization.  </p>"},{"location":"geniml/tutorials/create-consensus-peaks/#coverage-cutoff-universe","title":"Coverage cutoff universe","text":"<p>First, you will create a coverage cutoff universe (CC). This is the simplest type of a universe that only includes genomic positions with coverage greater or equal to cutoff x. This cutoff by default is calculated using simple likelihood model that calculates the probability of appearing in a collection. The universe can be build just based on genome coverage:</p> <pre><code>geniml build-universe cc --coverage-folder coverage/ \\\n                          --output-file universe_cc.bed\n</code></pre> <p>Where:</p> <ul> <li><code>--coverage-folder</code>, takes the path to bigWig folder with genome coverage by collection</li> <li><code>--output-file</code>, takes the path to output file</li> </ul> <p>Or we can import it directly into Python:</p> <pre><code>from geniml.universe.cc_universe import cc_universe\n\ncc_universe(\"tests/consenus/coverage/\",\n        file_out=\"tests/consenus/universe/universe.bed\")\n</code></pre> <p>Depending on the task we can also smooth the output universe by setting <code>--merge</code> flag with the distance beyond which peaks should be merged together and <code>--filter-size</code> with minimum size of peak that should be part of the universe. We can also not use the maximum likelihood cut-off and instead of it use user defined cutoff. For that we have to set <code>--cutoff</code>. If we set it to 1 we get union universe, and when to number of files we will get intersection universe.</p>"},{"location":"geniml/tutorials/create-consensus-peaks/#coverage-cutoff-flexible-universe","title":"Coverage cutoff flexible universe","text":"<p>A more complex version of coverage cutoff universe is coverage cutoff flexible universe (CCF). In contrast to its' fixed version it produces flexible universe. It builds confidence interval around the maximum likelihood cutoff. This results in two values one for the cutoff for boundaries, and the other one for the region core. Despite the fact that the CFF universe is more complex it is build using the same input as the CC universe: </p> <pre><code>geniml build-universe ccf --coverage-folder coverage/ \\\n                           --output-file universe_ccf.bed\n</code></pre> <p>Where:</p> <ul> <li><code>--coverage-folder</code>, takes the path to bigWig folder with genome coverage by collection</li> <li><code>--output-file</code>, takes the path to output file</li> </ul> <p>Or we can import it directly into Python: <pre><code>from geniml.universe.ccf_universe import ccf_universe\n\nccf_universe(\"tests/consenus/coverage/\",\n        file_out=\"tests/consenus/universe/universe.bed\")\n</code></pre></p>"},{"location":"geniml/tutorials/create-consensus-peaks/#maximum-likelihood-universe","title":"Maximum likelihood universe","text":"<p>In the previous examples both CC anf CCF universes used simple likelihood model to calculate the cutoff. However, we also developed more complex likelihood model that takes into account the positions of starts and ends of the regions in the collection. This LH model can build based on coverage files and number of analyzed files:</p> <pre><code>geniml lh build_model --model-file model.tar \\\n                      --coverage-folder coverage/ \\\n                      --file-no `wc -l file_list.txt`\n</code></pre> <p>The resulting tar archiver contains LH model. This model can be used as a scoring function that assigns to each position probability of it being a start, core or end of a region. It can be both used for universe assessment and universe building. Combination of LH model and optimization algorithm for building flexible universes results in maximum likelihood universe (ML):</p> <pre><code>geniml build-universe ml --model-file model.tar \\\n                         --coverage-folder coverage/ \\\n                         --output-file universe_ml.bed \n</code></pre>"},{"location":"geniml/tutorials/create-consensus-peaks/#hmm","title":"HMM","text":"<p>The forth presented method of creating universes utilizes Hidden Markov Models (HMM). In this approach the parts of flexible regions are hidden states of the model, while genome coverage by the collections are emissions. The resulting universe is called Hidden Markov Model universe. It can be build only based on the genome coverage by the collection:</p> <pre><code>geniml build-universe hmm --coverage-folder coverage/ \\\n                          --output-file universe_hmm.bed\n</code></pre>"},{"location":"geniml/tutorials/create-consensus-peaks/#how-to-assess-new-universe","title":"How to assess new universe?","text":"<p>So far you used many different methods for creating new universes. But choosing, which universe represents data the best can be challenging. To help with this we created a tutorial that can be found here, which presents different  methods that assess universe fit to the collection of files.</p>"},{"location":"geniml/tutorials/evaluation/","title":"How to evaluate genomic region embeddings","text":""},{"location":"geniml/tutorials/evaluation/#preparation","title":"Preparation","text":""},{"location":"geniml/tutorials/evaluation/#create-a-base-embedding-object","title":"Create a Base Embedding Object","text":"<p>Given a set of genomic region embeddings <code>embeddings</code> and the corresponding regions <code>vocab</code>, use <code>BaseEmbeddings</code> to create an <code>base</code> embedding object.</p> <pre><code>from geniml.eval.utils import BaseEmbeddings\nimport pickle\n\nbase_obj = BaseEmbeddings(embeddings, vocab)\nwith open(\"base_embed.pt\", \"wb\") as f:\n    pickle.dump(base_obj, f)\n</code></pre>"},{"location":"geniml/tutorials/evaluation/#generate-binary-embeddings","title":"Generate Binary Embeddings","text":"<pre><code>from geniml.eval.utils import get_bin_embeddings\n\nuniverse_file = \"/path/to/universe.bed\"\ntoken_files = [\"file1.bed\", \"file2.bed\"]\nbin_embed = get_bin_embeddings(universe_file, token_files)\n</code></pre> <p>Or use command line:</p> <pre><code>geniml eval bin-gen --universe /path/to/universe.bed --token-folder /path/to/tokenized/folder --file-name bin_embed.pickle\n</code></pre>"},{"location":"geniml/tutorials/evaluation/#statistical-tests","title":"Statistical Tests","text":""},{"location":"geniml/tutorials/evaluation/#cluster-tendency-test-ctt","title":"Cluster Tendency Test (CTT)","text":"<p>CTT analyzes how well a set of region embeddings can be clustered.  CTT score lies between 0 and 1. A larger CTT score indicates a greater tendency for the embeddings being evaluated to have clusters. When the embeddings are uniformly distributed, the score is 0.5. For evenly spaced embeddings, the score approaches 0.</p> <pre><code>from geniml.eval.ctt import get_ctt_score, ctt_eval\n\npath = \"/path/to/a/region2vec/model/\"\nembed_type = \"region2vec\"\nctt_score = get_ctt_score(path, embed_type, seed=42, num_data=10000, num_workers=10)\nprint(ctt_score)\n\n# evaluate a batch of models and run CTT for 5 times with different random seeds\nbatch = [(path, embed_type)]\nctt_score_arr = ctt_eval(batch, num_runs=5, num_data=10000, num_workers=10)\nprint(f\"Model: {ctt_score_arr[0][0]}\\n CTT scores:{ctt_score_arr[0][1]}\")  # CTT scores for the 1st model in the batch\n</code></pre> <p>Or use the command line <pre><code>geniml eval ctt --model-path /path/to/a/region2vec/model/ --embed-type region2vec\n</code></pre></p>"},{"location":"geniml/tutorials/evaluation/#reconstruction-test-rct","title":"Reconstruction Test (RCT)","text":"<p>RCT evaluates how well an embedding of a region preserves the region\u2019s occurrence information in the training data. The best RCT score is 1.</p> <pre><code>from geniml.eval.rct import get_rct_score, rct_eval\n\npath = \"/path/to/a/region2vec/model/\"\nembed_type = \"region2vec\"\nbin_path = \"/path/to/a/binary/embedding/for/the/same/tokenized/files/\"\n# set out_dim to -1 use all the dimensions of the binary embeddings. Set out_dim to a small positive number to reduce computational complexity.\nrct_score = get_rct_score(path, embed_type, bin_path, out_dim=-1, cv_num=5, seed=42, num_workers=10)\nprint(rct_score)\n\n# evaluate a batch of models and run RCT for 5 times with different random seeds\nbatch = [(path, embed_type, bin_path)]\nrct_score_arr = rct_eval(batch, num_runs=5, cv_num=5, out_dim=-1, num_workers=10)\nprint(f\"Model: {rct_score_arr[0][0]}\\n RCT scores:{rct_score_arr[0][1]}\")  # RCT scores for the 1st model in the batch\n</code></pre> <p>Or use the command line  <pre><code>geniml eval rct --model-path /path/to/a/region2vec/model/ --embed-type region2vec\n</code></pre> To change the learning setting, go to the definition of <code>get_rct_score</code> in <code>geniml/eval/rct.py</code> and change the constructor of <code>MLPRegressor</code>.</p>"},{"location":"geniml/tutorials/evaluation/#biological-tests","title":"Biological Tests","text":""},{"location":"geniml/tutorials/evaluation/#genome-distance-scaling-test-gdst","title":"Genome Distance Scaling Test (GDST)","text":"<p>GDST calculates a score measuring how much the embedding distance between two regions scales the corresponding genome distance.</p> <pre><code>from geniml.eval.gdst import get_gdst_score, gdst_eval\n\npath = \"/path/to/a/region2vec/model/\"\nembed_type = \"region2vec\"\ngdst_score = get_gdst_score(path, embed_type, num_samples=10000, seed=42)\nprint(gdst_score)\n\n# evaluate a batch of models and run GDST for 5 times with different random seeds\nbatch = [(path, embed_type)]\ngdst_score_arr = gdst_eval(batch, num_runs=5, num_samples=10000)\n</code></pre> <p>Or use the command line  <pre><code>geniml eval gdst --model-path /path/to/a/region2vec/model/ --embed-type region2vec\n</code></pre></p>"},{"location":"geniml/tutorials/evaluation/#neighborhood-preserving-test-npt","title":"Neighborhood Preserving Test (NPT)","text":"<p>NPT evaluates how significant genomic region embeddings preserve their neighboring regions on the genome against random embeddings. The code output the NPT score for a set of region embeddings.</p> <pre><code>from geniml.eval.npt import get_npt_score, npt_eval\n\npath = \"/path/to/a/region2vec/model/\"\nembed_type = \"region2vec\"\nK = 10\n# If resolution = K gives NPT for K neighbors\n# If resolution &lt; K, gives NPT for [resolution, resolution*2, ...] neighbors\nresolution = K\nnpt_score = get_npt_score(path, embed_type, K, num_samples=100, seed=0, resolution=resolution, num_workers=10)\nprint(npt_score['SNPR'])\n\n# evaluate a batch of models and run NPT for 5 times with different random seeds\nbatch = [(path, embed_type)]\nnpt_score_arr = npt_eval(batch, K, num_samples=100, num_workers=10, num_runs=5, resolution=resolution)\nprint(f\"Model: {npt_score_arr[0][0]}\\n NPT scores: {npt_score_arr[0][1]}\")  # NPT scores for the 1st model in the batch\n</code></pre> <p>Or use the command line (the output will be the result when resolution=K) <pre><code>geniml eval npt --model-path /path/to/a/region2vec/model/ --embed-type region2vec --K 50 --num-samples 1000\n</code></pre></p>"},{"location":"geniml/tutorials/fine-tune-region2vec-model/","title":"How to fine-tune a Region2Vec model (Very experimental)","text":""},{"location":"geniml/tutorials/fine-tune-region2vec-model/#overview","title":"Overview","text":"<p>Fine-tuning a model is a way to adapt a pre-trained model to a new task. For example, we may want to fine-tune a model trained using unsupervised learning and ChIP-seq data to predict enhancers. This tutorial discusses how to fine-tune a pre-trained model. To learn how to train a new model see the region2vec training documentation.</p>"},{"location":"geniml/tutorials/fine-tune-region2vec-model/#get-a-pretrained-model","title":"Get a pretrained model","text":"<p>To begin, we need to get a pretrained model. We can get one from huggingface: <pre><code>from geniml.region2vec.main import Region2VecExModel\n\nmodel = Region2VecExModel(\"databio/r2v-ChIP-atlas-hg38-v2\")\n</code></pre> This will download the model from huggingface and load it into memory. The model is now ready to be fine-tuned. First we need to create a new classifier using the pretrained model: <pre><code>import torch\nimport torch.nn as nn\nimport torch.functional as F\n\n# enhancer classifier\nclass EnhancerClassifier(nn.Module):\n    def __init__(self, region2vec_model: torch.nn.Module):\n        super().__init__()\n        self.region2vec = region2vec_model\n        self.classification = nn.Sequential(\n            nn.Linear(region2vec_model.embedding_dim, 1),\n        )\n\n    def forward(self, x: torch.Tensor):\n        x = self.region2vec(x)  # Get the embeddings from Region2Vec\n        x = x.mean(dim=0)  # Average the embeddings (if multiple regions are passed in, this can occur due to tokenization)\n        x = nn.ReLU()(x)  # Pass through a non-linearity\n        x = self.classification(x)  # Pass through additional layers\n        return x\n</code></pre> After instantiating the tokenizer, we can can use the model like so: <pre><code>from geniml.io import Region\nfrom geniml.tokenization import TreeTokenizer\n\nr = Region(\"chr1\", 1_000_000, 1_000_500) # some enhancer region (maybe)\n\ntokenizer = TreeTokenizer.from_pretrained(\"databio/r2v-ChIP-atlas-hg38-v2\")\nclassifier = EnhancerClassifier(model.model) # get the inner core of the model\n\nx = tokenizer.tokenize(r)\nx = torch.tensor([t.id for t in x], dtype=torch.long)\nout = classifier(x)\n\nout.shape # torch.Size([1])\n\n# apply sigmoid\nout = torch.sigmoid(out)\n\nprint(\"Enhancer probability:\", round(out.item(), 3))\n</code></pre></p>"},{"location":"geniml/tutorials/fine-tune-region2vec-model/#saving-the-fine-tuned-embeddings","title":"Saving the fine-tuned embeddings","text":"<p><code>torch</code>'s computational graph links the original region2vec model back to the <code>Region2VecExModel</code>. Therefore, if we want to save the fine-tuned embeddings, we simply need to call <code>export</code> on the original model: <pre><code>model.export(\"my-fine-tuned-model\")\n</code></pre></p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/","title":"Using our models with SnapATAC2","text":""},{"location":"geniml/tutorials/integrate-with-snapatac2/#overview","title":"Overview","text":"<p>SnapATAC2 is a flexible, versatile, and scalable single-cell omics analysis framework. It is designed to process and analyze single-cell ATAC-seq data. SnapATAC2 is written in Rust with Python bindings. It seamlessly integrates with <code>scanpy</code> and <code>anndata</code> objects. Therefore, it is extremely easy to use <code>geniml</code> models with SnapATAC2. Here's how you can do it:</p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/#install-tools","title":"Install tools","text":"<p>Ensure that you have <code>geniml</code> and <code>SnapATAC2</code> installed. You can install both using <code>pip</code>: <pre><code>pip install geniml snapatac2\n</code></pre></p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/#download-some-data","title":"Download some data","text":"<p>To get started, let's download some single-cell ATAC-seq data. We will use the 10x Genomics PBMC 10k dataset. The dataset contains 10,000 peripheral blood mononuclear cells (PBMCs) from a healthy donor.</p> <p>You can easily grab the fragment files like so: <pre><code>wget \"https://cf.10xgenomics.com/samples/cell-atac/2.1.0/10k_pbmc_ATACv2_nextgem_Chromium_Controller/10k_pbmc_ATACv2_nextgem_Chromium_Controller_fragments.tsv.gz\" -O pbmc_fragments.tsv.gz\n</code></pre></p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/#pre-process-with-snapatac2","title":"Pre-process with SnapATAC2","text":"<p>Lets start by pre-processing the data with SnapATAC2. We will closely follow the SnapATAC2 tutorial to get the data into an <code>anndata</code> object.</p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/#import-the-data","title":"Import the data","text":"<p>Lets import the data into <code>snapatac2</code>: <pre><code>from pathlib import Path\nimport snapatac2 as snap\n\nfragment_file = Path(\"pbmc_fragments.tsv.gz\")\ndata = snap.pp.import_data(\n    fragment_file,\n    chrom_sizes=snap.genome.hg38,\n    file=\"pbmc.h5ad\",  # Optional\n    sorted_by_barcode=False,\n)\n</code></pre></p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/#run-some-basic-quality-control","title":"Run some basic quality control","text":"<p>Using the <code>snapatac2</code> quality control functions, we can quickly assess the quality of the data:</p> <pre><code>snap.pl.frag_size_distr(data, interactive=False)\nfig = snap.pl.frag_size_distr(data, show=False)\nfig.update_yaxes(type=\"log\")\nfig.show()\n\nsnap.metrics.tsse(data, snap.genome.hg38)\nsnap.pl.tsse(data, interactive=False)\n\nsnap.pp.filter_cells(data, min_counts=5000, min_tsse=10, max_counts=100000)\n</code></pre> <p>Next, we can add a tile matrix to the data, select features, and run <code>scrublet</code> which is a doublet detection algorithm: <pre><code>snap.pp.add_tile_matrix(data)\nsnap.pp.select_features(data, n_features=250000)\nsnap.pp.scrublet(data)\n\n# actually filter the cells\nsnap.pp.filter_doublets(data)\n</code></pre></p> <p>With this, we have a clean <code>anndata</code> object that we can use with <code>geniml</code>.</p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/#analyze-with-geniml","title":"Analyze with geniml","text":"<p>We will use a Region2Vec model to cluster the cells by generating embeddings. This PBMC data comes from peripheral blood mononuclear cells (PBMCs) from a healthy donor. As such. we will use the <code>databio/r2v-luecken2021-hg38-v2</code> model to generate embeddings because it contains embeddings for the Luecken2021 dataset, a first-of-its-kind multimodal benchmark dataset of 120,000 single cells from the human bone marrow of 10 diverse donors measured with two commercially-available multi-modal technologies: nuclear GEX with joint ATAC, and cellular GEX with joint ADT profiles.</p> <pre><code>import numpy as np\nimport scanpy as sc\nfrom geniml.scembed import ScEmbed\n\nadata = sc.read_h5ad(\"pbmc.h5ad\")\nmodel = ScEmbed(\"databio/r2v-luecken2021-hg38-v2\")\n\nadata.obsm['scembed_X'] = np.array(model.encode(adata))\n</code></pre> <p>With the embeddings, we can run a usual workflow like UMAP, clustering, and visualization: <pre><code>sc.pp.neighbors(adata, use_rep=\"scembed_X\")\nsc.tl.umap(adata)\n\nsc.tl.leiden(adata)\nsc.pl.umap(adata, color=\"leiden\")\n</code></pre></p> <p>And that's it! You've now used <code>geniml</code> with SnapATAC2. You can use the embeddings to annotate cell types, or perform other analyses. If you want to learn more about this, check out the cell-type annotation tutorial.</p>"},{"location":"geniml/tutorials/load-qdrant-with-cell-embeddings/","title":"How to load a vector database with cell embeddings","text":""},{"location":"geniml/tutorials/load-qdrant-with-cell-embeddings/#overview","title":"Overview","text":"<p>In this tutorial, we will show how to load a vector database with cell embeddings. There are many benefits to storing cell-embeddings in a vector database: 1. Speed: Loading a vector database is much faster than re-encoding cells. 2. Reproducibility: You can share your cell embeddings with others. 3. Flexibility: You can use the same cell embeddings for many different analyses. 4. Interoperability: You can use the same cell embeddings with many different tools.</p> <p>In a subsequent tutorial, we will show how to use a vector database to query cell embeddings and annotate cells with cell-type labels using a KNN classification algorithm.</p>"},{"location":"geniml/tutorials/load-qdrant-with-cell-embeddings/#preqrequisites","title":"Preqrequisites","text":"<p>There are two core components to this tutorial: 1) the pre-trained model, and 2) the vector database.</p> <p>Pre-trained model: I will be using the <code>databio/luecken2021</code> model. It was trained on the Luecken2021 dataset, a first-of-its-kind multimodal benchmark dataset of 120,000 single cells from the human bone marrow of 10 diverse donors measured with two commercially-available multi-modal technologies: nuclear GEX with joint ATAC, and cellular GEX with joint ADT profiles.</p> <p>Vector database: Vector databases are a new and exciting technology that allow you to store and query high-dimensional vectors very quickly. This tutorial will use the <code>qdrant</code> vector database. As a lab, we really like <code>qdrant</code> because it is fast, easy to use, and has a great API. You can learn more about <code>qdrant</code> here. For <code>qdrant</code> setup, please refer to the qdrant documentation. In the end, you should have a running <code>qdrant</code> instance at <code>http://localhost:6333</code>.</p>"},{"location":"geniml/tutorials/load-qdrant-with-cell-embeddings/#data-preparation","title":"Data preparation","text":"<p>Grab a fresh copy of the Luecken2021 data from the geo accession. We want the <code>multiome</code> data. This dataset contains the binary accessibility matrix, the peaks, and the barcodes. It also conveniently contains the cell-type labels. Pre-trained models also requires that the data be in a <code>scanpy.AnnData</code> format and the <code>.var</code> attribute contain <code>chr</code>, <code>start</code>, and <code>end</code> values.</p> <pre><code>import scanpy as sc\n\nadata = sc.read_h5ad(\"path/to/adata.h5ad\")\nadata = adata[:, adata.var['feature_types'] == 'ATAC']\n</code></pre>"},{"location":"geniml/tutorials/load-qdrant-with-cell-embeddings/#getting-embeddings","title":"Getting embeddings","text":"<p>We can easily get embeddings of the dataset using the pre-trained model:</p> <pre><code>import scanpy as sc\n\nfrom geniml.scembed import ScEmbed\n\nadata = sc.read_h5ad(\"path/to/adata.h5ad\")\n\nmodel = ScEmbed(\"databio/r2v-luecken2021-hg38-v2\")\nembeddings = model.encode(adata)\n\nadata.obsm['scembed_X'] = np.array(embeddings)\n</code></pre>"},{"location":"geniml/tutorials/load-qdrant-with-cell-embeddings/#loading-the-vector-database","title":"Loading the vector database","text":"<p>With the embeddings, we can now upsert them to <code>qdrant</code>. Ensure you have <code>qdrant_client</code> installed:</p> <pre><code>pip install qdrant-client\n</code></pre> <pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.http.models import Distance, VectorParams, PointStruct\n\nclient = QdrantClient(\"localhost\", port=6333)\n\nclient.create_collection(\n    collection_name=\"luecken2021\",\n    vectors_config=VectorParams(size=embeddings.shape[1], distance=Distance.DOT),\n)\n\nembeddings, cell_types = adata.obsm['scembed_X'], adata.obs['cell_type']\n\npoints = []\nfor embedding, cell_type, i in zip(embeddings, cell_types, range(len(embeddings)):\n    points.append(\n        PointStruct(\n            id=adata.obs.index[i],\n            vector=embedding.tolist(),\n            payload={\"cell_type\": cell_type}\n\n    ))\n\n\nclient.upsert(collection_name=\"luecken2021\", points=points, wait=True)\n</code></pre> <p>You should now have a vector database with cell embeddings. In the next tutorial, we will show how to use this vector database to query cell embeddings and annotate cells with cell-type labels using a KNN classification algorithm.</p>"},{"location":"geniml/tutorials/pre-tokenization/","title":"Pre-tokening training data","text":""},{"location":"geniml/tutorials/pre-tokenization/#overview","title":"Overview","text":"<p>Before we train a model, we must do what is called pre-tokenization. Pre-tokenziation is the process of converting raw genomic region data into lists of tokens and saved into a special file format we call <code>.gtok</code> (genomic token) files. These are binary files that contain the tokenized data in the form of integers. The <code>.gtok</code> files are used directly to train the model. There are several benefits to this, including: 1. Speed: The tokenization process can be slow, especially when you have a lot of data. By pre-tokenizing the data, you only have to do this once. Then, you can use the <code>.gtok</code> files to train the model as many times as you want. 2. Memory: The <code>.gtok</code> files are much smaller than the original data. This means that you can store more data in memory and train larger models. Moreover, this enables streaming the data from disk, which is useful when you have a lot of data. 3. Reproducibility: By saving the tokenized data, you can ensure that the same data is used to train the model every time. This is important for reproducibility.</p>"},{"location":"geniml/tutorials/pre-tokenization/#how-to-pretokenize-data","title":"How to pretokenize data","text":"<p>Pretokenizing data is easy. You can use the built-in tokenizers and utilities in <code>geniml</code> to do this. Here is an example of how to pretokenize a bed file:</p> <pre><code>from genimtools.utils import write_tokens_to_gtok\nfrom geniml.tokenization import TreeTokenizer\n\n# instantiate a tokenizer\ntokenizer = TreeTokenizer(\"path/to/universe.bed\")\n\n# get tokens\ntokens = tokenizer.tokenize(\"path/to/bedfile.bed\")\nwrite_tokens_to_gtok(\"path/to/bedfile.gtok\", tokens.to_ids())\n</code></pre> <p>Thats it! Now you can use the <code>.gtok</code> file to train a model.</p>"},{"location":"geniml/tutorials/pre-tokenization/#how-to-use-the-gtok-files","title":"How to use the <code>.gtok</code> files","text":"<p>To facilitate working with <code>.gtok</code> files, we have some helper-classes that can be used to train a model directly from <code>.gtok</code> files. For example, you can use the <code>Region2VecDataset</code> class to load the <code>.gtok</code> files and train a model. See the training documentation for more information.</p> <pre><code>from geniml.region2vec.utils import Region2VecDataset\n\ntokens_dir = \"path/to/tokens\"\ndataset = Region2VecDataset(tokens_dir)\n\nfor tokens in dataset:\n    # train the model\n    print(tokens) # [42, 101, 99, ...]\n</code></pre>"},{"location":"geniml/tutorials/pre-tokenization/#caveats-and-considerations","title":"Caveats and considerations","text":"<p>When pretokenizing data, you should consider the following: 1. Tokens are specific to the universe that the tokenizer was trained on. If you use a different universe, you will get different tokens. This means that you should use the same universe to pretokenize the data as you will use to train the model. 2. The <code>.gtok</code> files are binary files. This means that they are not human-readable. You should keep the original bed files as well as the <code>.gtok</code> files. This is important for reproducibility and for debugging.</p>"},{"location":"geniml/tutorials/region2vec/","title":"How to train Region2Vec interval embeddings","text":"<p><code>Region2Vec</code> is an unsupervised method for creating embeddings for genomic regions and region sets from a set of raw BED files. The program will first map all raw regions to a given universe (vocabulary) set. Then, it will construct sentences by concatenating regions from a BED file in random order. The generated sentences will be used for Region2Vec training using word2vec.</p>"},{"location":"geniml/tutorials/region2vec/#usage","title":"Usage","text":"<ol> <li>Prepare a set of bed files in <code>src_folder</code>. [Optional] If only a subset of files will be used, specify a list of those files as <code>file_list</code>. By default, the program will use all the files in the folder to train a Region2Vec model.</li> <li>Prepare a universe file <code>universe_file</code>.</li> <li>Create a token folder which will be used to store tokenized files <code>dst_folder</code>.</li> <li>Run the following command <pre><code>from geniml.tokenization import hard_tokenization\nfrom geniml.region2vec import region2vec\n\nsrc_folder = '/path/to/raw/bed/files'\ndst_folder = '/path/to/tokenized_files'\nuniverse_file = '/path/to/universe_file'\n\n# must run tokenization first\nstatus = hard_tokenization(src_folder, dst_folder, universe_file, 1e-9)\n\nif status: # if hard_tokenization is successful, then run Region2Vec training\n    save_dir = '/path/to/training/results'\n    region2vec(dst_folder, save_dir, num_shufflings=1000)\n</code></pre> For customized settings, please go and check the parameters used in <code>main.py</code>.  For training a Region2Vec model, the parameters, <code>init_lr</code>, <code>window_size</code>, <code>num_shufflings</code>, <code>embedding_dim</code>, are frequently tuned in experiments.</li> </ol> <p>For command line usage, type <code>geniml region2vec --help</code> for details. We give a simple usage below</p> <pre><code>geniml region2vec \n  --token-folder /path/to/token/folder \\\n  --save-dir ./region2vec_model \\\n  --num-shuffle 10 \\\n  --embed-dim 100 \\\n  --context-len 50 \n</code></pre>"},{"location":"geniml/tutorials/text2bednn-search-interface/","title":"How to create a natural language search backend for BED files","text":"<p>The metadata of each BED file is needed to build a natural language search backend. BED files embedding vectors are created by <code>Region2Vec</code> model, and metadata embedding vectors are created by <code>FastEmbed</code>, <code>SentenceTransformers</code>, or other text embedding models.</p> <p><code>Vec2VecFNN</code>, a feedforward neural network (FNN), is trained to maps vectors from the embedding space of natural language to the embedding space of BED files. When a natural language query string is given, it will first be encoded to a vector by the text embedding model, and then created  vector will be encoded to a query vector by the FNN. <code>search</code> backend can perform k-nearest neighbors (KNN) search among the stored BED file embedding vectors, and the BED files whose embedding vectors are closest to that query vector are the search results.</p>"},{"location":"geniml/tutorials/text2bednn-search-interface/#search-distance-metrics","title":"Search distance metrics","text":"<p>The default distance metrics for KNN search in <code>geniml</code> is cosine similarity. Which is bounded in [0,1]. The smaller the value is, the higher similarity between the query vector and returned search results. HNSWBackend and QdrantBackend also have other options of distance metrics.</p>"},{"location":"geniml/tutorials/text2bednn-search-interface/#store-embedding-vectors","title":"Store embedding vectors","text":"<p>It is recommended to use <code>geniml.search.backend.HNSWBackend</code> to store embedding vectors. In the <code>HNSWBackend</code> that stores each BED file embedding vector, the <code>payload</code> should contain the name or identifier of BED file. In the <code>HNSWBackend</code> that stores the embedding vectors of each  metadata string, the <code>payload</code> should contain the original string text and the names of BED files that have that string in metadata.</p>"},{"location":"geniml/tutorials/text2bednn-search-interface/#train-the-model","title":"Train the model","text":"<p>Training a <code>Vec2VecFNN</code> needs x-y pairs of vectors (x: metadata embedding vector; y: BED embedding vector). A pair of a metadata embedding vector with the embedding vectors of BED files in its payload is a target pair, otherwise a non-target pair. Non-target pairs are sampled for contrastive loss. Here is sample code to generate pairs from storage backend and train the model:</p> <pre><code># target is an array of 1 (target) and -1 (non-target) \nX, Y, target = vec_pairs(\n    nl_backend,  # HNSWBackend that store metadata embedding vectors\n    bed_backend,  # HNSWBackend that store BED embedding vectors\n    \"name\",  # key to file name in BED backend payloads\n    \"files\",  # key to matching files in metadata backend payloads\n    True,  # sample non-target pairs\n    1.0  # number of non-target pairs /number of target pairs = 1\n)\n\n# train without validate data\nv2v_torch_contrast.train(\n    X,\n    Y,\n    folder_path=\"path/to/folder/for/checkpoint\",\n    loss_func=\"cosine_embedding_loss\",  # right now \"cosine_embedding_loss\" is the only contrastive loss function available\n    training_target=target,\n)\n</code></pre>"},{"location":"geniml/tutorials/text2bednn-search-interface/#search-interface","title":"Search interface","text":"<p>A search interface consists of a storage backend where vectors are stored, and a module (<code>geniml.search.query2vec</code>) that embed the query. <code>geniml.search</code> supports two types of queries: region set query and text query. </p>"},{"location":"geniml/tutorials/text2bednn-search-interface/#region-set-query","title":"Region set query","text":"<p><code>BED2Vec</code> embed the query region set with a <code>Region2VecExModel</code>, and the embedding vector is used to perform KNN search within the backend.</p> <pre><code>from geniml.search import BED2BEDSearchInterface, BED2Vec\n\n# init BED2Vec with a hugging face repo of a Region2VecExModel\nbed2vec = BED2Vec(\"databio/r2v-ChIP-atlas-hg38-v2\")\n\n# the search_backend can be QdrantBackend or HNSWBackend\nsearch_interface = BED2BEDSearchInterface(search_backend, bed2vec)\n\n# the query cam be a RegionSet object (see geniml.io) or path to a BED file in disk\nfile_search_result = search_interface.query_search(\"path/to/a/bed/file.bed\", 5)\n</code></pre>"},{"location":"geniml/tutorials/text2bednn-search-interface/#text-query","title":"Text query","text":"<p><code>Text2Vec</code> embed the query string with a with a natural language embedding model first (default: <code>FlagEmbedding</code>), and then maps the text embedding vector into the embedding space of region sets through a trained <code>Vec2VecFNN</code>.</p> <pre><code>from geniml.search import Text2BEDSearchInterface, Text2Vec\n\ntext2vec = Text2Vec(\n    \"sentence-transformers/all-MiniLM-L6-v2\",  # either a hugging face repo or an object from geniml.text2bednn.embedder\n    \"databio/v2v-geo-hg38\"  # either a hugging face repo or a Vec2VecFNN\n)\n\nsearch_interface = Text2BEDSearchInterface(search_backend, text2vec)\ntext_search_result = search_interface.query_search(\"cancer cells\", 5)\n</code></pre> <p>With a dictionary that contains query strings and id of relevant query results in search backend in this format:</p> <pre><code>{\n    &lt;query string&gt;: [\n        &lt;id of relevant result in backend&gt;,\n        ...    \n    ],\n    ...\n}\n</code></pre> <p><code>Text2BEDSearchInterface</code> can return mean average precision, average AUC-ROC, and average R-Precision, here is example code:</p> <pre><code>query_dict = {\n    \"metadata string 1\": [2, 3],\n    \"metadata string 12\": [1],\n    \"metadata string 3\": [2, 4, 5],\n    \"metadata string 1\": [0]\n}\n\nMAP, AUC, RP = search_interface.eval(query_dict)\n</code></pre>"},{"location":"geniml/tutorials/text2bednn-search-interface/#hugging-face","title":"Hugging Face","text":""},{"location":"geniml/tutorials/text2bednn-search-interface/#model","title":"Model","text":"<p><code>Vec2VecFNN</code> can be innitiated from a Hugging Face repository:</p> <pre><code>model = Vec2VecFNN(\"databio/v2v-sentencetransformers-encode\")\n</code></pre> <p>To upload the model onto huggingface, you can use <code>export</code> function to download the files of model(checkpoint.pt) and config(config.yaml).</p> <pre><code>v2v_torch1.export(\"path/totarget/folder\", \"checkpoint.pt\")\n</code></pre> <p>Then upload both files with correct names onto the Hugging Face repository</p>"},{"location":"geniml/tutorials/text2bednn-search-interface/#dataset","title":"Dataset","text":"<p><code>geniml.search.anecdotal_search_from_hf_data</code> can allow users to query with free-form natural language strings to search in a Hugging Face dataset. The dataset must have:</p> <ul> <li>hnsw index file of BED file embeddings (index.bin)</li> <li>dictionary file of payloads (payloads.pkl). It must have file name stored with the key of \"file\". For example:</li> </ul> <pre><code># key of the payload is the storage index in the hnsw index\n{\n    0: {\n        \"file\": \"Example.bed\",\n        ...\n    },\n    ...\n}\n</code></pre> <ul> <li>metadata file (metadata.json) in this format: <code>{&lt;metadata attribute&gt;: {&lt;annotation text&gt;: [&lt;files&gt;]}}</code>. For example:</li> </ul> <pre><code>{\n    \"tissue\": {\n        \"kidney\": [\n            \"Example.bed\",\n            ...\n        ],\n        ...\n    },\n    ...\n}\n</code></pre> <p>With the repo name of dataset, <code>Vec2VecFNN</code>, and repo name of the model that was used to encode training metadata, you can search through the dataset with any free-form query you type:</p> <pre><code>from geniml.search import anecdotal_search_from_hf_data\nimport pprint\n\n# vec2vec model\nsearch_repo = \"databio/v2v-sentencetransformers-encode\"\n# text encoder model\ntext_repo = \"sentence-transformers/all-MiniLM-L6-v2\"\n# dataset\ndata_repo = \"databio/geo-hg38-search-test\"\nresult = anecdotal_search_from_hf_data(\n    \"glioblastoma\",\n    data_repo,\n    search_repo,\n    text_repo,\n    10\n)\n\npprint.pprint(result)\n</code></pre>"},{"location":"geniml/tutorials/train-region2vec/","title":"How to train a new Region2Vec model","text":"<p>Region2Vec is an unsupervised method for creating embeddings of genomic regions and genomic region sets. This tutorial discusses how to train a new model. To learn how to use a pre-trained model see the region2vec usage documentation.</p>"},{"location":"geniml/tutorials/train-region2vec/#training-data-and-universe","title":"Training data and universe","text":"<p>Training a model requires two things: 1) a set of pre-tokenized data and 2) a universe. The universe is a set of regions that the model will be trained on. The universe is used to create the tokenizer, which is used to convert the raw data into tokens. The universe should be representative of the data that you will be training the model on. For example, if you are training a model on human data, you should use a universe that contains human regions. If you dont have a universe, a safe bet is to use the 1000 tiles hg38 genome.</p> <p>You can download the 1000 tiles hg38 genome here.</p> <p>The pre-tokenized data is a set of <code>.gtok</code> files. These are binary files that contain the tokenized data in the form of integers. The <code>.gtok</code> files are used directly to train the model. If you have not pre-tokenized your data, see the pre-tokenization documentation.</p>"},{"location":"geniml/tutorials/train-region2vec/#training-a-model","title":"Training a model","text":""},{"location":"geniml/tutorials/train-region2vec/#instantiate-a-new-model","title":"Instantiate a new model","text":"<p>To begin, create a new model from <code>Region2VecExModel</code>.</p> <p>Note: We use the <code>Region2VecExModel</code> because it is an extension of the <code>Region2Vec</code> class that comes with its own tokenizer. This ensures that the model and tokenizer are compatible.</p> <pre><code>import logging\nimport os\nfrom multiprocessing import cpu_count\n\nfrom geniml.io import RegionSet\nfrom geniml.tokenization import TreeTokenizer\nfrom geniml.region2vec.main import Region2VecExModel\nfrom rich.progress import track\n\n\nlogging.basicConfig(level=logging.INFO)\n\n# get the paths to data\nuniverse_path = os.path.expandvars(\"$RESOURCES/regions/genome_tiles/tiles1000.hg38.bed\")\ndata_path = os.path.expandvars(\"$DATA/ChIP-Atlas/hg38/ATAC_seq/tokens\")\n\nmodel = Region2VecExModel(\n    tokenizer=TreeTokenizer(universe_path),\n)\n</code></pre>"},{"location":"geniml/tutorials/train-region2vec/#training-data","title":"Training data","text":"<p>The training data is a set of <code>.gtok</code> files. You can use the <code>Region2VecDataset</code> class to load the <code>.gtok</code> files and train the model. <pre><code>from geniml.region2vec.utils import Region2VecDataset\n\ndataset = Region2VecDataset(data_path)\n</code></pre></p>"},{"location":"geniml/tutorials/train-region2vec/#training","title":"Training","text":"<p>Now, simply give the model the list of <code>RegionSet</code>s and run the training: <pre><code># train the model\nmodel.train(dataset, epochs=100)\n</code></pre></p> <p>You can export your model using the <code>export</code> function:</p> <pre><code>model.export(\"out\")\n</code></pre> <p>These files are intended to be directly uploaded to huggingface. You can upload them using the <code>huggingface-cli</code> or the huggingface website.</p>"},{"location":"geniml/tutorials/train-scembed-model/","title":"How to train a single-cell model with scEmbed","text":"<p>This example walks you through training an <code>scembed</code> region2vec model on a single-cell dataset. We start with data preparation, then train the model, and finally use the model to cluster the cells.</p> <p>For this example we are using the 10x Genomics PBMC 10k dataset. The dataset contains 10,000 peripheral blood mononuclear cells (PBMCs) from a healthy donor.</p>"},{"location":"geniml/tutorials/train-scembed-model/#installation","title":"Installation","text":"<p>Simply install the parent package <code>geniml</code> from PyPi:</p> <pre><code>pip install geniml\n</code></pre> <p>Then import <code>scEmbed</code> from <code>geniml</code>:</p> <pre><code>from geniml.scembed.main import ScEmbed\n</code></pre>"},{"location":"geniml/tutorials/train-scembed-model/#data-preparation","title":"Data preparation","text":"<p><code>scembed</code> requires that the input data is in the AnnData format. Moreover, the <code>.var</code> attribute of this object must have <code>chr</code>, <code>start</code>, and <code>end</code> values. The reason is two fold: 1) we can track which vectors belong to which genmomic regions, and 2) region vectors are now reusable. We need three files: 1) The <code>barcodes.txt</code> file, 2) the <code>peaks.bed</code> file, and 3) the <code>matrix.mtx</code> file. These will be used to create the <code>AnnData</code> object. To begin, download the data from the 10x Genomics website:</p>"},{"location":"geniml/tutorials/train-scembed-model/#todo-this-needs-to-be-the-filtered_peak_bc_matrix-not-the-raw_peak_bc_matrix","title":"TODO: This needs to be the filtered_peak_bc_matrix, not the raw_peak_bc_matrix","text":"<pre><code>wget https://cf.10xgenomics.com/samples/cell-atac/2.1.0/10k_pbmc_ATACv2_nextgem_Chromium_Controller/10k_pbmc_ATACv2_nextgem_Chromium_Controller_filtered_peak_bc_matrix.tar.gz\ntar -xzf 10k_pbmc_ATACv2_nextgem_Chromium_Controller_filtered_peak_bc_matrix.tar.gz\n</code></pre> <p>Your files will be inside <code>filtered_peak_bc_matrix/</code>. Assuming you've installed the proper dependencies, you can now use python to build the <code>AnnData</code> object:</p> <pre><code>import pandas as pd\nimport scanpy as sc\n\nfrom scipy.io import mmread\nfrom scipy.sparse import csr_matrix\n\nbarcodes = pd.read_csv(\"barcodes.tsv\", sep=\"\\t\", header=None, names=[\"barcode\"])\npeaks = pd.read_csv(\"peaks.bed\", sep=\"\\t\", header=None, names=[\"chr\", \"start\", \"end\"])\nmtx = mmread(\"matrix.mtx\")\nmtx_sparse = csr_matrix(mtx)\nmtx_sparse = mtx_sparse.T\n\nadata = sc.AnnData(X=mtx_sparse, obs=barcodes, var=peaks)\nadata.write_h5ad(\"pbmc.h5ad\")\n</code></pre> <p>We will use the <code>pbmc.h5ad</code> file for downstream work.</p>"},{"location":"geniml/tutorials/train-scembed-model/#training","title":"Training","text":"<p>Training an <code>scEmbed</code> model requires two key steps: 1) pre-tokenizing the data, and 2) training the model.</p>"},{"location":"geniml/tutorials/train-scembed-model/#pre-tokenizing-the-data","title":"Pre-tokenizing the data","text":"<p>To learn more about pre-tokenizing the data, see the pre-tokenization tutorial. Pre-tokenization offers many benefits, the two most important being 1) speeding up training, and 2) lower resource requirements. The pre-tokenization process is simple and can be done with a combination of <code>geniml</code> and <code>gtars</code> utilities. Here is an example of how to pre-tokenize the 10x Genomics PBMC 10k dataset:</p> <pre><code>import pyarrow as pa\nimport pyarrow.parquet as pq\n\nfrom geniml.tokenization.utils import tokenize_anndata\nfrom gtars.tokenizers import Tokenizer\n\nadata = sc.read_h5ad(\"path/to/adata.h5ad\")\ntokenizer = Tokenizer(\"peaks.bed\")\n\n# ensure that the data is in csr format (this speeds up tokenization)\nadata.X = adata.X.tocsr()\ntokenized_cells = tokenize_anndata(\n    adata,\n    tokenizer\n)\ncells = [t['input_ids'] for t in tokenized_cells]\ntable = pa.table({'tokens': pa.array(cells, type=pa.list_(pa.int32()))})\npq.write_table(table, '/path/to/tokens.parquet')\n</code></pre>"},{"location":"geniml/tutorials/train-scembed-model/#training-the-model","title":"Training the model","text":"<p>Now that the data is pre-tokenized, we can train the model. The <code>scEmbed</code> model is designed to be used with <code>scanpy</code>. Here is an example of how to train the model:</p> <pre><code>from geniml.region2vec.utils import Region2VecDataset\n\ndataset = Region2VecDataset(\"/path/to/tokens.parquet\")\n\nmodel = ScEmbed(tokenizer=tokenizer)\nmodel.train(dataset, epochs=100)\n</code></pre> <p>We can then export the model for upload to huggingface:</p> <pre><code>model.export(\"path/to/model\")\n</code></pre>"},{"location":"geniml/tutorials/train-scembed-model/#get-embeddings-of-single-cells","title":"Get embeddings of single-cells","text":"<p><code>scEmbed</code> is simple to use and designed to be used with <code>scanpy</code>. Here is a simple example of how to train a model and get cell embeddings:</p> <pre><code>model = ScEmbed.from_pretrained(\"path/to/model\")\nmodel = ScEmbed(\"databio/scembed-pbmc10k\")\n\nadata = sc.read_h5ad(\"path/to/adata.h5ad\")\nembeddings = model.encode(adata)\n\nadata.obsm[\"scembed_X\"] = embeddings\n</code></pre>"},{"location":"geniml/tutorials/train-scembed-model/#clustering-the-cells","title":"Clustering the cells","text":"<p>With the model now trained, and cell-embeddings obtained, we can get embeddings of our individual cells. You can use <code>scanpy</code> utilities to cluster the cells:</p> <pre><code>sc.pp.neighbors(adata, use_rep=\"scembed_X\")\nsc.tl.leiden(adata) # or louvain\n</code></pre> <p>And visualize with UMAP</p> <pre><code>sc.tl.umap(adata)\nsc.pl.umap(adata, color=\"leiden\")\n</code></pre>"},{"location":"geniml/tutorials/use-pretrained-region2vec-model/","title":"How to use a pre-trained Region2Vec model","text":"<p>Region2Vec is an unsupervised method for creating embeddings of genomic regions and genomic region sets. This tutorial discusses how to use pre-trained models. To learn how to train a new model see the region2vec training documentation We make available several pre-trained models available on our huggingface repo. These models can be used to create embeddings of genomic regions and region sets without having to train a new model.</p>"},{"location":"geniml/tutorials/use-pretrained-region2vec-model/#get-the-model","title":"Get the model","text":"<p>To use one of our pre-trained models, simply import the <code>Region2VecExModel</code> and download the model from huggingface:</p> <pre><code>from geniml.io import Region\nfrom geniml.region2vec.main import Region2VecExModel\n\nmodel = Region2VecExModel(\"databio/r2v-encode-hg38\")\n</code></pre> <p>Note: We use the <code>Region2VecExModel</code> class to load the model because it is an extension of the <code>Region2Vec</code> class that comes with its own tokenizer. This ensures that the model and tokenizer are compatible.</p>"},{"location":"geniml/tutorials/use-pretrained-region2vec-model/#encode-regions","title":"Encode regions","text":"<p>Now we can encode a genomic region or a list of regions:</p> <pre><code>region = Region(\"chr1\", 160100, 160200)\nregions = [region] * 10\n\nembedding = model.encode(region) # get one region embedding\nembeddings = model.encode(regions) # or, get many embeddings\n</code></pre> <p>We can also encode an entire bed file, which will return region embeddings for each region in the file:</p> <pre><code>bed = \"/path/to/bed/file.bed\"\n\nembeddings = model.encode(bed)\n</code></pre> <p>Note: It is possible that a region can not be tokenized by the tokenizer. This is because the tokenizer was instantiated with a specific set of regions. If this is the case, the model simply returns the unknown token (<code>chrUNK-0:0</code>). If you find that this is happening often, you may want to ensure that your regions are a good fit for the universe of regions that the model was trained on. The unknown token will indeed have an embedding, but it will not be a meaningful representation of the region.</p>"},{"location":"geniml/tutorials/use-pretrained-region2vec-model/#region-pooling","title":"Region pooling","text":"<p>It is often the case that we want a single embedding that represents a set of regions. For example, we may want to encode a patient by taking the average embedding of all the SNPs in the patient's genome. We can do this by simply averaging across the embeddings of the regions:</p> <pre><code>patient_snps = \"/path/to/bed/file.bed\"\n\nembeddings = model.encode(patient_snps) \npatient_embedding = np.mean(embeddings, axis=0)\n</code></pre>"},{"location":"geniml/tutorials/use-pretrained-scembed-model/","title":"How to use a pre-trained scEmbed model","text":"<p>One advantage of scEmbed is the ability to use pre-trained models. This is useful for quickly getting embeddings of new data without having to train a new model. In this tutorial, we will show how to use a pre-trained model to get embeddings of new data.</p> <p>I will be using the <code>databio/luecken2021</code> model. It was trained on the Luecken2021 dataset, a first-of-its-kind multimodal benchmark dataset of 120,000 single cells from the human bone marrow of 10 diverse donors measured with two commercially-available multi-modal technologies: nuclear GEX with joint ATAC, and cellular GEX with joint ADT profiles.</p> <p>This model will work best on PBMC-like data. It also requires your fragments be aligned to the GRCh38 genome.</p>"},{"location":"geniml/tutorials/use-pretrained-scembed-model/#example-data-preparation","title":"Example data preparation","text":"<p>Grab a fresh set of PBMC data from 10X genomics: https://www.10xgenomics.com/resources/datasets/10k-human-pbmcs-atac-v2-chromium-controller-2-standard</p> <p>You need the Peak by cell matrix (filtered). This  contains the binary accessibility matrix, the peaks, and the barcodes. Pre-trained models also requires that the data be in a <code>scanpy.AnnData</code> format and the <code>.var</code> attribute contain <code>chr</code>, <code>start</code>, and <code>end</code> values. For details on how to make this, see data preparation.</p> <p>Once your data is ready, you can load it into python and get embeddings.</p>"},{"location":"geniml/tutorials/use-pretrained-scembed-model/#encoding-cells","title":"Encoding cells","text":"<p>Encoding cells is as easy as:</p> <pre><code>import scanpy as sc\n\nfrom geniml.scembed import ScEmbed\n\n\nadata = sc.read_h5ad(\"path/to/adata.h5ad\")\nmodel = ScEmbed(\"databio/luecken2021\")\n\nembeddings = model.encode(adata)\nadata.obsm['scembed_X'] = embeddings\n</code></pre> <p>And, thats it! You can now cluster your cells using the <code>scembed_X</code> embeddings.</p>"},{"location":"gtars/","title":"Gtars","text":""},{"location":"gtars/#introduction","title":"Introduction","text":"<p><code>gtars</code> is a high-performance toolkit for genomic tools and algorithms in Rust. Built with Rust for speed and reliability, gtars provides core utilities for machine learning on genomic intervals for the geniml Python package. It also provides lots of utility as a standalone library for alternative downstream use cases.</p>"},{"location":"gtars/#installation","title":"Installation","text":""},{"location":"gtars/#rust-library","title":"Rust Library","text":"<p>Gtars uses a feature-flag system to allow you to include only the modules you need. Add to your <code>Cargo.toml</code>:</p> <pre><code>[dependencies]\n# Install specific features\ngtars = { version = \"0.5\", features = [\"overlaprs\", \"tokenizers\"] }\n\n# Or install from GitHub\ngtars = { git = \"https://github.com/databio/gtars\", features = [\"overlaprs\", \"tokenizers\"] }\n</code></pre> <p>Modules:</p> <ul> <li><code>core</code> - Core functionality and data structures</li> <li><code>tokenizers</code> - Genomic region tokenizers</li> <li><code>io</code> - I/O utilities</li> <li><code>refget</code> - Reference sequence access</li> <li><code>overlaprs</code> - Overlap operations</li> <li><code>uniwig</code> - Coverage computation</li> <li><code>igd</code> - Interval search</li> <li><code>bbcache</code> - BED file caching</li> <li><code>scoring</code> - Fragment scoring</li> <li><code>fragsplit</code> - Fragment splitting</li> </ul> <p>Example combinations:</p> <pre><code># For machine learning tasks\ngtars = { version = \"0.5\", features = [\"tokenizers\", \"core\"] }\n\n# For genomic analysis\ngtars = { version = \"0.5\", features = [\"overlaprs\", \"uniwig\", \"scoring\"] }\n\n# For data access\ngtars = { version = \"0.5\", features = [\"refget\", \"bbcache\", \"io\"] }\n</code></pre>"},{"location":"gtars/#python-package","title":"Python Package","text":"<pre><code>pip install gtars\n</code></pre> <p>See further documentation under Python bindings.</p>"},{"location":"gtars/#command-line-interface","title":"Command-Line Interface","text":"<p>Install from source: <pre><code>git clone https://github.com/databio/gtars\ncd gtars\ncargo install --path gtars-cli\n</code></pre></p> <p>Or download precompiled binaries from the releases page.</p>"},{"location":"gtars/#development","title":"Development","text":"<p>Run tests with <code>cargo test</code> from the workspace root. Please see CONTRIBUTING.md for development guidelines.</p>"},{"location":"gtars/#module-organization","title":"Module organization","text":"<p><code>gtars</code> is organized into modules. The modules section gives an overview of each module.</p>"},{"location":"gtars/bbcache/","title":"BED file caching and loading from BEDbase","text":""},{"location":"gtars/bbcache/#introduction","title":"Introduction","text":"<p>With <code>gtars.bbcache</code>, users can download and cache BED files and BED sets from the BEDbase API to their local disk, as well as process, search, and remove cached files.</p> <p>This document provides tutorials for using <code>gtars.bbcache</code> via either:</p> <ol> <li>the Rust interface, or</li> <li>the command-line interface.</li> </ol>"},{"location":"gtars/bbcache/#rust-interface","title":"Rust interface","text":""},{"location":"gtars/bbcache/#create-an-instance-of-the-bbclient-class","title":"Create an instance of the BBClient Class:","text":"<pre><code>use gtars::bbcache::client::BBClient;\nuse std::path::PathBuf;\n\nlet cache_folder = PathBuf::from(\"cache\");\nlet mut bbclient = BBClient::new(Some(cache_folder), None).expect(\"Failed to create BBClient\");\n</code></pre>"},{"location":"gtars/bbcache/#download-and-cache-a-remote-bed-file-from-bedbase-load-a-bed-file-from-cache","title":"Download and cache a remote BED file from BEDbase / Load a BED file from cache","text":"<pre><code>let bedfile_id = \"...\"; // find interesting bedfile on bedbase\n// download, cache and return a RegionSet object\n// or return a RegionSet from a cached BED file\nlet regionset = bbclient.load_bed(bedfile_id).expect(\"Failed to load bed file\");\n</code></pre>"},{"location":"gtars/bbcache/#cache-a-local-bed-file","title":"Cache a local BED file","text":"<pre><code>// compute its ID and add it to the cache\nlet bedfile_id = bbclient.add_local_bed_to_cache(PathBuf::from(\"path/to/bedfile\"), Some(false)).unwrap();\n</code></pre>"},{"location":"gtars/bbcache/#cache-a-bed-file-from-within-python-memory","title":"Cache a BED file from within Python memory","text":"<p>You can also provide a URL and it will add to cache for you:</p> <pre><code>use gtars::common::models::RegionSet;\n\nlet file_url = String::from(\"https://...\");\nlet regionset = RegionSet::try_from(file_url)?;\nlet bedfile_id = bbclient.add_regionset_to_cache(regionset, Some(false)).unwrap();\n</code></pre>"},{"location":"gtars/bbcache/#download-and-cache-a-bedset-from-bedbase","title":"Download and cache a BEDset from BEDbase","text":"<pre><code>let bedset_identifier = \"xyz\"; // find some interesting bedset on bedbase.org\nlet bedset = bbclient.load_bedset(bedset_identifier).unwrap(); //download, cache and return a BedSet object\n</code></pre>"},{"location":"gtars/bbcache/#cache-a-local-bedset","title":"Cache a local BEDset","text":"<pre><code>let bedset_id = bbc.add_local_folder_as_bedset(PathBuf::from(\"path/go/bed/files/folder\")).unwrap();\n</code></pre>"},{"location":"gtars/bbcache/#seek-for-a-cached-bed-file-bedset","title":"Seek for a cached BED file / BEDset","text":"<pre><code>let bedset_id = \"...\";\nlet bedfile_id = \"...\";\n\nlet cached_bed_path = bbclient.seek(bedfile_id).expect(\"Failed to seek cached bed file\");\nlet cached_bedset_path = bbclient.seek(bedset_id).expect(\"Failed to seek cached bed file\");\n</code></pre>"},{"location":"gtars/bbcache/#remove-a-cached-bed-file-bedset","title":"Remove a cached BED file / BEDset","text":"<pre><code>let bedset_id = \"...\";\nlet bedfile_id = \"...\";\n\nbbc.remove(bedset_id).expect(\"Failed to remove bedset file and its bed files\");\nbbc.remove(bedfile_id).expect(\"Failed to remove bedset file and its bed files\");\n</code></pre>"},{"location":"gtars/bbcache/#command-line-interface","title":"Command line interface","text":""},{"location":"gtars/bbcache/#overall","title":"Overall","text":"<pre><code>Downloads, processes, and caches BED files from the BEDbase API\n\nUsage: bbcache &lt;COMMAND&gt;\n\nCommands:\n  cache-bed           Cache a BED file from local file or BEDbase\n  cache-bedset        Cache a BED set from local file or BEDbase\n  seek                Seek the BED file path by giving identifier\n  inspect-bedfiles    Inspect the contents of bedfile cache folder\n  inspect-bedsets     Inspect the contents of bedsets cache folder\n  rm                  Remove the BED file or BED set from cache with given identifier\n  help                Print this message or the help of the given subcommand(s)\n\nOptions (for applicable subcommands):\n  -i, --identifier &lt;identifier&gt;    BED file or BED set identifier, URL, or file path\n  -f, --cache-folder &lt;cache-folder&gt;  Cache folder path\n  -h, --help                       Print help\n</code></pre>"},{"location":"gtars/bbcache/#cache-bed-file","title":"Cache BED file","text":"<pre><code>gtars bbcache cache-bed -i &lt;BED_file_or_identifier_or_url&gt;\n</code></pre> <p>The <code>&lt;BED_file_or_identifier_or_url&gt;</code> variable can be one of 3 things:</p> <ol> <li>a path to a local BED file;</li> <li>a BED record identifier from BEDbase; or,</li> <li>a URL to a BED file hosted anywhere.</li> </ol>"},{"location":"gtars/bbcache/#cache-bedset","title":"Cache BEDset","text":"<pre><code>gtars bbcache cache-bedset -i &lt;BED_files_folder_or_identifier&gt;\n</code></pre> <p>The <code>&lt;BED_files_folder_or_identifier&gt;</code> variable may be:</p> <ol> <li>local path to a folder containing BED files; or,</li> <li>a BEDbase BEDset identifier</li> </ol>"},{"location":"gtars/bbcache/#seek-the-path-of-a-bed-file-or-bedset-in-cache-folder","title":"Seek the path of a BED file or BEDset in cache folder","text":"<p>To retrieve the local file path to a BED file stored locally,</p> <pre><code>gtars bbcache seek -i &lt;identifier&gt;\n</code></pre> <p>Replace  with the identifier of the BED file or BEDset you want to seek."},{"location":"gtars/bbcache/#count-the-subdirectories-and-files-in-bedfiles-bedsets-folder","title":"Count the subdirectories and files in <code>bedfiles</code> &amp; <code>bedsets</code> folder","text":"<pre><code>gtars bbcache inspect-bedfiles\ngtars bbcache inspect-bedsets\n</code></pre>"},{"location":"gtars/bbcache/#remove-a-bed-file-or-bedset-from-the-cache-folder","title":"Remove a BED file or BEDset from the cache folder","text":"<pre><code>gtars bbcache rm  &lt;identifier&gt;\n</code></pre> <p>Replace  with the identifier of the BED file or BEDset you want to remove."},{"location":"gtars/bbcache/#cache-folder","title":"Cache Folder","text":"<p>By default, the downloaded and processed BED files are cached in the bed_cache folder. You can specify a different cache folder using the <code>--cache-folder</code> argument, or set the environment variable <code>$BBCLIENT_CACHE_ENV</code>. The cache folder has this structure: <pre><code>cache_folder\n  bedfiles\n    a/b/ab1234xyz.bed.gz\n    ..\n  bedsets\n    c/d/cd123hij.txt\n</code></pre></p>"},{"location":"gtars/changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"gtars/changelog/#051-2025-10-08","title":"[0.5.1] -- 2025-10-08","text":"<ul> <li>introduce web assembly bindings</li> <li>introduce R bindings</li> </ul>"},{"location":"gtars/changelog/#050-2025-09-25","title":"[0.5.0] -- 2025-09-25","text":"<ul> <li>converted the codebase into a workspace with multiple crates</li> </ul>"},{"location":"gtars/changelog/#040-2025-09-10","title":"[0.4.0] -- 2025-09-10","text":"<ul> <li>added <code>get_max_end_per_chr</code> and <code>nucleotides_length</code> methods to <code>RegionSet</code></li> </ul>"},{"location":"gtars/changelog/#030-2025-07-30","title":"[0.3.0] -- 2025-07-30","text":"<ul> <li>move digests functionality to refget</li> <li>add RefgetStore to refget and its associated python bindings</li> <li>integrate support for <code>bits</code> + backend types for tokenizers (AIList or BITS)</li> <li>reworked the tokenization CLI to support the new <code>bits</code> and <code>backend</code> options</li> </ul>"},{"location":"gtars/changelog/#025-2025-04-06","title":"[0.2.5] -- 2025-04-06","text":"<ul> <li>Rework tokenizer API to be more consistent with the HuggingFace tokenizers API.</li> <li>Updates to <code>RegionSet</code> to improve performance and usability.</li> <li>Added file_digest function to RegionSet struct</li> <li>Fixed reqwest error in R bindings</li> <li>Fixed #107</li> </ul>"},{"location":"gtars/changelog/#024-2025-03-05","title":"[0.2.4] -- 2025-03-05","text":"<ul> <li>Attempt to fix failing python bindings in CI linux #104</li> </ul>"},{"location":"gtars/changelog/#023-2025-03-05","title":"[0.2.3] -- 2025-03-05","text":"<ul> <li>Improved RegionSet, by adding a multiple new methods: <code>to_bed</code>, <code>to_bed_gz</code>, <code>to_bigbed</code>, <code>identifier()</code>, and others.</li> <li>Fixed allowed <code>fasta_digest</code> to accept <code>Path</code> or <code>bytes</code> #93</li> </ul>"},{"location":"gtars/changelog/#022-2025-02-18","title":"[0.2.2] -- 2025-02-18","text":"<ul> <li>fix #90</li> <li>fix #89</li> </ul>"},{"location":"gtars/changelog/#021-2025-02-11","title":"[0.2.1] -- 2025-02-11","text":"<ul> <li>allow comments at the beginning of fragment files</li> <li>bump bigtools to 0.5.5, fixing #74 and #77</li> </ul>"},{"location":"gtars/changelog/#020-2025-01-13","title":"[0.2.0] -- 2025-01-13","text":"<ul> <li>add position shift workflow for bam to bw (was previously added for bam to bed)</li> <li>add scaling argument for bam to bw workflow #53</li> <li>fix accumulation issue for bam workflow #56</li> <li>fix wiggle file (core) beginning at 0 #43</li> <li>fix npy file (end) using start instead of end #61</li> <li>force zoom to 1 for bed/narrowPeak to bw #34</li> <li>fix IGD overlap issue #45</li> <li>add ga4gh refget digest functionality #58</li> <li>fix wig and npy inconsistency #64</li> <li>fix narrowPeak to bw zoom  #34</li> <li>fix bed to bw fileheader consistency issue  #52</li> <li>change npy metadata file structure #65</li> </ul>"},{"location":"gtars/changelog/#012","title":"[0.1.2]","text":"<ul> <li>add position shift workflow for <code>bam</code> to <code>bw</code> (was previously added for <code>bam</code> to <code>bed</code>)</li> <li>add scaling argument for <code>bam</code> to <code>bw</code> workflow #53</li> <li>fix accumulation issue for <code>bam</code> workflow #56</li> <li>fix wiggle file (core) beginning at 0 #43</li> <li>fix npy file (end) using start instead of end #61</li> <li>force zoom to 1 for bed/narrowPeak to bw #34</li> <li>fix IGD overlap issue #45</li> <li>add ga4gh refget digest functionality #58</li> </ul>"},{"location":"gtars/changelog/#011-2024-12-03","title":"[0.1.1] -- 2024-12-03","text":"<ul> <li>hot fix for broken python bindings; remove IGD from the python bindings for now</li> </ul>"},{"location":"gtars/changelog/#010-2024-12-03","title":"[0.1.0] -- 2024-12-03","text":"<ul> <li>Rust implementation of <code>uniwig</code> that expands on the C++ version</li> <li>Uniwig now accepts a single sorted  <code>.bed</code> file, <code>.narrowPeak</code> file, or <code>.bam</code> file.</li> <li>Outputs now include  <code>.wig</code>, <code>.npy</code>, <code>.bedGraph</code>, and <code>.bw</code></li> <li>Accumulations can now be counted via <code>.narrowPeak</code> scoring</li> <li>Rust implementation of <code>igd</code> ported from the C version (experimental).</li> <li>Region scoring matrix calculation for region clustering</li> <li>Fragment file splitter for pseudobulking</li> </ul>"},{"location":"gtars/changelog/#0015-2024-07-29","title":"[0.0.15] -- 2024-07-29","text":"<ul> <li>added meta tokenization tools and a new <code>MetaTokenizer</code> struct that can be used to tokenize regions using the meta-token strategy.</li> <li>added some annotations to the <code>pyo3</code> <code>#[pyclass]</code> and <code>#[pymethods]</code> attributes to make the python bindings more readable.</li> </ul>"},{"location":"gtars/changelog/#0014-2024-06-11","title":"[0.0.14] -- 2024-06-11","text":"<ul> <li>renamed repository to <code>gtars</code> to better reflect the project's goals.</li> </ul>"},{"location":"gtars/changelog/#0013-2024-06-03","title":"[0.0.13] -- 2024-06-03","text":"<ul> <li>implemented a fragment file tokenizer that will generate <code>.gtok</code> files directly from <code>fragments.tsv.gz</code> files.</li> <li>fix an off-by-one error in the <code>region-to-id</code> maps in the <code>Universe</code> structs. This was leading to critical bugs in our models.</li> </ul>"},{"location":"gtars/changelog/#0012-2024-05-28","title":"[0.0.12] -- 2024-05-28","text":"<ul> <li>optimize creation of <code>PyRegionSet</code> to reduce expensive cloning of <code>Universe</code> structs.</li> </ul>"},{"location":"gtars/changelog/#0011-2024-05-22","title":"[0.0.11] -- 2024-05-22","text":"<ul> <li>redesigned API for the tokenizers to better emulate the huggingface tokenizers API.</li> <li>implemented new traits for tokenizers to allow for more flexibility when creating new tokenizers.</li> <li>bumped the version <code>pyo3</code> to <code>0.21.0</code></li> <li>added <code>rust-numpy</code> dependency to the python bindings for exporting tokenized regions as numpy arrays.</li> <li>overall stability improvements to the tokenizers and the python bindings.</li> </ul>"},{"location":"gtars/changelog/#0010-2024-01-24","title":"[0.0.10] -- 2024-01-24","text":"<ul> <li>update file format specifications</li> </ul>"},{"location":"gtars/changelog/#009-2024-01-22","title":"[0.0.9] -- 2024-01-22","text":"<ul> <li>start working on the concept of a <code>.gtok</code> file-format to store tokenized regions</li> <li>added basic readers and writers for this format</li> </ul>"},{"location":"gtars/changelog/#008-2024-01-17","title":"[0.0.8] -- 2024-01-17","text":"<ul> <li>add a new <code>ids_as_strs</code> getter to the <code>TokenizedRegionSet</code> struct so that we can get the ids as strings quickly, this is meant mostly for interface with geniml.</li> </ul>"},{"location":"gtars/changelog/#007-2023-11-30","title":"[0.0.7] -- 2023-11-30","text":"<ul> <li>move things around based on rust club feedback</li> </ul>"},{"location":"gtars/changelog/#006-2024-02-20","title":"[0.0.6] -- 2024-02-20","text":"<ul> <li>update python bindings to support the module/submodule structure (https://github.com/PyO3/pyo3/issues/759#issuecomment-1828431711)</li> <li>change name of some submodules</li> <li>remove <code>consts</code> submodule, just add to base</li> <li>expose a <code>__version__</code> attribute in the python bindings</li> </ul>"},{"location":"gtars/changelog/#005-2024-02-19","title":"[0.0.5] -- 2024-02-19","text":"<ul> <li>add many \"core utils\"</li> <li>move <code>gtokenizers</code> into this package inside <code>gtars::tokenizers</code></li> <li>create <code>tokenize</code> cli</li> <li>add tests for core utils and tokenizers</li> <li>RegionSet is now backed by a polars DataFrame</li> <li>new python bindings for core utils and tokenizers</li> </ul>"},{"location":"gtars/changelog/#004-2023-11-06","title":"[0.0.4] -- 2023-11-06","text":"<ul> <li>add type annotations to the python bindings</li> </ul>"},{"location":"gtars/changelog/#003-2023-11-06","title":"[0.0.3] -- 2023-11-06","text":"<ul> <li>work on python bindings initialization</li> </ul>"},{"location":"gtars/changelog/#002-2023-09-20","title":"[0.0.2] -- 2023-09-20","text":"<ul> <li>prepare for first release</li> </ul>"},{"location":"gtars/changelog/#001-2023-08-15","title":"[0.0.1] -- 2023-08-15","text":"<ul> <li>initial setup of repository</li> <li>two main wrappers: 1) wrapper binary crate, and 2) wrapper library crate</li> <li><code>gtars</code> can be used as a library crate. or as a command line tool</li> </ul>"},{"location":"gtars/cli/","title":"Gtars command line interface (CLI)","text":"<p>We've packaged some gtars functionality into a command line interface (CLI) for easy use in shell scripts and terminal environments. The CLI provides access to various genomic tools and utilities built on top of the core gtars Rust library.</p>"},{"location":"gtars/cli/#installation","title":"Installation","text":"<p>To facilitate fast compilation and broad compatibility, the CLI is feature-gated. You can enable only the tools you need to minimize dependencies, binary size, and build time.</p> <p>There are two main ways to install the gtars CLI:</p>"},{"location":"gtars/cli/#via-cargo-rusts-package-manager","title":"Via Cargo (Rust's package manager)","text":"<pre><code>cargo install gtars-cli --features \"uniwig overlaprs igd bbcache scoring fragsplit\"\n</code></pre>"},{"location":"gtars/cli/#from-source","title":"From source","text":"<pre><code>git clone https://github.com/databio/gtars.git\ncd gtars\ncargo install --path gtars-cli --features \"uniwig overlaprs igd bbcache scoring fragsplit\"\n</code></pre> <p>You can also enable all features with: <pre><code>cargo install gtars-cli --all-features\n</code></pre></p>"},{"location":"gtars/cli/#available-commands","title":"Available Commands","text":"<p>The CLI provides the following subcommands (availability depends on features enabled during compilation):</p>"},{"location":"gtars/cli/#igd","title":"igd","text":"<p>Build and query IGD (Integrated Genome Database) indexes: <pre><code>gtars igd create --input regions.bed --output index.igd\ngtars igd query --database index.igd --query chr1:1000-2000\n</code></pre></p>"},{"location":"gtars/cli/#overlaprs","title":"overlaprs","text":"<p>Compute overlaps between genomic intervals: <pre><code>gtars overlaprs --input1 regions1.bed --input2 regions2.bed\n</code></pre></p>"},{"location":"gtars/cli/#uniwig","title":"uniwig","text":"<p>Generate coverage tracks from BED/BAM files: <pre><code>gtars uniwig --input reads.bam --output coverage.bw\n</code></pre></p>"},{"location":"gtars/cli/#bbcache","title":"bbcache","text":"<p>Cache and manage BED files from bedbase.org: <pre><code>gtars bbcache get --id GSM123456\ngtars bbcache list\n</code></pre></p>"},{"location":"gtars/cli/#scoring","title":"scoring","text":"<p>Score fragment overlaps against a reference: <pre><code>gtars scoring --fragments frags.tsv.gz --universe peaks.bed --output scores.txt\n</code></pre></p>"},{"location":"gtars/cli/#fragsplit","title":"fragsplit","text":"<p>Split fragment files by cell barcodes or clusters: <pre><code>gtars fragsplit --fragments frags.tsv.gz --barcodes clusters.csv --output-dir splits/\n</code></pre></p>"},{"location":"gtars/cli/#global-options","title":"Global Options","text":"<pre><code>gtars --help              # Show help\ngtars --version           # Show version\ngtars &lt;command&gt; --help    # Show command-specific help\n</code></pre>"},{"location":"gtars/cli/#building-with-specific-features","title":"Building with Specific Features","text":"<p>To build the CLI with specific tools: <pre><code>cargo build --release --features \"uniwig,overlaprs,igd\"\n</code></pre></p>"},{"location":"gtars/core/","title":"gtars-core","text":"<p>Core library providing fundamental data structures and utilities for genomic interval operations. This is the foundation that all other gtars modules build upon.</p>"},{"location":"gtars/core/#features","title":"Features","text":"<ul> <li>Common genomic data structures (Region, RegionSet)</li> <li>BED file parsing utilities</li> <li>Shared constants and helper functions</li> <li>Foundation for all gtars modules</li> </ul>"},{"location":"gtars/core/#core-data-types","title":"Core Data Types","text":""},{"location":"gtars/core/#region","title":"Region","text":"<p>Represents a genomic interval with chromosome, start, and end coordinates: <pre><code>use gtars_core::models::Region;\n\n// Create a region\nlet region = Region::new(\"chr1\", 1000, 2000);\n\n// Access properties\nprintln!(\"Chr: {}\", region.chr);\nprintln!(\"Start: {}\", region.start);\nprintln!(\"End: {}\", region.end);\n</code></pre></p>"},{"location":"gtars/core/#regionset","title":"RegionSet","text":"<p>Collection of genomic regions: <pre><code>use gtars_core::models::RegionSet;\nuse std::path::Path;\n\n// Load from BED file\nlet rs = RegionSet::try_from(Path::new(\"peaks.bed\"))?;\n\n// Access regions\nprintln!(\"Number of regions: {}\", rs.regions.len());\n\n// Iterate over regions\nfor region in &amp;rs.regions {\n    println!(\"{}: {}-{}\", region.chr, region.start, region.end);\n}\n</code></pre></p>"},{"location":"gtars/core/#available-modules","title":"Available Modules","text":"<ul> <li><code>models</code> - Core data structures (Region, RegionSet)</li> <li><code>utils</code> - Utility functions for file handling and parsing</li> <li><code>consts</code> - Shared constants</li> </ul>"},{"location":"gtars/core/#dependencies","title":"Dependencies","text":"<p>Minimal external dependencies:</p> <ul> <li><code>anyhow</code> - Error handling</li> <li><code>flate2</code> - Gzip compression support</li> <li>Other standard bioinformatics libraries</li> </ul> <p>This module serves as the foundation for all other gtars modules and maintains backward compatibility within major versions.</p>"},{"location":"gtars/fragsplit/","title":"gtars-fragsplit","text":"<p>Split fragment files according to pseudobulk assignments for single-cell ATAC-seq data processing.</p>"},{"location":"gtars/fragsplit/#features","title":"Features","text":"<ul> <li>Efficient fragment file splitting</li> <li>Pseudobulk generation from single-cell data</li> <li>Support for cell type annotations</li> <li>Gzip compression support</li> <li>Memory-efficient streaming</li> </ul>"},{"location":"gtars/fragsplit/#usage","title":"Usage","text":""},{"location":"gtars/fragsplit/#command-line","title":"Command Line","text":"<pre><code>gtars fragsplit \\\n  --fragments fragments.tsv.gz \\\n  --barcodes barcodes.txt \\\n  --output-dir pseudobulk/\n</code></pre>"},{"location":"gtars/fragsplit/#from-python","title":"From Python","text":"<pre><code>from gtars import fragsplit\n\nfragsplit.split_by_clusters(\n    fragments=\"fragments.tsv.gz\",\n    clusters=\"clusters.csv\",\n    output_dir=\"pseudobulk/\"\n)\n</code></pre>"},{"location":"gtars/fragsplit/#input-formats","title":"Input Formats","text":""},{"location":"gtars/fragsplit/#fragment-file","title":"Fragment File","text":"<p>Standard 10x Genomics format:</p> <pre><code>chr1    1000    1500    AAACCCAAGAAACACT-1    1\n</code></pre>"},{"location":"gtars/fragsplit/#barcode-assignment","title":"Barcode Assignment","text":"<pre><code>AAACCCAAGAAACACT-1    cluster1\nAAACCCAAGAAACAGC-1    cluster2\n</code></pre>"},{"location":"gtars/fragsplit/#output","title":"Output","text":"<p>Creates one fragment file per cluster/pseudobulk group with appropriate cell barcodes.</p>"},{"location":"gtars/igd/","title":"gtars-igd","text":"<p>Implementation of IGD (Integrated Genome Database) - a high-performance genomic interval search tool.</p>"},{"location":"gtars/igd/#features","title":"Features","text":"<ul> <li>Fast interval queries (logarithmic time complexity)</li> <li>Memory-efficient indexing</li> <li>Binary format for persistence</li> <li>Support for multiple genomes</li> </ul>"},{"location":"gtars/igd/#usage","title":"Usage","text":"<p>Ensure <code>gtars</code> is compiled with igd: <code>cargo build --release --all-features</code> or <code>cargo build --release --features igd</code></p>"},{"location":"gtars/igd/#building-an-index","title":"Building an Index","text":"<pre><code>gtars igd create --output /home/igd_output/ --filelist /home/my_bedfiles/\n</code></pre>"},{"location":"gtars/igd/#querying-with-a-single-bed-file","title":"Querying with a single bed file","text":"<pre><code>gtars igd search --database my_igd_database.igd --query my_query.bed\n</code></pre>"},{"location":"gtars/igd/#performance","title":"Performance","text":"<ul> <li>Index creation: O(n log n)</li> <li>Query time: O(log n + k) where k is number of results</li> <li>Memory usage: ~30% of input file size</li> </ul>"},{"location":"gtars/igd/#file-format","title":"File Format","text":"<p>IGD uses a binary format optimized for:</p> <ul> <li>Fast loading</li> <li>Compact storage</li> <li>Memory-mapped access</li> </ul>"},{"location":"gtars/install/","title":"\ud83d\udcbf Installing gtars","text":"<p>gtars is available in many forms to support your chosen compute environment and framework. We've extended our core rust implementation into many forms:</p> <ul> <li>Rust Library: The core rust implementation can be used as a library in other Rust projects, allowing for deep integration and customization.</li> <li>Command Line Interface (CLI): The core rust implementation can be installed via cargo. This is the most flexible way to use gtars, and is suitable for any environment that supports rust.</li> <li>Python Package: A Python package is available via pip, making it easy to integrate gtars into Python-based workflows and applications.</li> <li>Wasm/JS Package: A WebAssembly (Wasm) package is available for JavaScript environments, allowing gtars to be used in web applications and other JS-based projects.</li> <li>R bindings (package): R bindings are available for R users, enabling the use of gtars within R scripts and applications.</li> </ul> <p>Each environment has its own installation instructions, which are detailed below.</p>"},{"location":"gtars/install/#rust-library","title":"Rust Library","text":"<p>To use gtars as a Rust library, add the following to your <code>Cargo.toml</code>. We feature-gate optional dependencies to keep the base install small.</p> <pre><code>[dependencies]\ngtars = { version = \"0.5.0\", features = [\"uniwig\", \"tokenizers\"] }\n</code></pre> <p>Then, in your Rust code, you can import and use gtars as follows:</p> <pre><code>use gtars::uniwig;\n// Your code here\n</code></pre> <p>For more details, refer to the gtars Rust documentation.</p>"},{"location":"gtars/install/#command-line-interface-cli","title":"Command Line Interface (CLI)","text":"<p>To install the gtars CLI, you need to have Rust and Cargo installed. You can then install gtars using the following command: <pre><code>cargo install gtars-cli\n</code></pre></p> <p>Similarly, we feature-gate binary dependencies maximize compatibility and minimize install size. You can specify features during installation like so:</p> <pre><code>cargo install gtars-cli --features \"uniwig tokenizers\"\n</code></pre> <p>Additionally, you can add all features with: <pre><code>cargo install gtars-cli --all-features\n</code></pre> Once installed, you can verify the installation by running:</p> <pre><code>gtars --help\n</code></pre>"},{"location":"gtars/install/#python-package","title":"Python Package","text":"<p>To install the gtars Python package, you can use pip. The package is available on PyPI and can be installed with the following command: <pre><code>pip install gtars\n</code></pre></p> <p>You can then import and use gtars in your Python code as follows:</p> <pre><code>from gtars.tokenizers import Tokenizer\n\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-base-hg38\")\n</code></pre>"},{"location":"gtars/install/#wasmjs-package","title":"Wasm/JS Package","text":"<p>To use gtars in a JavaScript environment, you can install the Wasm package via npm. First, ensure you have Node.js and npm installed. Then, you can install the gtars package with the following command: <pre><code>npm install @databio/gtars-js\n</code></pre></p> <p>You can then import and use gtars in your JavaScript code as follows:</p> <pre><code>import { Overlapper } from '@databio/gtars-js';\n\nconst universe = [\n    ['chr1', 100, 200],\n    ['chr1', 150, 250],\n    ['chr2', 300, 400],\n]\n\nconst overlapper = new Overlapper(universe, 'ailist');\nconsole.log(`Using backend: ${overlapper.get_backend()}`);\n\nconst query = ['chr1', 180, 220];\nconst overlaps = overlapper.find(query);\n\nconsole.log(`Overlaps for ${query}:`, overlaps);\n</code></pre>"},{"location":"gtars/install/#r-package","title":"R Package","text":"<p>COMING SOON.</p>"},{"location":"gtars/io/","title":"gtars-io","text":"<p>I/O focused utilities for gtars, providing efficient file format parsers and stream processing.</p>"},{"location":"gtars/io/#features","title":"Features","text":"<ul> <li>BED file parsing and writing</li> <li>Fragment file I/O</li> <li>Compression support (gzip, bgzip)</li> <li>Stream processing for large files</li> <li>Memory-efficient iterators</li> </ul>"},{"location":"gtars/io/#file-formats","title":"File Formats","text":""},{"location":"gtars/io/#bed-files","title":"BED Files","text":"<pre><code>use gtars_io::BedReader;\n\nlet reader = BedReader::from_path(\"regions.bed\")?;\nfor region in reader {\n    println!(\"{:?}\", region?);\n}\n</code></pre>"},{"location":"gtars/io/#fragment-files","title":"Fragment Files","text":"<pre><code>use gtars_io::FragmentReader;\n\nlet fragments = FragmentReader::from_path(\"fragments.tsv.gz\")?;\nfor fragment in fragments {\n    let frag = fragment?;\n    println!(\"{}\\t{}\\t{}\", frag.chr, frag.start, frag.end);\n}\n</code></pre>"},{"location":"gtars/io/#compression","title":"Compression","text":"<p>Automatic detection and handling of compressed files:</p> <ul> <li><code>.gz</code> - gzip compression</li> <li><code>.bgz</code> - bgzip compression</li> <li>Uncompressed files</li> </ul>"},{"location":"gtars/io/#performance","title":"Performance","text":"<ul> <li>Buffered I/O for optimal throughput</li> <li>Zero-copy parsing where possible</li> <li>Parallel decompression support</li> </ul>"},{"location":"gtars/models/","title":"Models and Region Set objects in Gtars","text":"<p>Gtars has multiple objects (structs/models) for representation of genomic regions and other related data. </p>"},{"location":"gtars/models/#region","title":"\ud83d\udfe2 Region","text":"<p>Region is Python representation of a genomic region. e.g. <code>chr1:100-200</code> + additional information.</p>"},{"location":"gtars/models/#example","title":"Example","text":"PythonRust <pre><code>from gtars.models import Region\n\n# Create a Region\ngenomic_region = Region(chr=\"chr1\", \n                         start=100, \n                         end=200, \n                         rest=\"peak1\")\nprint(genomic_region)\n</code></pre> <pre><code>use gtars::models::Region;\n\n// Create a Region\nlet genomic_region: Region = Region { chr: \"chr1\".to_string(), \n                                      start: 100, \n                                      end: 200, \n                                      rest: Some(\"peak1\".to_string()) \n                                    },\nlet identifier = genomic_region.digest();\n\nprintln!(\"{:?}\", identifier);\n</code></pre>"},{"location":"gtars/models/#regionset","title":"\ud83d\udfe2 RegionSet","text":"<p>RegionSet is Python representation of a genomic region set, commonly named as BED file.</p>"},{"location":"gtars/models/#quick-example","title":"Quick example","text":"<p>Open BED file from URL and get its identifier.</p> PythonRustTypeScript <pre><code>from gtars.models import RegionSet\n\n# Create a RegionSet from a url, or lcoal BED file.\nrs = RegionSet(\"https://data2.bedbase.org/files/d/a/dafd661aa70590999e0ff9e1980217db.bed.gz\")\n\n# Get identifier for the RegionSet\nrs.identifier\n\nprint(rs)\n</code></pre> <pre><code>use gtars::models::RegionSet;\n\n// Create a RegionSet from a url, or lcoal BED file.\nlet rs = RegionSet::try_from(\"https://data2.bedbase.org/files/d/a/dafd661aa70590999e0ff9e1980217db.bed.gz\").unwrap();\n\n// Get identifier for the RegionSet\nlet id = rs.identifier();\n\nprintln!(\"{:?}\", rs);\n</code></pre> <p>\u2757 Note: This is test example and may require additional setup to run.</p> <pre><code>import init from '@databio/gtars';\nimport { RegionSet } from '@databio/gtars';\n\ninit();\n\nexport type BedEntry1 = [string, number, number, string];\n\n// Define entries (regions)\nexport const entries1: BedEntry1[] = [\n  ['chr1', 100, 200, 'peak1'],\n  ['chr2', 150, 250, 'peak2'],\n  ['chr3', 300, 400, 'peak3'],\n];\n\n// Create a Region\nconst rs = new RegionSet(entries1);\n\nconsole.log(rs);\n</code></pre> <p>\u2757 Note: RegionSet can be created from a local file path, URL, or by passing a list (vector) or Region objects.</p>"},{"location":"gtars/models/#main-commands-in-python","title":"Main commands in Python","text":"<ul> <li>Load a BED file from local path or URL <pre><code>rs = RegionSet(\"path/to/bedfile.bed\")\n</code></pre></li> <li>Get number of regions <pre><code>len(rs)\n</code></pre></li> <li>Calculate mean reagion width <pre><code>rs.mean_region_width()\n</code></pre></li> <li>Get last base pair location for each chromosome <pre><code>rs.get_max_end_per_ch()\n</code></pre></li> <li>Get number of base pairs in the region set <pre><code>rs.get_nucleotide_length()\n</code></pre></li> <li>Save the regionSet as a BED file <pre><code>rs.to_bed(\"path/to/save/bedfile.bed\")\nrs.to_bed_gz(\"path/to/save/bedfile.bed.gz\")  # gzipped\n</code></pre></li> <li>Save the regionSet as a bigBed file <pre><code>rs.to_bigbed(\"path/to/save/bedfile.bb\", chrom_sizes=\"path/to/chrom.sizes\")\n</code></pre></li> </ul>"},{"location":"gtars/module-vision/","title":"gtars Module Vision","text":""},{"location":"gtars/module-vision/#conceptual-organization","title":"Conceptual Organization","text":"<p>The gtars modules follow a clear separation of concerns:</p>"},{"location":"gtars/module-vision/#1-gtars-overlaprs","title":"1. gtars-overlaprs","text":"<p>Core overlap computation infrastructure - All interval overlap algorithms live here - Single source of truth for overlap operations - Other modules should never reimplement overlap logic</p>"},{"location":"gtars/module-vision/#2-gtars-scoring","title":"2. gtars-scoring","text":"<p>Wraps overlaprs to produce X-by-peak matrices - Handles all matrix generation use cases:   - Cell-by-peak (single-cell)   - Sample-by-peak (bulk)   - Pseudobulk-by-peak (aggregated) - Focuses on matrix data structures and I/O formats</p>"},{"location":"gtars/module-vision/#3-gtars-tokenizers","title":"3. gtars-tokenizers","text":"<p>Wraps overlaprs to produce tokens for ML - Converts genomic regions to vocabulary tokens - Designed for transformer models and ML pipelines - Manages token vocabularies and encoding strategies</p>"},{"location":"gtars/module-vision/#key-principle","title":"Key Principle","text":"<p>All overlap computation flows through gtars-overlaprs. Higher-level modules add domain-specific value: - scoring adds matrix generation - tokenizers adds ML tokenization - Both delegate overlap computation to overlaprs</p>"},{"location":"gtars/modules/","title":"gtars Modules","text":"<p>gtars is organized as a workspace of independent Rust crates, each providing specific functionality for genomic interval analysis. All modules share common dependencies and infrastructure through <code>gtars-core</code>.</p> <p>\ud83d\udcd6 See Module Vision for the conceptual organization and design principles.</p>"},{"location":"gtars/modules/#core-infrastructure","title":"Core Infrastructure","text":"<ul> <li>gtars-core - Fundamental data structures and utilities</li> <li>gtars-io - I/O operations and file format parsers</li> </ul>"},{"location":"gtars/modules/#genomic-analysis","title":"Genomic Analysis","text":"<ul> <li>gtars-overlaprs - High-performance interval overlap operations</li> <li>gtars-uniwig - Coverage computation from BED/BAM files</li> <li>gtars-igd - Fast genomic interval search</li> <li>gtars-scoring - Fragment overlap scoring</li> <li>gtars-fragsplit - Fragment file splitting for pseudobulk</li> </ul>"},{"location":"gtars/modules/#machine-learning","title":"Machine Learning","text":"<ul> <li>gtars-tokenizers - Genomic region tokenizers for ML</li> </ul>"},{"location":"gtars/modules/#data-access","title":"Data Access","text":"<ul> <li>gtars-refget - GA4GH refget protocol implementation</li> <li>gtars-bbcache - BED file caching for bedbase.org</li> </ul>"},{"location":"gtars/modules/#language-bindings","title":"Language Bindings","text":"<ul> <li>gtars-python - Python API bindings</li> <li>gtars-wasm - WebAssembly bindings</li> <li>gtars-cli - Command-line interface</li> </ul>"},{"location":"gtars/overlaprs/","title":"gtars-overlaprs","text":"<p>Core infrastructure for all genomic interval overlap computations.</p>"},{"location":"gtars/overlaprs/#purpose","title":"Purpose","text":"<p>gtars-overlaprs provides the foundational overlap computation primitives that all other modules build upon. This is the single source of truth for efficient interval operations in gtars.</p>"},{"location":"gtars/overlaprs/#features","title":"Features","text":"<ul> <li>Efficient interval intersection algorithms using AIList data structure</li> <li>Coverage calculations with bit vectors</li> <li>Support for large-scale genomic datasets (millions of intervals)</li> <li>Optimized memory usage and logarithmic query time</li> <li>Common <code>Overlapper</code> trait for consistent interface</li> </ul>"},{"location":"gtars/overlaprs/#usage","title":"Usage","text":""},{"location":"gtars/overlaprs/#from-rust","title":"From Rust","text":"<pre><code>use gtars_overlaprs::{AIList, Interval};\n\n// Create an AIList from intervals\nlet mut ailist = AIList::new();\nailist.add(Interval::new(100, 200));\nailist.add(Interval::new(150, 250));\n\n// Query overlaps\nlet overlaps = ailist.query(120, 180);\n</code></pre>"},{"location":"gtars/overlaprs/#data-structures","title":"Data Structures","text":"<ul> <li>AIList - Augmented Interval List for fast overlap queries</li> <li>Bits - Bit vector implementation for coverage calculations</li> <li>Overlapper - Trait for implementing overlap operations</li> </ul>"},{"location":"gtars/overlaprs/#performance","title":"Performance","text":"<p>Optimized for:</p> <ul> <li>Large BED files (millions of intervals)</li> <li>Fast overlap queries (logarithmic time)</li> <li>Memory efficiency through compact data structures</li> </ul>"},{"location":"gtars/python-overview/","title":"gtars-python","text":"<p>Python bindings for gtars functionality, providing native Python API for genomic interval analysis.</p>"},{"location":"gtars/python-overview/#installation","title":"Installation","text":"<pre><code>pip install gtars\n</code></pre>"},{"location":"gtars/python-overview/#available-modules","title":"Available Modules","text":""},{"location":"gtars/python-overview/#tokenizers","title":"tokenizers","text":"<pre><code>from gtars.tokenizers import Tokenizer\n\n# Create tokenizer from BED file\ntokenizer = Tokenizer.from_bed(\"regions.bed\")\n\n# Or from configuration\ntokenizer = Tokenizer.from_config(\"config.toml\")\n\n# Tokenize regions\ntokens = tokenizer.tokenize([\"chr1:1000-2000\", \"chr2:3000-4000\"])\n</code></pre>"},{"location":"gtars/python-overview/#models-regionset","title":"models (RegionSet)","text":"<pre><code>from gtars.models import RegionSet\n\n# Create from BED file\nrs = RegionSet.from_bed(\"peaks.bed\")\n\n# Access properties\nprint(f\"Number of regions: {len(rs)}\")\nprint(f\"Total coverage: {rs.coverage()}\")\n\n# Operations\nrs.sort()\nmerged = rs.merge()\n</code></pre>"},{"location":"gtars/python-overview/#refget","title":"refget","text":"<pre><code>from gtars.refget import RefgetStore, compute_digest\n\n# Compute sequence digest\ndigest = compute_digest(b\"ACGTACGT\")\n\n# Use RefgetStore for sequence management\nstore = RefgetStore()\n</code></pre>"},{"location":"gtars/python-overview/#performance","title":"Performance","text":"<ul> <li>Native Rust performance</li> <li>Zero-copy data transfer where possible</li> <li>NumPy array integration</li> <li>Parallel processing support</li> </ul>"},{"location":"gtars/r/","title":"gtars-r","text":"<p>R bindings for gtars functionality, providing an R API for genomic interval analysis.</p>"},{"location":"gtars/r/#installation","title":"Installation","text":"<p>To install the development version, use the <code>remotes</code> package:</p> <pre><code>install.packages(\"remotes\")\nremotes::install_github(\"databio/gtars\", ref = \"dev\", subdir = \"gtars-r\")\n</code></pre> <p>You can also install the R package locally from the repo directory:</p> <pre><code>R CMD INSTALL gtars-r\n</code></pre>"},{"location":"gtars/r/#available-modules","title":"Available Modules","text":""},{"location":"gtars/r/#refget","title":"refget","text":"<pre><code>library(gtars)\n\n# Compute sequence digest\nreadable &lt;- 'ACGTACGT'\ngtars::sha512t24u_digest(readable)\ngtars::md5_digest(readable)\n\n# Use RefgetStore for sequence management\nstore &lt;- global_refget_store('raw')\n</code></pre>"},{"location":"gtars/r/#igd","title":"IGD","text":"<pre><code>library(gtars)\n\n### Building an Index\nigd_create(output_path = '/home/igd_output/', filelist = '/home/my_bedfiles/')\n\n### Querying with a single bed file\nigd_search(database_path = 'my_igd_database.igd', query_path = 'my_query.bed')\n</code></pre>"},{"location":"gtars/refget-store-format/","title":"RefgetStore File Format","text":"<p>The RefgetStore is a directory-based file format for storing reference genome sequences with content-addressable access via GA4GH refget digests. It provides efficient storage, deduplication, and retrieval of sequences across multiple genome assemblies.</p>"},{"location":"gtars/refget-store-format/#overview","title":"Overview","text":"<p>A RefgetStore is a self-contained directory that stores sequence data in individual files (one per sequence), sequence metadata (names, lengths, digests, alphabets), collection metadata (grouping sequences by genome assembly), and index files for efficient lookup.</p>"},{"location":"gtars/refget-store-format/#directory-structure","title":"Directory Structure","text":"<pre><code>refget-store/\n\u251c\u2500\u2500 index.json                    # Store metadata and configuration\n\u251c\u2500\u2500 sequences.farg                # Index of all sequences\n\u251c\u2500\u2500 sequences/                    # Sequence data files\n\u2502   \u251c\u2500\u2500 Ab/                       # Subdirectories by digest prefix\n\u2502   \u2502   \u251c\u2500\u2500 AbCdEf123....seq     # Individual sequence file\n\u2502   \u2502   \u2514\u2500\u2500 AbXyZ789....seq\n\u2502   \u2514\u2500\u2500 Xy/\n\u2502       \u2514\u2500\u2500 XyZabc456....seq\n\u2514\u2500\u2500 collections/                  # Collection metadata\n    \u251c\u2500\u2500 collection1.farg\n    \u2514\u2500\u2500 collection2.farg\n</code></pre>"},{"location":"gtars/refget-store-format/#file-specifications","title":"File Specifications","text":""},{"location":"gtars/refget-store-format/#indexjson","title":"index.json","text":"<p>The root metadata file containing store configuration.</p> <p>Location: <code>&lt;store-root&gt;/index.json</code></p> <p>Format: JSON</p> <p>Schema: <pre><code>{\n  \"version\": 1,\n  \"seqdata_path_template\": \"sequences/%s2/%s.seq\",\n  \"collections_path_template\": \"collections/%s.farg\",\n  \"sequence_index\": \"sequences.farg\",\n  \"mode\": \"Encoded\",\n  \"created_at\": \"2025-01-15T10:30:00Z\"\n}\n</code></pre></p> <p>Fields:</p> <ul> <li><code>version</code> (integer): Format version number (currently <code>1</code>)</li> <li><code>seqdata_path_template</code> (string): Template for sequence file paths</li> <li><code>%s</code> = full digest string</li> <li><code>%s2</code> = first 2 characters of digest</li> <li><code>%s4</code> = first 4 characters of digest</li> <li>Example: <code>\"sequences/%s2/%s.seq\"</code> \u2192 <code>\"sequences/Ab/AbCdEf123....seq\"</code></li> <li><code>collections_path_template</code> (string): Template for collection file paths</li> <li>Example: <code>\"collections/%s.farg\"</code></li> <li><code>sequence_index</code> (string): Path to the sequence metadata index file</li> <li>Default: <code>\"sequences.farg\"</code></li> <li><code>mode</code> (string): Storage mode for sequence data</li> <li><code>\"Raw\"</code>: Uncompressed sequence data</li> <li><code>\"Encoded\"</code>: Bit-packed encoded sequences (space efficient)</li> <li><code>created_at</code> (string): ISO 8601 timestamp of store creation</li> </ul>"},{"location":"gtars/refget-store-format/#sequencesfarg","title":"sequences.farg","text":"<p>Master index of all sequences in the store.</p> <p>Location: <code>&lt;store-root&gt;/sequences.farg</code></p> <p>Format: Tab-separated values (TSV)</p> <p>Schema: <pre><code>#name    length    alphabet    sha512t24u                md5\nchr1     248956422  dna2bit    AbCdEf123GhIjK...         a1b2c3d4e5f6...\nchr2     242193529  dna2bit    XyZabc456DefGh...         f7e8d9c0b1a2...\nchrM     16569      dna2bit    MnOpQr789StUv...          1a2b3c4d5e6f...\n</code></pre></p> <p>The header line starts with <code>#</code> and defines column names.</p> <p>Data Columns:</p> <ol> <li>name (string): Sequence name (e.g., chromosome name)</li> <li>length (integer): Sequence length in base pairs</li> <li>alphabet (string): Alphabet type</li> <li><code>dna2bit</code>: 2-bit DNA encoding (ACGT only)</li> <li><code>dna3bit</code>: 3-bit DNA encoding (includes N)</li> <li><code>dnaio</code>: Full IUPAC DNA alphabet</li> <li><code>protein</code>: Protein sequences</li> <li><code>ASCII</code>: Generic ASCII sequences</li> <li>sha512t24u (string): GA4GH SHA-512/24u digest (base64url, 32 chars)</li> <li>Content-addressable identifier</li> <li>Used as primary key for sequence lookup</li> <li>md5 (string): MD5 digest (hex, 32 chars)</li> <li>Legacy support for backwards compatibility</li> </ol> <p>Each sequence occupies one line, with lines starting with <code>#</code> serving as comments or headers. Fields are tab-separated and no quoting is required since sequence names cannot contain tabs.</p>"},{"location":"gtars/refget-store-format/#sequence-files-seq","title":"Sequence Files (.seq)","text":"<p>Individual sequence data files, one per sequence.</p> <p>Location: Determined by <code>seqdata_path_template</code> in <code>index.json</code></p> <p>Naming: Based on SHA-512/24u digest</p> <p>Format: Binary</p> <p>Content depends on storage mode: Raw mode stores plain sequence data as bytes (DNA as ASCII characters like A, C, G, T, N; protein as A, R, N, D, C, etc.) that is directly readable as text, while encoded mode uses bit-packed sequence data (DNA 2-bit packs 4 nucleotides per byte for ACGT; DNA 3-bit stores ~2.67 nucleotides per byte including N) that is more space-efficient but requires decoding to read. For example, human chr1 (248 Mbp) takes ~248 MB in raw mode but only ~62 MB in encoded 2-bit mode (4\u00d7 compression).</p>"},{"location":"gtars/refget-store-format/#collection-files-farg","title":"Collection Files (.farg)","text":"<p>Metadata files grouping sequences into collections (e.g., genome assemblies).</p> <p>Location: <code>&lt;store-root&gt;/collections/&lt;collection-digest&gt;.farg</code></p> <p>Format: Tab-separated values (TSV) with header sections</p> <p>Structure: <pre><code>##seqcol_digest=uC_UorBNf3YUu1YIDainBhI94CedlNeH\n##names_digest=zxcvbnmasdfghjkl\n##sequences_digest=qwertyuiopasdfgh\n##lengths_digest=poiuytrewqlkjhgf\n#name   length  alphabet    sha512t24u  md5\nchr1    248956422   dna2bit AbCdEf123GhIjK...   a1b2c3d4e5f6...\nchr2    242193529   dna2bit XyZabc456DefGh...   f7e8d9c0b1a2...\n</code></pre></p> <p>The header section uses <code>##</code> (double hash) for collection-level metadata headers, including the sequence collection digest (<code>##seqcol_digest</code>), and digests for the names, sequences, and lengths arrays. The data section header uses <code>#</code> (single hash) and is tab-separated.</p> <p>Data Section: Same format as <code>sequences.farg</code>, but only sequences in this collection.</p>"},{"location":"gtars/refget-store-format/#storage-modes","title":"Storage Modes","text":"<p>RefgetStore supports two storage modes: Raw mode stores sequences as plain text (for DNA/protein), making them simple to debug and inspect with no decoding overhead, but results in larger file sizes without compression\u2014use this when storage space is not a concern, you need human-readable sequences, or during debugging and development. Encoded mode provides 2-4\u00d7 smaller file sizes through efficient bit-packing and faster I/O, though it requires decoding and is slightly more complex\u2014use this for production deployments and storing large genomes where storage space matters.</p>"},{"location":"gtars/refget-store-format/#path-templates","title":"Path Templates","text":"<p>Templates use placeholders to organize files hierarchically:</p>"},{"location":"gtars/refget-store-format/#sequence-path-templates","title":"Sequence Path Templates","text":"<p>Pattern: <code>sequences/%s2/%s.seq</code></p> <p>Placeholders include <code>%s</code> (full 32-character digest), <code>%s2</code> (first 2 characters), and <code>%s4</code> (first 4 characters).</p> <p>Example: <pre><code>Digest: AbCdEf123GhIjKlMnOpQrStUvWxYzAb\nTemplate: sequences/%s2/%s.seq\nResult: sequences/Ab/AbCdEf123GhIjKlMnOpQrStUvWxYzAb.seq\n</code></pre></p> <p>Using digest prefixes prevents directories with millions of files, provides better filesystem performance, and cleaner organization. Common patterns include <code>sequences/%s2/%s.seq</code> (2-char prefix, 256 subdirectories), <code>sequences/%s4/%s.seq</code> (4-char prefix, 65,536 subdirectories), and <code>sequences/%s.seq</code> (flat structure, not recommended for large stores).</p>"},{"location":"gtars/refget-store-format/#content-addressable-storage","title":"Content-Addressable Storage","text":"<p>RefgetStore uses content-addressable storage: sequences are identified by their digest (hash of content), not by name.</p>"},{"location":"gtars/refget-store-format/#benefits","title":"Benefits","text":"<p>Content-addressable storage enables deduplication by storing identical sequences only once, even when they appear in different assemblies (like chrM shared between GRCh38 and GRCh37). The digest-based approach ensures integrity by verifying that content hasn't been corrupted, providing tamper-evident storage. Finally, it creates universal identifiers where the same sequence has the same digest everywhere, enabling distributed, federated stores that are portable across systems.</p>"},{"location":"gtars/refget-store-format/#example","title":"Example","text":"<pre><code>GRCh38 chr1: sha512t24u = AbCdEf123...\nGRCh37 chr1: sha512t24u = XyZabc456...  (different sequence)\nGRCh38 chrM: sha512t24u = MnOpQr789...\nGRCh37 chrM:   sha512t24u = MnOpQr789...  (same sequence as GRCh38!)\n</code></pre> <p>Only 3 sequence files needed, even though we have 4 sequence references.</p>"},{"location":"gtars/refget-store-format/#ga4gh-compliance","title":"GA4GH Compliance","text":"<p>RefgetStore implements the GA4GH refget specification, using SHA-512/24u digests (truncated SHA-512, base64url encoded) and supporting both Level 1 and Level 2 sequence collection digests.</p>"},{"location":"gtars/refget-store-format/#usage-patterns","title":"Usage Patterns","text":""},{"location":"gtars/refget-store-format/#creating-a-store","title":"Creating a Store","text":"<pre><code>use gtars_refget::store::{GlobalRefgetStore, StorageMode};\n\n// Create new store\nlet mut store = GlobalRefgetStore::new(StorageMode::Encoded);\n\n// Import FASTA file\nstore.add_sequence_collection_from_fasta(\"genome.fa\")?;\n\n// Write to directory\nstore.write_store_to_dir(\n    \"/path/to/store\",\n    \"sequences/%s2/%s.seq\"\n)?;\n</code></pre>"},{"location":"gtars/refget-store-format/#loading-a-store","title":"Loading a Store","text":"<pre><code>// Load from local directory (with lazy loading)\nlet mut store = GlobalRefgetStore::load_local(\"/path/to/cache\")?;\n\n// Load from remote URL with custom cache location\nlet mut store = GlobalRefgetStore::load_remote(\n    \"/path/to/cache\",                    // Local cache directory\n    \"https://example.com/refget-store\"   // Remote URL\n)?;\n</code></pre>"},{"location":"gtars/refget-store-format/#querying-sequences","title":"Querying Sequences","text":"<pre><code>// Get sequence by digest\nlet seq = store.get_sequence_by_id(\"AbCdEf123GhIjK...\")?;\n\n// Get sequence by name in a collection\nlet seq = store.get_sequence_by_collection_and_name(\n    \"uC_UorBNf3YUu1YIDainBhI94CedlNeH\",  // collection digest\n    \"chr1\"\n)?;\n\n// Get substring (0-based, half-open interval)\nlet substring = store.get_substring(\n    \"AbCdEf123GhIjK...\",\n    1000,    // start\n    2000     // end (exclusive)\n)?;\n</code></pre>"},{"location":"gtars/refget-store-format/#extracting-sequences-from-bed-file","title":"Extracting Sequences from BED file","text":"<pre><code>// Extract sequences for regions in BED file\nstore.export_fasta_from_regions(\n    \"collection_digest\",\n    \"regions.bed\",\n    \"output.fa\"\n)?;\n\n// Or get as iterator (more memory efficient)\nlet sequences = store.substrings_from_regions(\n    \"collection_digest\",\n    \"regions.bed\"\n)?;\n\n// Collect into vector if needed\nlet sequences_vec: Vec&lt;_&gt; = sequences.collect();\n</code></pre>"},{"location":"gtars/refget-store-format/#distribution","title":"Distribution","text":""},{"location":"gtars/refget-store-format/#local-distribution","title":"Local Distribution","text":"<p>Package the entire directory and distribute: <pre><code>tar -czf refget-store.tar.gz /path/to/refget-store/\n</code></pre></p>"},{"location":"gtars/refget-store-format/#remote-distribution","title":"Remote Distribution","text":"<p>Host on any static file server or object storage: <pre><code># S3\naws s3 sync /path/to/refget-store/ s3://bucket/refget-store/\n\n# HTTP server\npython -m http.server -d /path/to/refget-store/\n\n# Users access via URL with explicit cache location\nstore = GlobalRefgetStore::load_remote(\n    \"/local/cache/path\",                              // User-specified cache\n    \"https://mybucket.s3.amazonaws.com/refget-store\"  // Remote URL\n)?;\n</code></pre></p> <p>Remote access provides lazy loading (only downloading sequences when requested), user-controlled caching (you specify where cached data is stored), bandwidth efficiency (only transferring needed data), and selective downloads (skipping sequences you don't need).</p>"},{"location":"gtars/refget-store-format/#cache-directory","title":"Cache Directory","text":"<p>When loading remote stores with <code>load_remote()</code>, you explicitly specify the cache location:</p> <pre><code>// Example: Cache in a specific directory\nlet cache_dir = \"/data/genomes/cache/hg38\";\nlet store = GlobalRefgetStore::load_remote(\n    cache_dir,\n    \"https://example.com/hg38-store\"\n)?;\n</code></pre> <p>The cache directory has the same structure as the remote store, with <code>index.json</code> and <code>sequences.farg</code> downloaded on load, while sequence files in <code>sequences/</code> and collection files are downloaded on-demand only when accessed.</p> <p>Important: The cache location is user-controlled, not automatic, giving you control over disk usage location, the ability to share caches between processes, explicit cleanup (just delete the directory), and no hidden ~/.cache directories.</p>"},{"location":"gtars/refget-store-format/#design-rationale","title":"Design Rationale","text":""},{"location":"gtars/refget-store-format/#why-separate-files-per-sequence","title":"Why separate files per sequence?","text":"<p>Separate files per sequence enable selective memory mapping (mmap only the sequences you need, not an entire archive), automatic deduplication (identical sequences naturally share the same digest-named file), and simplified remote access (download only the specific sequence files you need with standard HTTP range requests). The key advantage over indexed single files is granular resource management\u2014you can load, cache, and mmap individual sequences independently, which is particularly beneficial for distributed storage systems, content delivery networks, and partial synchronization where you don't want to handle a monolithic file.</p>"},{"location":"gtars/refget-store-format/#why-use-digest-prefixes-in-paths","title":"Why use digest prefixes in paths?","text":"<p>Digest prefixes avoid filesystem limits (directories with millions of files), improve directory lookup performance, and make it easier to shard across multiple servers or buckets.</p>"},{"location":"gtars/refget-store-format/#why-support-both-raw-and-encoded","title":"Why support both Raw and Encoded?","text":"<p>Supporting both modes provides flexibility to trade space for simplicity, with raw mode easier to debug during development and encoded mode providing efficiency for production.</p>"},{"location":"gtars/refget-store-format/#why-include-md5","title":"Why include MD5?","text":"<p>MD5 support provides compatibility with legacy systems, easier migration from MD5-based systems, and cross-referencing between old and new identifiers.</p>"},{"location":"gtars/refget-store-format/#see-also","title":"See Also","text":"<ul> <li>GA4GH refget specification</li> <li>gtars-refget module documentation</li> <li>Python reference guide with examples</li> <li>Python bindings documentation</li> </ul>"},{"location":"gtars/refget/","title":"gtars-refget","text":"<p>Rust implementation of GA4GH refget sequence collection functions.</p>"},{"location":"gtars/refget/#features","title":"Features","text":"<ul> <li>GA4GH refget protocol support</li> <li>Sequence digest computation (SHA512t24u and others)</li> <li>Sequence collection management</li> <li>FASTA file reading and writing</li> <li>Efficient sequence storage and retrieval</li> </ul>"},{"location":"gtars/refget/#modules","title":"Modules","text":""},{"location":"gtars/refget/#digest","title":"digest","text":"<p>Compute and verify sequence digests: <pre><code>use gtars_refget::digest;\n\n// Compute SHA512t24u digest for a sequence\nlet seq = b\"ACGTACGTACGT\";\nlet digest = digest::sha512t24u(seq);\n</code></pre></p>"},{"location":"gtars/refget/#collection","title":"collection","text":"<p>Manage sequence collections: <pre><code>use gtars_refget::collection::SequenceCollection;\n\n// Create and work with sequence collections\nlet mut collection = SequenceCollection::new();\n// Add sequences, compute digests, etc.\n</code></pre></p>"},{"location":"gtars/refget/#store","title":"store","text":"<p>Efficient sequence storage indexed by digest: <pre><code>use gtars_refget::store::SequenceStore;\n\n// Store and retrieve sequences by their digest\nlet store = SequenceStore::new();\n</code></pre></p>"},{"location":"gtars/refget/#fasta","title":"fasta","text":"<p>Read and write FASTA files: <pre><code>use gtars_refget::fasta;\n\n// Parse FASTA files\n// Write sequences to FASTA format\n</code></pre></p>"},{"location":"gtars/refget/#alphabet","title":"alphabet","text":"<p>Define sequence alphabets (DNA, protein, ASCII): <pre><code>use gtars_refget::alphabet;\n\n// Work with different sequence alphabets\n</code></pre></p>"},{"location":"gtars/refget/#standards-compliance","title":"Standards Compliance","text":"<p>Implements GA4GH refget specification for sequence collections and digests.</p>"},{"location":"gtars/scatrs/","title":"gtars-scatrs","text":"<p>SCATRS (Single-Cell ATAC-seq Region Simulator) - High-performance single-cell ATAC-seq fragment simulation.</p>"},{"location":"gtars/scatrs/#features","title":"Features","text":"<ul> <li>Realistic fragment generation based on empirical distributions</li> <li>Cell type-specific simulation</li> <li>Quality score modeling</li> <li>Batch and barcode generation</li> <li>Parallel processing with rayon</li> </ul>"},{"location":"gtars/scatrs/#usage","title":"Usage","text":""},{"location":"gtars/scatrs/#command-line","title":"Command Line","text":"<pre><code>scatrs simulate \\\n  --peaks peaks.bed \\\n  --cells 10000 \\\n  --output fragments.tsv.gz\n</code></pre>"},{"location":"gtars/scatrs/#from-rust","title":"From Rust","text":"<pre><code>use gtars_scatrs::{Simulator, SimulationParams};\n\nlet params = SimulationParams {\n    num_cells: 10000,\n    mean_fragments_per_cell: 5000,\n    ..Default::default()\n};\n\nlet simulator = Simulator::new(params);\nsimulator.run(\"peaks.bed\", \"output.tsv.gz\")?;\n</code></pre>"},{"location":"gtars/scatrs/#simulation-parameters","title":"Simulation Parameters","text":"<ul> <li>Number of cells</li> <li>Fragment size distribution</li> <li>Coverage depth</li> <li>Cell type proportions</li> <li>Technical noise parameters</li> </ul>"},{"location":"gtars/scatrs/#output-format","title":"Output Format","text":"<p>Standard 10x Genomics fragment file format: <pre><code>chr1    1000    1500    AAACCCAAGAAACACT-1    1\nchr1    2000    2300    AAACCCAAGAAACACT-1    1\n</code></pre></p>"},{"location":"gtars/scoring/","title":"gtars-scoring","text":"<p>Wrapper around gtars-overlaprs for generating X-by-peak count matrices.</p>"},{"location":"gtars/scoring/#purpose","title":"Purpose","text":"<p>gtars-scoring wraps the core overlap infrastructure from gtars-overlaprs to produce count matrices for genomic analysis. This module handles all use cases that need X-by-peak matrices:</p> <ul> <li>Cell-by-peak matrices (single-cell ATAC-seq)</li> <li>Sample-by-peak matrices (bulk ATAC-seq/ChIP-seq)</li> <li>Pseudobulk-by-peak matrices (aggregated single-cell data)</li> </ul>"},{"location":"gtars/scoring/#what-it-does","title":"What it does","text":"<p>Generates count matrices from fragment-peak overlaps where:</p> <ul> <li>Rows = analysis units (cells, samples, or pseudobulk aggregations)</li> <li>Columns = consensus regions (peaks)</li> <li>Values = overlap counts</li> </ul>"},{"location":"gtars/scoring/#cli-usage","title":"CLI Usage","text":"<pre><code>gtars fscoring &lt;fragments&gt; &lt;consensus&gt; /\n  --mode &lt;atac|chip&gt; /\n  --output &lt;output.csv.gz&gt;\n</code></pre> <p>Arguments:</p> <ul> <li><code>fragments</code>: Glob pattern for fragment files (e.g., <code>\"*.bed.gz\"</code>)</li> <li><code>consensus</code>: Path to consensus peak set (BED file)</li> <li><code>--mode</code>: Scoring mode (<code>atac</code> or <code>chip</code>, default: <code>atac</code>)</li> <li><code>atac</code>: Counts 5' and 3' cut sites with shifts (+4/-5 bp)</li> <li><code>chip</code>: Counts full fragment overlaps</li> <li><code>--output</code>: Output file path (default: <code>count_matrix.csv.gz</code>)</li> </ul> <p>Example: <pre><code>gtars fscoring \"fragments/*.bed.gz\" peaks.bed /\n  --mode atac /\n  --output counts.csv.gz\n</code></pre></p>"},{"location":"gtars/scoring/#rust-library-usage","title":"Rust Library Usage","text":"<pre><code>use gtars::scoring::{\n    region_scoring_from_fragments, \n    ConsensusSet, \n    FragmentFileGlob, \n    ScoringMode\n};\n\n// Set up inputs\nlet mut fragments = FragmentFileGlob::new(\"path/to/*.bed.gz\")?;\nlet consensus = ConsensusSet::new(\"peaks.bed\".into())?;\n\n// Generate count matrix\nlet count_matrix = region_scoring_from_fragments(\n    &amp;mut fragments, \n    &amp;consensus, \n    ScoringMode::Atac\n)?;\n\n// Write to file\ncount_matrix.write_to_file(\"output.csv.gz\")?;\n\n// Or access counts directly\nlet count = count_matrix.get(row, col);\n</code></pre>"},{"location":"gtars/scoring/#output-format","title":"Output Format","text":"<p>Gzipped CSV file with:</p> <ul> <li>Each row = one fragment file</li> <li>Each column = one consensus region</li> <li>Values = count of overlapping fragments</li> </ul>"},{"location":"gtars/tokenizers/","title":"gtars tokenizers","text":"<p>Wrapper around gtars-overlaprs for producing tokens for machine learning models.</p>"},{"location":"gtars/tokenizers/#purpose","title":"Purpose","text":"<p>gtars-tokenizers wraps the core overlap infrastructure from gtars-overlaprs to convert genomic regions into vocabulary tokens for machine learning pipelines. This module is specifically designed for ML applications that need to represent genomic intervals as discrete tokens.</p> <p> </p>"},{"location":"gtars/tokenizers/#features","title":"Features","text":"<ul> <li>Multiple tokenization strategies configurable via TOML</li> <li>HuggingFace integration support</li> <li>Configurable tokenization schemes</li> <li>Optimized for transformer models and other ML pipelines</li> </ul>"},{"location":"gtars/tokenizers/#usage","title":"Usage","text":"PythonRust <pre><code>from gtars.tokenizers import Tokenizer\n\n# load tokenizer from a BED file (creates vocabulary from regions)\ntokenizer = Tokenizer.from_bed(\"path/to/bedfile.bed\")\n\n# load from configuration file\ntokenizer = Tokenizer.from_config(\"tokenizer_config.toml\")\n\n# load pretrained tokenizer\ntokenizer = Tokenizer.from_pretrained(\"path/to/model\")\n\n# tokenize regions from a list of region strings\ntokens = tokenizer.tokenize([\"chr1:1000-2000\", \"chr2:3000-4000\"])\n\n# tokenize regions from a RegionSet\nfrom gtars.models import RegionSet\nregions = RegionSet(\"path/to/bedfile.bed\")\ntokens = tokenizer.tokenize(regions)\n\n # or dirdectly from a BED file\ntokens = tokenizer.tokenize_from_bed(\"path/to/bedfile.bed\")\n</code></pre> <pre><code>use gtars_tokenizers::Tokenizer;\n\n// load tokenizer\nlet tokenizer = Tokenizer::from_bed(\"regions.bed\")?;\n\n// tokenize regions\nlet tokens = tokenizer.tokenize(&amp;regions)?;\n</code></pre>"},{"location":"gtars/tokenizers/#integration-with-huggingface-transformers","title":"Integration with HuggingFace Transformers","text":"<p>The tokenizers were designed to be as compatible as possible with HuggingFace Transformers. You can easily integrate them into your ML pipelines.</p> Python <pre><code>from gtars.tokenizers import Tokenizer\n\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-base-hg38\")\n\nvocab_size = tokenizer.vocab_size\nspecial_tokens_map = tokenizer.special_tokens_map\n</code></pre>"},{"location":"gtars/tokenizers/#using-a-tokenizer-from-a-pre-trained-model","title":"Using a tokenizer from a pre-trained model","text":"<p>We can also download the universe (vocabulary) for a pre-trained model from huggingface and use that to instantiate our tokenizer.</p> Python <pre><code>from gtars.tokenizers import Tokenizer\n\n# identical API to huggingface\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-base-hg38\")\n\ntokens = tokenizer.tokenize(\"path/to/intervals.bed\")\nprint(tokens)\n# &gt;&gt;&gt; [\"chr1:100-200\", \"chr1:200-300\", ...]\n</code></pre>"},{"location":"gtars/tokenizers/#working-with-the-tokenizer-api","title":"Working with the tokenizer API","text":"<p>We designed the tokenizer API to be congruent with the Hugging Face Tokenizers library, making it easy to integrate with modern machine learning workflows tailored to genomic data.</p>"},{"location":"gtars/tokenizers/#getting-input-ids","title":"Getting input ids","text":"<p>It is common to represent genomic intervals as input ids for machine learning models, particularly for transformer-based architectures. These input ids are typically derived from the tokenized representation of the genomic intervals. You can obtain the input ids from the tokenizer as follows:</p> Python <pre><code>from gtars.tokenizers import Tokenizer\nfrom gtars.models import RegionSet\n\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-base-hg38\")\nrs = RegionSet(\"path/to/intervals.bed\")\n\ntokens = tokenizer(rs)\nprint(tokens[\"input_ids\"])\n# &gt;&gt;&gt; [101, 202, 111]\n</code></pre>"},{"location":"gtars/tokenizers/#getting-special-tokens","title":"Getting special tokens","text":"<p>Special tokens are integral to the tokenizer's functionality, providing markers for padding, masking, and classification tasks. You can access the special tokens map from the tokenizer as follows:</p> Python <pre><code>from gtars.tokenizers import Tokenizer\n\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-base-hg38\")\nprint(tokenizer.special_tokens_map)\n# &gt;&gt;&gt; {'pad_token': '&lt;pad&gt;', 'mask_token': '&lt;mask&gt;', 'cls_token': '&lt;cls&gt;', ...}\n</code></pre>"},{"location":"gtars/tokenizers/#decoding-input-ids","title":"Decoding input id's","text":"<p>For generative tasks, or when you need to convert input ids back to their original genomic interval representation, you can use the tokenizer's decode method:</p> Python <pre><code>from gtars.tokenizers import Tokenizer\n\ntokenizer = Tokenizer.from_pretrained(\"databio/atacformer-base-hg38\")\nspecial_tokens_mask = tokenizer.decode([101, 202, 111])\n# &gt;&gt;&gt; chr1:100-200, chr1:200-300, ...\n</code></pre>"},{"location":"gtars/uniwig/","title":"Current Steps to Run Uniwig","text":""},{"location":"gtars/uniwig/#these-docs-are-under-construction-please-see-below-for-some-basic-examples","title":"These docs are under construction; please see below for some basic examples.","text":""},{"location":"gtars/uniwig/#input-bed-file","title":"Input Bed File","text":"<p>Currently, Uniwig accepts a single <code>.bed</code> file, <code>.narrowPeak</code> file, <code>.bam</code> file. It should be sorted by chromosome. This single file will be used to create 3 output files: <code>_start</code> -&gt; accumulations of start coordinates <code>_end</code> -&gt; accumulations of end coordinates <code>_core</code> -&gt; accumulations of peaks (between starts and ends)</p> <p>The below script can be used to create a sorted bed file from a directory of bed files:</p> <pre><code>#!/bin/sh\n# directory for the raw data (bed files)\nRAWDATA_DIR=\"./data/raw/\"\n# directory for combined data\nCOMBDATA_DIR=\"./data/combined/\"\n# raw data filename\nraw=\"*.bed\"\n# unsorted combined data filename\nunsorted=\"combined_unsort.bed\"\n# chrsorted combined data filename\nchrsorted=\"combined_chrsort.bed\"\nawk 'NF {print} END {print \"\"}' $RAWDATA_DIR$raw &gt; $COMBDATA_DIR$unsorted\nsort -k1,1V -k2,2n $COMBDATA_DIR$unsorted | grep '.' &gt; $COMBDATA_DIR$chrsorted\n</code></pre>"},{"location":"gtars/uniwig/#running-uniwig","title":"Running uniwig","text":"<p>Once you have your single, sorted bedfile, you can run uniwig with the following command:</p> <pre><code>cargo run uniwig -f test_30_lines_sorted.bed -c hg38.chrom.sizes -m 5 -s 1 -l /wiggles_created_with_rust/final_wiggles/ -y wig\n</code></pre> <p>Note that we provide a chrom.sizes reference file (hg38) in the testing folder -&gt; <code>/tests/hg38.chrom.sizes</code></p>"},{"location":"gtars/uniwig/#usage","title":"Usage","text":"<pre><code>Create accumulation files from a BED or BAM file\n\nUsage: gtars uniwig [OPTIONS] --file &lt;file&gt; --smoothsize &lt;smoothsize&gt; --stepsize &lt;stepsize&gt; --fileheader &lt;fileheader&gt; --outputtype &lt;outputtype&gt;\n\nOptions:\n  -f, --file &lt;file&gt;              Path to the combined bed file we want to transform or a sorted bam file\n  -t, --filetype &lt;filetype&gt;      Input file type, 'bed' 'bam' or 'narrowpeak' [default: bed]\n  -c, --chromref &lt;chromref&gt;      Path to chromreference\n  -m, --smoothsize &lt;smoothsize&gt;  Integer value for smoothing\n  -s, --stepsize &lt;stepsize&gt;      Integer value for stepsize\n  -l, --fileheader &lt;fileheader&gt;  Name of the file\n  -y, --outputtype &lt;outputtype&gt;  Output as wiggle or npy\n  -u, --counttype &lt;counttype&gt;    Select to only output start, end, or core. Select `shift` for bam workflows. Defaults to all. [default: all]\n  -p, --threads &lt;threads&gt;        Number of rayon threads to use for parallel processing [default: 6]\n  -o, --score                    Count via score (narrowPeak only!)\n  -a, --no-bamshift              Set bam shift to False, i.e. uniwig will count raw reads without considering read direction.\n  -z, --zoom &lt;zoom&gt;              Number of zoom levels (for bw file output only [default: 1]\n  -d, --debug                    Print more verbose debug messages?\n  -h, --help                     Print help\n</code></pre>"},{"location":"gtars/uniwig/#processing-bam-files-to-bw","title":"Processing bam files to bw","text":"<p>Example command <pre><code>gtars uniwig -f \"test1_chr1_chr2.bam\" -m 5 -s 1 -l /myoutput/directory/test_file_name -y bw -t bam -p 6 -c /genome/alias/hg38/fasta/default/hg38.chrom.sizes -u all\n</code></pre></p>"},{"location":"gtars/uniwig/#export-types","title":"Export types","text":"<p>For Input types: <code>.bed</code> and <code>.narrowPeak</code> Output types include <code>.wig</code>, <code>.npy</code>, <code>.bedGraph</code>, and <code>.bw</code></p> <p>For Input Types: <code>.bam</code> Output types include <code>.bw</code> and <code>.bed</code></p>"},{"location":"gtars/versioning/","title":"Versioning policy for gtars and its crates","text":"<p>Because <code>gtars</code> is organized as a workspace with multiple crates, we need to ensure that we have a clear versioning policy for the different crates, bindings, and command line tools.</p> <p>The versioning and tagging scheme was influenced by several other Rust projects we admire/use:</p> <ul> <li><code>polars</code>: https://github.com/pola-rs/polars/tags</li> <li><code>bigtools</code>: https://github.com/jackh726/bigtools/releases</li> <li><code>noodles</code>: https://github.com/zaeleus/noodles/tags?after=noodles-fastq-0.19.0</li> </ul>"},{"location":"gtars/versioning/#versioning-scheme","title":"Versioning scheme","text":"<p>In general, we will follow Semantic Versioning for all crates, bindings, and command line tools. We allow independent versioning for each crate, and we will bump the version(s) of any wrapper/binding/CLI crate that depends on a crate that has been updated.</p> <p>Finally, we also have a specific tagging scheme for releases on GitHub.</p>"},{"location":"gtars/versioning/#tagging-scheme","title":"Tagging scheme","text":"<ul> <li>Core Rust Crates: <code>vX.Y.Z</code> (e.g., <code>v0.5.2</code>). This tag signifies a release of the core library crates to crates.io. It should trigger the cargo publish workflow.</li> <li>Python Bindings: <code>py-X.Y.Z</code> (e.g., <code>py-0.3.1</code>). This tag signifies a release of the Python package to PyPI.</li> <li>CLI: <code>cli-X.Y.Z</code> (e.g., <code>cli-1.1.0</code>). This tag signifies a release of the CLI binaries, which will be attached to a GitHub Release.</li> <li>WASM Bindings: <code>wasm-X.Y.Z</code> (e.g., <code>wasm-0.1.5</code>). This tag signifies a release of the WASM package to npm.</li> </ul>"},{"location":"gtars/versioning/#an-example-scenario","title":"An example scenario:","text":"<p>Say we fix a bug in <code>uniwig</code>, we will bump its version by a single patch <code>x.x.1</code> inside its <code>Cargo.toml</code>: <pre><code>// gtars-uniwig/Cargo.toml\n- version=\"0.5.0\"\n+ version=\"0.5.1\"\n</code></pre> We publish this to <code>crates.io</code> using <code>cargo publish</code>.</p> <p>Then we will bump this version accordingly in the <code>gtars</code> wrapper crate and bump the crates version (since it got a new uniwig) <pre><code>// gtars/Cargo.toml\n- gtars-uniwig = { version=\"0.5.0\" }\n+ gtars-uniwig = { version=\"0.5.1\" }\n</code></pre> <pre><code>// gtars/Cargo.toml\n- version=\"0.5.11\"\n+ version=\"0.5.12\"\n</code></pre> We will publish this to <code>crates.io</code> using <code>cargo publish</code></p> <p>Finally, because <code>uniwig</code> is a tool used in the command line interface, we will bump the version of <code>gtars-uniwig</code> in <code>gtars-cli</code> to the most recent version with the bug fix similarly to before. Then we will bump the version of <code>gtars-cli</code> as a whole (a single patch since its a simple bug fix in <code>gtars-uniwig</code>). <pre><code>- gtars-uniwig = { version=\"0.5.0\" }\n+ gtars-uniwig = { version=\"0.5.1\" }\n</code></pre> <pre><code>- version=\"0.4.1\"\n+ version=\"0.4.2\"\n</code></pre> We will publish this to <code>crates.io</code> using <code>cargo publish</code></p>"},{"location":"gtars/wasm/","title":"gtars Wasm/JS","text":"<p>We provide WebAssembly (Wasm) bindings for gtars functionality, enabling genomic interval analysis in JavaScript/TypeScript environments. This allows for client-side processing of genomic data without the need for server-side computation.</p>"},{"location":"gtars/wasm/#features","title":"Features","text":"<ul> <li>Zero-installation genomic tools</li> <li>Client-side processing (no server required)</li> <li>JavaScript/TypeScript interoperability</li> </ul>"},{"location":"gtars/wasm/#installation","title":"Installation","text":"<p>You can install the gtars Wasm package via npm:</p> <pre><code>npm install @databio/gtars-js\n</code></pre>"},{"location":"gtars/wasm/#usage","title":"Usage","text":"<p>We don't currently provide full feature parity bindings, but some functionality is available.</p> <pre><code>import init, { Overlapper } from 'gtars-js';\n\nawait init();\n\nimport { Overlapper } from '@databio/gtars-js';\n\nconst universe = [\n    ['chr1', 100, 200],\n    ['chr1', 150, 250],\n    ['chr2', 300, 400],\n]\n\nconst overlapper = new Overlapper(universe, 'ailist');\nconsole.log(`Using backend: ${overlapper.get_backend()}`);\n\nconst query = ['chr1', 180, 220];\nconst overlaps = overlapper.find(query);\n\nconsole.log(`Overlaps for ${query}:`, overlaps);\n</code></pre>"},{"location":"gtars/wasm/#integration","title":"Integration","text":""},{"location":"gtars/wasm/#an-example-react-component","title":"An example React component","text":"<pre><code>import { useEffect, useState } from 'react';\nimport init, { TreeTokenizer } from 'gtars-js';\n\nfunction TokenizerComponent() {\n  const [tokenizer, setTokenizer] = useState(null);\n\n  useEffect(() =&gt; {\n    init().then(() =&gt; {\n      setTokenizer(new TreeTokenizer());\n    });\n  }, []);\n\n  // Use tokenizer...\n}\n</code></pre>"},{"location":"gtars/python/digests/","title":"Gtars digests using python bindings","text":""},{"location":"gtars/python/digests/#introduction","title":"Introduction","text":"<p>You can use this to compute md5 or GA4GH sha512t24u digests of strings, or FASTA files.</p>"},{"location":"gtars/python/digests/#tutorial","title":"Tutorial","text":"<p>Computing digests for all sequences in a FASTA file:</p> <pre><code>import os\nimport tempfile\nfrom gtars.refget import digest_fasta\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    fasta_content = (\n        \"&gt;chr1 description\\n\"\n        \"ATGCATGCATGC\\n\"\n        \"&gt;chr2\\n\"\n        \"GGGGAAAA\\n\"\n    )\n    fasta_path = os.path.join(temp_dir, \"example.fa\")\n    with open(fasta_path, \"w\") as f:\n        f.write(fasta_content)\n\n    collection = digest_fasta(fasta_path)\n    print(f\"Collection-level digest: {collection.digest}\")\n    print(f\"Number of sequences in collection: {len(collection)}\")\n    print(f\"Metadata for first sequence: {collection[0].metadata.name}, Length: {collection[0].metadata.length}\")\n</code></pre> <p>Compute a digest for a given sequence:</p> <pre><code>from gtars.refget import sha512t24u_digest\n\nsequence_data = \"AGCT\"\ndigest = sha512t24u_digest(sequence_data)\nprint(f\"SHA512t24u digest for '{sequence_data}': {digest}\")\n</code></pre>"},{"location":"gtars/python/refget-api/","title":"Refget Module API Reference","text":""},{"location":"gtars/python/refget-api/#gtars.refget","title":"refget","text":"<p>Type stubs and documentation for the gtars.refget module.</p> <p>This file serves two purposes:</p> <ol> <li> <p>Type Hints: Provides type annotations for IDE autocomplete and static    type checking tools like mypy.</p> </li> <li> <p>Documentation: Contains Google-style docstrings that mkdocstrings uses    to generate the API reference documentation website.</p> </li> </ol> <p>Note: The actual implementation is in Rust (gtars-python/src/refget/mod.rs) and compiled via PyO3. This stub file provides the Python interface definition and structured documentation that tools can parse properly.</p>"},{"location":"gtars/python/refget-api/#gtars.refget-classes","title":"Classes","text":""},{"location":"gtars/python/refget-api/#gtars.refget.AlphabetType","title":"AlphabetType","text":"<p>               Bases: <code>Enum</code></p> <p>Represents the type of alphabet for a sequence.</p>"},{"location":"gtars/python/refget-api/#gtars.refget.AlphabetType-attributes","title":"Attributes","text":""},{"location":"gtars/python/refget-api/#gtars.refget.AlphabetType.Dna2bit","title":"Dna2bit  <code>instance-attribute</code>","text":"<pre><code>Dna2bit: int\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.AlphabetType.Dna3bit","title":"Dna3bit  <code>instance-attribute</code>","text":"<pre><code>Dna3bit: int\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.AlphabetType.DnaIupac","title":"DnaIupac  <code>instance-attribute</code>","text":"<pre><code>DnaIupac: int\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.AlphabetType.Protein","title":"Protein  <code>instance-attribute</code>","text":"<pre><code>Protein: int\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.AlphabetType.Ascii","title":"Ascii  <code>instance-attribute</code>","text":"<pre><code>Ascii: int\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.AlphabetType.Unknown","title":"Unknown  <code>instance-attribute</code>","text":"<pre><code>Unknown: int\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.AlphabetType-functions","title":"Functions","text":""},{"location":"gtars/python/refget-api/#gtars.refget.AlphabetType.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceMetadata","title":"SequenceMetadata","text":"<p>Metadata for a biological sequence.</p>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceMetadata-attributes","title":"Attributes","text":""},{"location":"gtars/python/refget-api/#gtars.refget.SequenceMetadata.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceMetadata.length","title":"length  <code>instance-attribute</code>","text":"<pre><code>length: int\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceMetadata.sha512t24u","title":"sha512t24u  <code>instance-attribute</code>","text":"<pre><code>sha512t24u: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceMetadata.md5","title":"md5  <code>instance-attribute</code>","text":"<pre><code>md5: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceMetadata.alphabet","title":"alphabet  <code>instance-attribute</code>","text":"<pre><code>alphabet: AlphabetType\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceMetadata-functions","title":"Functions","text":""},{"location":"gtars/python/refget-api/#gtars.refget.SequenceMetadata.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceMetadata.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceRecord","title":"SequenceRecord","text":"<p>A record representing a biological sequence, including its metadata and optional data.</p>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceRecord-attributes","title":"Attributes","text":""},{"location":"gtars/python/refget-api/#gtars.refget.SequenceRecord.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: SequenceMetadata\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceRecord.sequence","title":"sequence  <code>instance-attribute</code>","text":"<pre><code>sequence: Optional[bytes]\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceRecord-functions","title":"Functions","text":""},{"location":"gtars/python/refget-api/#gtars.refget.SequenceRecord.decode","title":"decode","text":"<pre><code>decode() -&gt; Optional[str]\n</code></pre> <p>Decode and return the sequence data as a string.</p> <p>For Full records with sequence data, returns the decoded sequence. For Stub records without sequence data, returns None.</p> <p>Returns:</p> <ul> <li> <code>Optional[str]</code>           \u2013            <p>Decoded sequence string if data is available, None otherwise.</p> </li> </ul>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceRecord.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceRecord.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SeqColDigestLvl1","title":"SeqColDigestLvl1","text":"<p>Level 1 digests for a sequence collection.</p>"},{"location":"gtars/python/refget-api/#gtars.refget.SeqColDigestLvl1-attributes","title":"Attributes","text":""},{"location":"gtars/python/refget-api/#gtars.refget.SeqColDigestLvl1.sequences_digest","title":"sequences_digest  <code>instance-attribute</code>","text":"<pre><code>sequences_digest: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SeqColDigestLvl1.names_digest","title":"names_digest  <code>instance-attribute</code>","text":"<pre><code>names_digest: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SeqColDigestLvl1.lengths_digest","title":"lengths_digest  <code>instance-attribute</code>","text":"<pre><code>lengths_digest: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SeqColDigestLvl1-functions","title":"Functions","text":""},{"location":"gtars/python/refget-api/#gtars.refget.SeqColDigestLvl1.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SeqColDigestLvl1.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionMetadata","title":"SequenceCollectionMetadata","text":"<p>Metadata for a sequence collection.</p> <p>Contains the collection digest and level 1 digests for names, sequences, and lengths. This is a lightweight representation of a collection without the actual sequence list.</p> <p>Attributes:</p> <ul> <li> <code>digest</code>               (<code>str</code>)           \u2013            <p>The collection's SHA-512/24u digest.</p> </li> <li> <code>n_sequences</code>               (<code>int</code>)           \u2013            <p>Number of sequences in the collection.</p> </li> <li> <code>names_digest</code>               (<code>str</code>)           \u2013            <p>Level 1 digest of the names array.</p> </li> <li> <code>sequences_digest</code>               (<code>str</code>)           \u2013            <p>Level 1 digest of the sequences array.</p> </li> <li> <code>lengths_digest</code>               (<code>str</code>)           \u2013            <p>Level 1 digest of the lengths array.</p> </li> </ul>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionMetadata-attributes","title":"Attributes","text":""},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionMetadata.digest","title":"digest  <code>instance-attribute</code>","text":"<pre><code>digest: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionMetadata.n_sequences","title":"n_sequences  <code>instance-attribute</code>","text":"<pre><code>n_sequences: int\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionMetadata.names_digest","title":"names_digest  <code>instance-attribute</code>","text":"<pre><code>names_digest: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionMetadata.sequences_digest","title":"sequences_digest  <code>instance-attribute</code>","text":"<pre><code>sequences_digest: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionMetadata.lengths_digest","title":"lengths_digest  <code>instance-attribute</code>","text":"<pre><code>lengths_digest: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionMetadata-functions","title":"Functions","text":""},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionMetadata.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionMetadata.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollection","title":"SequenceCollection","text":"<p>A collection of biological sequences.</p>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollection-attributes","title":"Attributes","text":""},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollection.sequences","title":"sequences  <code>instance-attribute</code>","text":"<pre><code>sequences: List[SequenceRecord]\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollection.digest","title":"digest  <code>instance-attribute</code>","text":"<pre><code>digest: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollection.lvl1","title":"lvl1  <code>instance-attribute</code>","text":"<pre><code>lvl1: SeqColDigestLvl1\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollection.file_path","title":"file_path  <code>instance-attribute</code>","text":"<pre><code>file_path: Optional[str]\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollection.has_data","title":"has_data  <code>instance-attribute</code>","text":"<pre><code>has_data: bool\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollection-functions","title":"Functions","text":""},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollection.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollection.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionRecord","title":"SequenceCollectionRecord","text":"<p>A record representing a sequence collection, which may be a Stub or Full.</p> <p>Stub records contain only metadata (digest, n_sequences, level 1 digests). Full records contain metadata plus the list of SequenceRecord objects.</p>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionRecord-attributes","title":"Attributes","text":""},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionRecord.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: SequenceCollectionMetadata\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionRecord.sequences","title":"sequences  <code>property</code>","text":"<pre><code>sequences: Optional[List[SequenceRecord]]\n</code></pre> <p>Get the sequences if loaded (Full), None if stub-only.</p>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionRecord-functions","title":"Functions","text":""},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionRecord.has_sequences","title":"has_sequences","text":"<pre><code>has_sequences() -&gt; bool\n</code></pre> <p>Check if this record has sequences loaded (is Full, not Stub).</p>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionRecord.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.SequenceCollectionRecord.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RetrievedSequence","title":"RetrievedSequence","text":"<p>Represents a retrieved sequence segment with its metadata. Exposed from the Rust <code>PyRetrievedSequence</code> struct.</p>"},{"location":"gtars/python/refget-api/#gtars.refget.RetrievedSequence-attributes","title":"Attributes","text":""},{"location":"gtars/python/refget-api/#gtars.refget.RetrievedSequence.sequence","title":"sequence  <code>instance-attribute</code>","text":"<pre><code>sequence: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RetrievedSequence.chrom_name","title":"chrom_name  <code>instance-attribute</code>","text":"<pre><code>chrom_name: str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RetrievedSequence.start","title":"start  <code>instance-attribute</code>","text":"<pre><code>start: int\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RetrievedSequence.end","title":"end  <code>instance-attribute</code>","text":"<pre><code>end: int\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RetrievedSequence-functions","title":"Functions","text":""},{"location":"gtars/python/refget-api/#gtars.refget.RetrievedSequence.__init__","title":"__init__","text":"<pre><code>__init__(sequence: str, chrom_name: str, start: int, end: int) -&gt; None\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RetrievedSequence.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RetrievedSequence.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.StorageMode","title":"StorageMode","text":"<p>               Bases: <code>Enum</code></p> <p>Defines how sequence data is stored in the Refget store.</p>"},{"location":"gtars/python/refget-api/#gtars.refget.StorageMode-attributes","title":"Attributes","text":""},{"location":"gtars/python/refget-api/#gtars.refget.StorageMode.Raw","title":"Raw  <code>instance-attribute</code>","text":"<pre><code>Raw: int\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.StorageMode.Encoded","title":"Encoded  <code>instance-attribute</code>","text":"<pre><code>Encoded: int\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore","title":"RefgetStore","text":"<p>A global store for GA4GH refget sequences with lazy-loading support.</p> <p>RefgetStore provides content-addressable storage for reference genome sequences following the GA4GH refget specification. Supports both local and remote stores with on-demand sequence loading.</p> <p>Attributes:</p> <ul> <li> <code>cache_path</code>               (<code>Optional[str]</code>)           \u2013            <p>Local directory path where the store is located or cached. None for in-memory stores.</p> </li> <li> <code>remote_url</code>               (<code>Optional[str]</code>)           \u2013            <p>Remote URL of the store if loaded remotely, None otherwise.</p> </li> </ul> Note <p>Boolean evaluation: RefgetStore follows Python container semantics, meaning <code>bool(store)</code> is <code>False</code> for empty stores (like <code>list</code>, <code>dict</code>, etc.). To check if a store variable is initialized (not None), use <code>if store is not None:</code> rather than <code>if store:</code>.</p> <p>Example::</p> <pre><code>store = RefgetStore.in_memory()  # Empty store\nbool(store)  # False (empty container)\nlen(store)   # 0\n\n# Wrong: checks emptiness, not initialization\nif store:\n    process(store)\n\n# Right: checks if variable is set\nif store is not None:\n    process(store)\n</code></pre> <p>Examples:</p> <p>Create a new store and import sequences::</p> <pre><code>from gtars.refget import RefgetStore, StorageMode\nstore = RefgetStore(StorageMode.Encoded)\nstore.import_fasta(\"genome.fa\")\n</code></pre> <p>Open an existing local store::</p> <pre><code>store = RefgetStore.open_local(\"/data/hg38\")\nseq = store.get_substring(\"chr1_digest\", 0, 1000)\n</code></pre> <p>Open a remote store with caching::</p> <pre><code>store = RefgetStore.open_remote(\n    \"/local/cache\",\n    \"https://example.com/hg38\"\n)\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore-attributes","title":"Attributes","text":""},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.cache_path","title":"cache_path  <code>instance-attribute</code>","text":"<pre><code>cache_path: Optional[str]\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.remote_url","title":"remote_url  <code>instance-attribute</code>","text":"<pre><code>remote_url: Optional[str]\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore-functions","title":"Functions","text":""},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.__init__","title":"__init__","text":"<pre><code>__init__(mode: StorageMode) -&gt; None\n</code></pre> <p>Create a new empty RefgetStore.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>StorageMode</code>)           \u2013            <p>Storage mode - StorageMode.Raw (uncompressed) or StorageMode.Encoded (bit-packed, space-efficient).</p> </li> </ul> <p>Example::</p> <pre><code>store = RefgetStore(StorageMode.Encoded)\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.in_memory","title":"in_memory  <code>classmethod</code>","text":"<pre><code>in_memory() -&gt; RefgetStore\n</code></pre> <p>Create a new in-memory RefgetStore.</p> <p>Creates a store that keeps all sequences in memory. Use this for temporary processing or when you don't need disk persistence.</p> <p>Returns:</p> <ul> <li> <code>RefgetStore</code>           \u2013            <p>New empty RefgetStore with Encoded storage mode.</p> </li> </ul> <p>Example::</p> <pre><code>store = RefgetStore.in_memory()\nstore.import_fasta(\"genome.fa\")\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.on_disk","title":"on_disk  <code>classmethod</code>","text":"<pre><code>on_disk(cache_path: Union[str, PathLike]) -&gt; RefgetStore\n</code></pre> <p>Create or load a disk-backed RefgetStore.</p> <p>If the directory contains an existing store (rgstore.json), loads it. Otherwise creates a new store with Encoded mode.</p> <p>Parameters:</p> <ul> <li> <code>cache_path</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Directory path for the store. Created if it doesn't exist.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RefgetStore</code>           \u2013            <p>RefgetStore (new or loaded from disk).</p> </li> </ul> <p>Example::</p> <pre><code>store = RefgetStore.on_disk(\"/data/my_store\")\nstore.import_fasta(\"genome.fa\")\n# Store is automatically persisted to disk\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.open_local","title":"open_local  <code>classmethod</code>","text":"<pre><code>open_local(path: Union[str, PathLike]) -&gt; RefgetStore\n</code></pre> <p>Open a local RefgetStore from a directory.</p> <p>Loads only lightweight metadata and stubs. Collections and sequences remain as stubs until explicitly accessed with get_collection()/get_sequence().</p> <p>Expects: rgstore.json, sequences.rgsi, collections.rgci, collections/*.rgsi</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Local directory containing the refget store.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RefgetStore</code>           \u2013            <p>RefgetStore with metadata loaded, sequences lazy-loaded.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>IOError</code>             \u2013            <p>If the store directory or index files cannot be read.</p> </li> </ul> <p>Example::</p> <pre><code>store = RefgetStore.open_local(\"/data/hg38_store\")\nseq = store.get_substring(\"chr1_digest\", 0, 1000)\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.open_remote","title":"open_remote  <code>classmethod</code>","text":"<pre><code>open_remote(cache_path: Union[str, PathLike], remote_url: str) -&gt; RefgetStore\n</code></pre> <p>Open a remote RefgetStore with local caching.</p> <p>Loads only lightweight metadata and stubs from the remote URL. Data is fetched on-demand when get_collection()/get_sequence() is called.</p> <p>By default, persistence is enabled (sequences are cached to disk). Call <code>disable_persistence()</code> after loading to keep only in memory.</p> <p>Parameters:</p> <ul> <li> <code>cache_path</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Local directory to cache downloaded metadata and sequences. Created if it doesn't exist.</p> </li> <li> <code>remote_url</code>               (<code>str</code>)           \u2013            <p>Base URL of the remote refget store (e.g., \"https://example.com/hg38\" or \"s3://bucket/hg38\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RefgetStore</code>           \u2013            <p>RefgetStore with metadata loaded, sequences fetched on-demand.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>IOError</code>             \u2013            <p>If remote metadata cannot be fetched or cache cannot be written.</p> </li> </ul> <p>Example::</p> <pre><code>store = RefgetStore.open_remote(\n    \"/data/cache/hg38\",\n    \"https://refget-server.com/hg38\"\n)\n# First access fetches from remote and caches\nseq = store.get_substring(\"chr1_digest\", 0, 1000)\n# Second access uses cache\nseq2 = store.get_substring(\"chr1_digest\", 1000, 2000)\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.set_encoding_mode","title":"set_encoding_mode","text":"<pre><code>set_encoding_mode(mode: StorageMode) -&gt; None\n</code></pre> <p>Change the storage mode, re-encoding/decoding existing sequences as needed.</p> <p>When switching from Raw to Encoded, all Full sequences in memory are encoded (2-bit packed). When switching from Encoded to Raw, all Full sequences in memory are decoded back to raw bytes.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>StorageMode</code>)           \u2013            <p>The storage mode to switch to (StorageMode.Raw or StorageMode.Encoded).</p> </li> </ul> <p>Example::</p> <pre><code>store = RefgetStore.in_memory()\nstore.set_encoding_mode(StorageMode.Raw)\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.enable_persistence","title":"enable_persistence","text":"<pre><code>enable_persistence(path: Union[str, PathLike]) -&gt; None\n</code></pre> <p>Enable disk persistence for this store.</p> <p>Sets up the store to write sequences to disk. Any in-memory Full sequences are flushed to disk and converted to Stubs.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Directory for storing sequences and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>IOError</code>             \u2013            <p>If the directory cannot be created or written to.</p> </li> </ul> <p>Example::</p> <pre><code>store = RefgetStore.in_memory()\nstore.add_sequence_collection_from_fasta(\"genome.fa\")\nstore.enable_persistence(\"/data/store\")  # Flush to disk\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.disable_persistence","title":"disable_persistence","text":"<pre><code>disable_persistence() -&gt; None\n</code></pre> <p>Disable disk persistence for this store.</p> <p>New sequences will be kept in memory only. Existing Stub sequences can still be loaded from disk if local_path is set.</p> <p>Example::</p> <pre><code>store = RefgetStore.open_remote(\"/cache\", \"https://example.com\")\nstore.disable_persistence()  # Stop caching new sequences\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.import_fasta","title":"import_fasta","text":"<pre><code>import_fasta(file_path: Union[str, PathLike]) -&gt; None\n</code></pre> <p>Import sequences from a FASTA file into the store.</p> <p>Reads all sequences from a FASTA file and adds them to the store. Computes GA4GH digests and creates a sequence collection.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Path to the FASTA file.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>IOError</code>             \u2013            <p>If the file cannot be read or parsed.</p> </li> </ul> <p>Example::</p> <pre><code>store = RefgetStore(StorageMode.Encoded)\nstore.import_fasta(\"genome.fa\")\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.list_collections","title":"list_collections","text":"<pre><code>list_collections() -&gt; List[SequenceCollectionMetadata]\n</code></pre> <p>List all collection metadata in the store.</p> <p>Returns metadata for all collections without loading full collection data. Use this for browsing/inventory operations.</p> <p>Returns:</p> <ul> <li> <code>List[SequenceCollectionMetadata]</code>           \u2013            <p>List of metadata for all collections.</p> </li> </ul> <p>Example::</p> <pre><code>for meta in store.list_collections():\n    print(f\"Collection {meta.digest}: {meta.n_sequences} sequences\")\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.get_collection_metadata","title":"get_collection_metadata","text":"<pre><code>get_collection_metadata(collection_digest: str) -&gt; Optional[SequenceCollectionMetadata]\n</code></pre> <p>Get metadata for a collection by digest.</p> <p>Returns lightweight metadata without loading the full collection. Use this for quick lookups of collection information.</p> <p>Parameters:</p> <ul> <li> <code>collection_digest</code>               (<code>str</code>)           \u2013            <p>The collection's SHA-512/24u digest.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[SequenceCollectionMetadata]</code>           \u2013            <p>Collection metadata if found, None otherwise.</p> </li> </ul> <p>Example::</p> <pre><code>meta = store.get_collection_metadata(\"uC_UorBNf3YUu1YIDainBhI94CedlNeH\")\nif meta:\n    print(f\"Collection has {meta.n_sequences} sequences\")\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.get_collection","title":"get_collection","text":"<pre><code>get_collection(collection_digest: str) -&gt; SequenceCollection\n</code></pre> <p>Get a collection by digest with all sequences loaded.</p> <p>Loads the collection and all its sequence data into memory. Use this when you need full access to sequence content.</p> <p>Parameters:</p> <ul> <li> <code>collection_digest</code>               (<code>str</code>)           \u2013            <p>The collection's SHA-512/24u digest.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SequenceCollection</code>           \u2013            <p>The collection with all sequence data loaded.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>IOError</code>             \u2013            <p>If the collection cannot be loaded.</p> </li> </ul> <p>Example::</p> <pre><code>collection = store.get_collection(\"uC_UorBNf3YUu1YIDainBhI94CedlNeH\")\nfor seq in collection.sequences:\n    print(f\"{seq.metadata.name}: {seq.decode()[:20]}...\")\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.iter_collections","title":"iter_collections","text":"<pre><code>iter_collections() -&gt; List[SequenceCollection]\n</code></pre> <p>Iterate over all collections with their sequences loaded.</p> <p>This loads all collection data upfront and returns a list of SequenceCollection objects with full sequence data.</p> <p>For browsing without loading data, use list_collections() instead.</p> <p>Returns:</p> <ul> <li> <code>List[SequenceCollection]</code>           \u2013            <p>List of all collections with loaded sequences.</p> </li> </ul> <p>Example::</p> <pre><code>for coll in store.iter_collections():\n    print(f\"{coll.digest}: {len(coll.sequences)} sequences\")\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.is_collection_loaded","title":"is_collection_loaded","text":"<pre><code>is_collection_loaded(collection_digest: str) -&gt; bool\n</code></pre> <p>Check if a collection is fully loaded.</p> <p>Returns True if the collection's sequence list is loaded in memory, False if it's only metadata (stub).</p> <p>Parameters:</p> <ul> <li> <code>collection_digest</code>               (<code>str</code>)           \u2013            <p>The collection's SHA-512/24u digest.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if loaded, False otherwise.</p> </li> </ul>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.list_sequences","title":"list_sequences","text":"<pre><code>list_sequences() -&gt; List[SequenceMetadata]\n</code></pre> <p>List all sequence metadata in the store.</p> <p>Returns metadata for all sequences without loading sequence data. Use this for browsing/inventory operations.</p> <p>Returns:</p> <ul> <li> <code>List[SequenceMetadata]</code>           \u2013            <p>List of metadata for all sequences in the store.</p> </li> </ul> <p>Example::</p> <pre><code>for meta in store.list_sequences():\n    print(f\"{meta.name}: {meta.length} bp\")\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.get_sequence_metadata","title":"get_sequence_metadata","text":"<pre><code>get_sequence_metadata(seq_digest: str) -&gt; Optional[SequenceMetadata]\n</code></pre> <p>Get metadata for a sequence by digest (no data loaded).</p> <p>Use this for lightweight lookups when you don't need the actual sequence.</p> <p>Parameters:</p> <ul> <li> <code>seq_digest</code>               (<code>str</code>)           \u2013            <p>The sequence's SHA-512/24u digest.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[SequenceMetadata]</code>           \u2013            <p>Sequence metadata if found, None otherwise.</p> </li> </ul>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.get_sequence","title":"get_sequence","text":"<pre><code>get_sequence(digest: str) -&gt; Optional[SequenceRecord]\n</code></pre> <p>Retrieve a sequence record by its digest (SHA-512/24u or MD5).</p> <p>Loads the sequence data if not already in memory. Supports lookup by either SHA-512/24u (preferred) or MD5 digest.</p> <p>Parameters:</p> <ul> <li> <code>digest</code>               (<code>str</code>)           \u2013            <p>Sequence digest (SHA-512/24u base64url or MD5 hex string).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[SequenceRecord]</code>           \u2013            <p>The sequence record with data if found, None otherwise.</p> </li> </ul> <p>Example::</p> <pre><code>record = store.get_sequence(\"aKF498dAxcJAqme6QYQ7EZ07-fiw8Kw2\")\nif record:\n    print(f\"Found: {record.metadata.name}\")\n    print(f\"Sequence: {record.decode()[:50]}...\")\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.get_sequence_by_name","title":"get_sequence_by_name","text":"<pre><code>get_sequence_by_name(collection_digest: str, sequence_name: str) -&gt; Optional[SequenceRecord]\n</code></pre> <p>Retrieve a sequence by collection digest and sequence name.</p> <p>Looks up a sequence within a specific collection using its name (e.g., \"chr1\", \"chrM\"). Loads the sequence data if needed.</p> <p>Parameters:</p> <ul> <li> <code>collection_digest</code>               (<code>str</code>)           \u2013            <p>The collection's SHA-512/24u digest.</p> </li> <li> <code>sequence_name</code>               (<code>str</code>)           \u2013            <p>Name of the sequence within that collection.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[SequenceRecord]</code>           \u2013            <p>The sequence record with data if found, None otherwise.</p> </li> </ul> <p>Example::</p> <pre><code>record = store.get_sequence_by_name(\n    \"uC_UorBNf3YUu1YIDainBhI94CedlNeH\",\n    \"chr1\"\n)\nif record:\n    print(f\"Sequence: {record.decode()[:50]}...\")\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.iter_sequences","title":"iter_sequences","text":"<pre><code>iter_sequences() -&gt; List[SequenceRecord]\n</code></pre> <p>Iterate over all sequences with their data loaded.</p> <p>This ensures all sequence data is loaded and returns a list of SequenceRecord objects with full sequence data.</p> <p>For browsing without loading data, use list_sequences() instead.</p> <p>Returns:</p> <ul> <li> <code>List[SequenceRecord]</code>           \u2013            <p>List of all sequences with loaded data.</p> </li> </ul> <p>Example::</p> <pre><code>for seq in store.iter_sequences():\n    print(f\"{seq.metadata.name}: {seq.decode()[:20]}...\")\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.get_substring","title":"get_substring","text":"<pre><code>get_substring(seq_digest: str, start: int, end: int) -&gt; Optional[str]\n</code></pre> <p>Extract a substring from a sequence.</p> <p>Retrieves a specific region from a sequence using 0-based, half-open coordinates [start, end). Automatically loads sequence data if not already cached (for lazy-loaded stores).</p> <p>Parameters:</p> <ul> <li> <code>seq_digest</code>               (<code>str</code>)           \u2013            <p>Sequence digest (SHA-512/24u).</p> </li> <li> <code>start</code>               (<code>int</code>)           \u2013            <p>Start position (0-based, inclusive).</p> </li> <li> <code>end</code>               (<code>int</code>)           \u2013            <p>End position (0-based, exclusive).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[str]</code>           \u2013            <p>The substring sequence if found, None otherwise.</p> </li> </ul> <p>Example::</p> <pre><code># Get first 1000 bases of chr1\nseq = store.get_substring(\"chr1_digest\", 0, 1000)\nprint(f\"First 50bp: {seq[:50]}\")\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.stats","title":"stats","text":"<pre><code>stats() -&gt; dict\n</code></pre> <p>Returns statistics about the store.</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>dict with keys: - 'n_sequences': Total number of sequences (Stub + Full) - 'n_sequences_loaded': Number of sequences with data loaded (Full) - 'n_collections': Total number of collections (Stub + Full) - 'n_collections_loaded': Number of collections with sequences loaded (Full) - 'storage_mode': Storage mode ('Raw' or 'Encoded') - 'total_disk_size': Total size of all files on disk in bytes</p> </li> </ul> Note <p>n_collections_loaded only reflects collections fully loaded in memory. For remote stores, collections are loaded on-demand when accessed.</p> <p>Example::</p> <pre><code>stats = store.stats()\nprint(f\"Store has {stats['n_sequences']} sequences\")\nprint(f\"Collections: {stats['n_collections']} total, {stats['n_collections_loaded']} loaded\")\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.write_store_to_directory","title":"write_store_to_directory","text":"<pre><code>write_store_to_directory(root_path: Union[str, PathLike], seqdata_path_template: str) -&gt; None\n</code></pre> <p>Write the store to a directory on disk.</p> <p>Persists the store with all sequences and metadata to disk using the RefgetStore directory format.</p> <p>Parameters:</p> <ul> <li> <code>root_path</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Directory path to write the store to.</p> </li> <li> <code>seqdata_path_template</code>               (<code>str</code>)           \u2013            <p>Path template for sequence files (e.g., \"sequences/%s2/%s.seq\" where %s2 = first 2 chars of digest, %s = full digest).</p> </li> </ul> <p>Example::</p> <pre><code>store.write_store_to_directory(\n    \"/data/my_store\",\n    \"sequences/%s2/%s.seq\"\n)\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.get_seqs_bed_file","title":"get_seqs_bed_file","text":"<pre><code>get_seqs_bed_file(collection_digest: str, bed_file_path: Union[str, PathLike], output_fasta_path: Union[str, PathLike]) -&gt; None\n</code></pre> <p>Extract sequences for BED regions and write to FASTA.</p> <p>Parameters:</p> <ul> <li> <code>collection_digest</code>               (<code>str</code>)           \u2013            <p>Collection digest to look up sequence names.</p> </li> <li> <code>bed_file_path</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Path to BED file with regions.</p> </li> <li> <code>output_fasta_path</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Path to write output FASTA file.</p> </li> </ul>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.get_seqs_bed_file_to_vec","title":"get_seqs_bed_file_to_vec","text":"<pre><code>get_seqs_bed_file_to_vec(collection_digest: str, bed_file_path: Union[str, PathLike]) -&gt; List[RetrievedSequence]\n</code></pre> <p>Extract sequences for BED regions and return as list.</p> <p>Parameters:</p> <ul> <li> <code>collection_digest</code>               (<code>str</code>)           \u2013            <p>Collection digest to look up sequence names.</p> </li> <li> <code>bed_file_path</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Path to BED file with regions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[RetrievedSequence]</code>           \u2013            <p>List of retrieved sequence segments.</p> </li> </ul>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.export_fasta","title":"export_fasta","text":"<pre><code>export_fasta(collection_digest: str, output_path: Union[str, PathLike], sequence_names: Optional[List[str]] = None, line_width: Optional[int] = None) -&gt; None\n</code></pre> <p>Export sequences from a collection to a FASTA file.</p> <p>Parameters:</p> <ul> <li> <code>collection_digest</code>               (<code>str</code>)           \u2013            <p>Collection to export from.</p> </li> <li> <code>output_path</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Path to write FASTA file.</p> </li> <li> <code>sequence_names</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of sequence names to export. If None, exports all sequences in the collection.</p> </li> <li> <code>line_width</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional line width for wrapping sequences. If None, uses default of 80.</p> </li> </ul>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.export_fasta_by_digests","title":"export_fasta_by_digests","text":"<pre><code>export_fasta_by_digests(digests: List[str], output_path: Union[str, PathLike], line_width: Optional[int] = None) -&gt; None\n</code></pre> <p>Export sequences by their digests to a FASTA file.</p> <p>Parameters:</p> <ul> <li> <code>digests</code>               (<code>List[str]</code>)           \u2013            <p>List of sequence digests to export.</p> </li> <li> <code>output_path</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Path to write FASTA file.</p> </li> <li> <code>line_width</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional line width for wrapping sequences. If None, uses default of 80.</p> </li> </ul>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget.RefgetStore.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre>"},{"location":"gtars/python/refget-api/#gtars.refget-functions","title":"Functions","text":""},{"location":"gtars/python/refget-api/#gtars.refget.sha512t24u_digest","title":"sha512t24u_digest","text":"<pre><code>sha512t24u_digest(readable: Union[str, bytes]) -&gt; str\n</code></pre> <p>Compute the GA4GH SHA-512/24u digest for a sequence.</p> <p>This function computes the GA4GH refget standard digest (truncated SHA-512, base64url encoded) for a given sequence string or bytes.</p> <p>Parameters:</p> <ul> <li> <code>readable</code>               (<code>Union[str, bytes]</code>)           \u2013            <p>Input sequence as str or bytes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The SHA-512/24u digest (32 character base64url string).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If input is not str or bytes.</p> </li> </ul> <p>Example::     from gtars.refget import sha512t24u_digest     digest = sha512t24u_digest(\"ACGT\")     print(digest)     # Output: 'aKF498dAxcJAqme6QYQ7EZ07-fiw8Kw2'</p>"},{"location":"gtars/python/refget-api/#gtars.refget.md5_digest","title":"md5_digest","text":"<pre><code>md5_digest(readable: Union[str, bytes]) -&gt; str\n</code></pre> <p>Compute the MD5 digest for a sequence.</p> <p>This function computes the MD5 hash for a given sequence string or bytes. MD5 is supported for backward compatibility with legacy systems.</p> <p>Parameters:</p> <ul> <li> <code>readable</code>               (<code>Union[str, bytes]</code>)           \u2013            <p>Input sequence as str or bytes.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The MD5 digest (32 character hexadecimal string).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If input is not str or bytes.</p> </li> </ul> <p>Example::     from gtars.refget import md5_digest     digest = md5_digest(\"ACGT\")     print(digest)     # Output: 'f1f8f4bf413b16ad135722aa4591043e'</p>"},{"location":"gtars/python/refget-api/#gtars.refget.digest_fasta","title":"digest_fasta","text":"<pre><code>digest_fasta(fasta: Union[str, PathLike]) -&gt; SequenceCollection\n</code></pre> <p>Digest all sequences in a FASTA file and compute collection-level digests.</p> <p>This function reads a FASTA file and computes GA4GH-compliant digests for each sequence, as well as collection-level digests (Level 1 and Level 2) following the GA4GH refget specification.</p> <p>Parameters:</p> <ul> <li> <code>fasta</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Path to FASTA file (str or PathLike).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SequenceCollection</code>           \u2013            <p>Collection containing all sequences with their metadata and computed digests.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>IOError</code>             \u2013            <p>If the FASTA file cannot be read or parsed.</p> </li> </ul> <p>Example::     from gtars.refget import digest_fasta     collection = digest_fasta(\"genome.fa\")     print(f\"Collection digest: {collection.digest}\")     print(f\"Number of sequences: {len(collection)}\")</p>"},{"location":"gtars/python/refget-api/#gtars.refget.compute_fai","title":"compute_fai","text":"<pre><code>compute_fai(fasta: Union[str, PathLike]) -&gt; List[FaiRecord]\n</code></pre> <p>Compute FASTA index (FAI) metadata for all sequences in a FASTA file.</p> <p>This function computes the FAI index metadata (offset, line_bases, line_bytes) for each sequence in a FASTA file, compatible with samtools faidx format. Only works with uncompressed FASTA files.</p> <p>Parameters:</p> <ul> <li> <code>fasta</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Path to FASTA file (str or PathLike). Must be uncompressed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[FaiRecord]</code>           \u2013            <p>List of FAI records, one per sequence, containing name, length,</p> </li> <li> <code>List[FaiRecord]</code>           \u2013            <p>and FAI metadata (offset, line_bases, line_bytes).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>IOError</code>             \u2013            <p>If the FASTA file cannot be read or is compressed.</p> </li> </ul> <p>Example::     from gtars.refget import compute_fai     fai_records = compute_fai(\"genome.fa\")     for record in fai_records:     ...     print(f\"{record.name}: {record.length} bp\")</p>"},{"location":"gtars/python/refget-api/#gtars.refget.load_fasta","title":"load_fasta","text":"<pre><code>load_fasta(fasta: Union[str, PathLike]) -&gt; SequenceCollection\n</code></pre> <p>Load a FASTA file with sequence data into a SequenceCollection.</p> <p>This function reads a FASTA file and loads all sequences with their data into memory. Unlike digest_fasta(), this includes the actual sequence data, not just metadata.</p> <p>Parameters:</p> <ul> <li> <code>fasta</code>               (<code>Union[str, PathLike]</code>)           \u2013            <p>Path to FASTA file (str or PathLike).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SequenceCollection</code>           \u2013            <p>Collection containing all sequences with their metadata and sequence data loaded.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>IOError</code>             \u2013            <p>If the FASTA file cannot be read or parsed.</p> </li> </ul> <p>Example::     from gtars.refget import load_fasta     collection = load_fasta(\"genome.fa\")     first_seq = collection[0]     print(f\"Sequence: {first_seq.data[:50]}...\")</p>"},{"location":"gtars/python/refget-api/#gtars.refget.digest_sequence","title":"digest_sequence","text":"<pre><code>digest_sequence(name: str, data: bytes, description: Optional[str] = None) -&gt; SequenceRecord\n</code></pre> <p>Create a SequenceRecord from raw data, computing all metadata.</p> <p>This is the sequence-level parallel to digest_fasta() for collections. It computes the GA4GH sha512t24u digest, MD5 digest, detects the alphabet, and returns a SequenceRecord with computed metadata and the original data.</p> <p>The input data is automatically uppercased to ensure consistent digest computation (matching FASTA processing behavior).</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The sequence name (e.g., \"chr1\").</p> </li> <li> <code>data</code>               (<code>bytes</code>)           \u2013            <p>The raw sequence bytes (e.g., b\"ACGTACGT\").</p> </li> <li> <code>description</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional description text for the sequence.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SequenceRecord</code>           \u2013            <p>A SequenceRecord with computed metadata and the original data (uppercased).</p> </li> </ul> <p>Example::     from gtars.refget import digest_sequence     seq = digest_sequence(\"chr1\", b\"ACGTACGT\")     print(seq.metadata.name, seq.metadata.length)     # Output: chr1 8</p> <pre><code># With description\nseq2 = digest_sequence(\"chr1\", b\"ACGT\", description=\"Chromosome 1\")\nprint(seq2.metadata.description)\n# Output: Chromosome 1\n</code></pre>"},{"location":"gtars/python/refget-store/","title":"GlobalRefgetStore Python Reference","text":"<p>This is a Python-specific reference guide that provides quick examples for using the <code>GlobalRefgetStore</code> class from the <code>gtars.refget</code> module. For detailed information about the underlying RefgetStore file format specification, see refget-store-format.md.</p>"},{"location":"gtars/python/refget-store/#creating-and-populating-a-store","title":"Creating and Populating a Store","text":"<pre><code>from gtars.refget import GlobalRefgetStore, StorageMode, digest_fasta\n\n# Create a new store in Encoded mode (space-efficient)\nstore = GlobalRefgetStore(StorageMode.Encoded)\nprint(f\"Initialized store: {store}\")\n\n# Add sequences from a FASTA file\nstore.add_sequence_collection_from_fasta(\"genome.fa\")\n\n# Inspect what's in the store\nsequence_records = store.sequence_records()\nsequence_metadata = store.sequence_metadata()\ncollections = store.collections()\n\n# Access individual sequences\nfirst_seq = sequence_records[0]\nprint(f\"First sequence: {first_seq.metadata.name}\")\n\n# Decode sequence data to string\nif first_seq.sequence:\n    decoded = first_seq.decode()\n    print(f\"Sequence: {decoded}\")\n</code></pre>"},{"location":"gtars/python/refget-store/#saving-and-loading-local-stores","title":"Saving and Loading Local Stores","text":"<pre><code>import os\n\n# Save the store to disk\nstore_path = \"my_refget_store\"\nstore.write_store_to_dir(store_path, \"sequences/%s2/%s.seq\")\n\n# Load a local store\nloaded_store = GlobalRefgetStore.load_local(store_path)\n</code></pre>"},{"location":"gtars/python/refget-store/#loading-remote-stores-with-caching","title":"Loading Remote Stores with Caching","text":"<p>You can load stores from remote URLs (HTTP/HTTPS) with local caching:</p> <pre><code># Load from a remote server with local caching\ncache_dir = \"local_cache\"\nremote_url = \"https://refget-server.example.com/hg38\"\nremote_store = GlobalRefgetStore.load_remote(cache_dir, remote_url)\n\n# Get sequence metadata\nseq_metadata = list(remote_store.sequence_metadata())\nfirst_seq = seq_metadata[0]\n\n# Get a substring (automatically fetches and caches data)\nsubstring = remote_store.get_substring(first_seq.sha512t24u, 0, 1000)\nprint(f\"First 1000 bases: {substring[:50]}...\")\n\n# Iterate over all sequences in the store\nfor seq_meta in remote_store:\n    print(f\"{seq_meta.name}: {seq_meta.length} bp\")\n</code></pre>"},{"location":"gtars/python/refget-store/#working-with-collections","title":"Working with Collections","text":"<pre><code># Get collections in the store\ncollections = store.collections()\ncollection = collections[0]\n\n# Get a sequence by collection and name\nrecord = store.get_sequence_by_collection_and_name(\n    collection.digest,\n    \"chr1\"\n)\n\n# Export entire collection to FASTA\nstore.export_fasta(\n    collection.digest,\n    \"output.fa\",\n    sequence_names=None,  # None = all sequences\n    line_width=80\n)\n\n# Export specific sequences from a collection\nstore.export_fasta(\n    collection.digest,\n    \"chr1_and_chr2.fa\",\n    sequence_names=[\"chr1\", \"chr2\"],\n    line_width=80\n)\n</code></pre>"},{"location":"gtars/python/refget-store/#extracting-regions-from-bed-files","title":"Extracting Regions from BED Files","text":"<pre><code># Get sequences for regions defined in a BED file\nretrieved_seqs = store.substrings_from_regions(\n    collection.digest,\n    \"regions.bed\"\n)\n\nfor seq in retrieved_seqs:\n    print(f\"{seq.chrom_name}:{seq.start}-{seq.end} = {seq.sequence}\")\n\n# Export BED regions to a FASTA file\nstore.export_fasta_from_regions(\n    collection.digest,\n    \"regions.bed\",\n    \"output_regions.fa\"\n)\n</code></pre>"},{"location":"gtars/python/refget-store/#local-http-server-example","title":"Local HTTP Server Example","text":"<p>For testing remote loading locally, you can serve a store directory:</p> <pre><code># In the directory containing your refget store\npython -m http.server 8200\n</code></pre> <p>Then connect to it:</p> <pre><code>remote_store = GlobalRefgetStore.load_remote(\n    \"local_cache\",\n    \"http://localhost:8200/my_refget_store/\"\n)\n\n# Use it like any other store\nsubstring = remote_store.get_substring(seq_digest, 0, 100)\n</code></pre>"},{"location":"gtars/python/refget-store/#more-information","title":"More Information","text":"<p>For a comprehensive tutorial with detailed examples, see refgetstore.ipynb.</p> <p>For the full API documentation, visit the gtars repository.</p>"},{"location":"gtars/python/refgetstore/","title":"RefgetStore Tutorial","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport tempfile\nfrom gtars.refget import GlobalRefgetStore, StorageMode, digest_fasta\n</pre> import os import tempfile from gtars.refget import GlobalRefgetStore, StorageMode, digest_fasta In\u00a0[\u00a0]: Copied! <pre># Create a temporary directory for our example\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Working in temporary directory: {temp_dir}\")\n\n# Create a sample FASTA file\nfasta_content = (\n    \"&gt;chr1\\n\"\n    \"ATGCATGCATGCAGTCGTAGC\\n\"\n    \"&gt;chr2\\n\"\n    \"GGGGAAAA\\n\"\n)\nsource_fasta_path = os.path.join(temp_dir, \"source.fa\")\nwith open(source_fasta_path, \"w\") as f:\n    f.write(fasta_content)\n\nprint(f\"Created FASTA file: {source_fasta_path}\")\n</pre> # Create a temporary directory for our example temp_dir = tempfile.mkdtemp() print(f\"Working in temporary directory: {temp_dir}\")  # Create a sample FASTA file fasta_content = (     \"&gt;chr1\\n\"     \"ATGCATGCATGCAGTCGTAGC\\n\"     \"&gt;chr2\\n\"     \"GGGGAAAA\\n\" ) source_fasta_path = os.path.join(temp_dir, \"source.fa\") with open(source_fasta_path, \"w\") as f:     f.write(fasta_content)  print(f\"Created FASTA file: {source_fasta_path}\") In\u00a0[\u00a0]: Copied! <pre># Digest the FASTA to get collection info\ncollection = digest_fasta(source_fasta_path)\ncollection_digest = collection.digest\n\nprint(f\"Collection digest: {collection_digest}\")\nprint(f\"Number of sequences: {len(collection)}\")\nprint(f\"\\nSequences in collection:\")\nfor seq in collection:\n    print(f\"  - {seq.metadata.name}: {seq.metadata.sha512t24u}\")\n</pre> # Digest the FASTA to get collection info collection = digest_fasta(source_fasta_path) collection_digest = collection.digest  print(f\"Collection digest: {collection_digest}\") print(f\"Number of sequences: {len(collection)}\") print(f\"\\nSequences in collection:\") for seq in collection:     print(f\"  - {seq.metadata.name}: {seq.metadata.sha512t24u}\") In\u00a0[\u00a0]: Copied! <pre># Initialize store in Encoded mode\nstore = GlobalRefgetStore(StorageMode.Encoded)\nprint(f\"Initialized store: {store}\")\n\n# Import the FASTA file into the store\nstore.add_sequence_collection_from_fasta(source_fasta_path)\nprint(\"\\nFASTA imported into the store.\")\nprint(f\"Store now contains: {store}\")\n</pre> # Initialize store in Encoded mode store = GlobalRefgetStore(StorageMode.Encoded) print(f\"Initialized store: {store}\")  # Import the FASTA file into the store store.add_sequence_collection_from_fasta(source_fasta_path) print(\"\\nFASTA imported into the store.\") print(f\"Store now contains: {store}\") In\u00a0[\u00a0]: Copied! <pre># Get the digest for chr1\nseq_digest_chr1 = collection[0].metadata.sha512t24u\n\n# Retrieve the sequence record\nrecord_chr1 = store.get_sequence_by_id(seq_digest_chr1)\n\nif record_chr1:\n    print(f\"Retrieved sequence: {record_chr1.metadata.name}\")\n    print(f\"  Length: {record_chr1.metadata.length}\")\n    print(f\"  Alphabet: {record_chr1.metadata.alphabet}\")\n    \n    # Get the full sequence\n    full_seq = store.get_substring(seq_digest_chr1, 0, record_chr1.metadata.length)\n    print(f\"  Sequence: {full_seq}\")\n</pre> # Get the digest for chr1 seq_digest_chr1 = collection[0].metadata.sha512t24u  # Retrieve the sequence record record_chr1 = store.get_sequence_by_id(seq_digest_chr1)  if record_chr1:     print(f\"Retrieved sequence: {record_chr1.metadata.name}\")     print(f\"  Length: {record_chr1.metadata.length}\")     print(f\"  Alphabet: {record_chr1.metadata.alphabet}\")          # Get the full sequence     full_seq = store.get_substring(seq_digest_chr1, 0, record_chr1.metadata.length)     print(f\"  Sequence: {full_seq}\") In\u00a0[\u00a0]: Copied! <pre># Get a substring from chr1 (positions 5-15)\nsub_seq = store.get_substring(seq_digest_chr1, 5, 15)\nprint(f\"Substring chr1[5:15]: {sub_seq}\")\n\n# Get a substring from chr2\nseq_digest_chr2 = collection[1].metadata.sha512t24u\nsub_seq_chr2 = store.get_substring(seq_digest_chr2, 0, 4)\nprint(f\"Substring chr2[0:4]: {sub_seq_chr2}\")\n</pre> # Get a substring from chr1 (positions 5-15) sub_seq = store.get_substring(seq_digest_chr1, 5, 15) print(f\"Substring chr1[5:15]: {sub_seq}\")  # Get a substring from chr2 seq_digest_chr2 = collection[1].metadata.sha512t24u sub_seq_chr2 = store.get_substring(seq_digest_chr2, 0, 4) print(f\"Substring chr2[0:4]: {sub_seq_chr2}\") In\u00a0[\u00a0]: Copied! <pre># Create a BED file with regions of interest\nbed_content = (\n    \"chr1\\t0\\t10\\n\"\n    \"chr2\\t2\\t6\\n\"\n    \"chr_nonexistent\\t0\\t5\\n\"  # This will be skipped\n)\nbed_path = os.path.join(temp_dir, \"regions.bed\")\nwith open(bed_path, \"w\") as f:\n    f.write(bed_content)\n\n# Retrieve sequences as a list\nretrieved_list = store.substrings_from_regions(collection_digest, bed_path)\n\nprint(\"Retrieved sequences from BED file:\")\nfor rs in retrieved_list:\n    print(f\"  {rs.chrom_name}[{rs.start}-{rs.end}]: {rs.sequence}\")\n</pre> # Create a BED file with regions of interest bed_content = (     \"chr1\\t0\\t10\\n\"     \"chr2\\t2\\t6\\n\"     \"chr_nonexistent\\t0\\t5\\n\"  # This will be skipped ) bed_path = os.path.join(temp_dir, \"regions.bed\") with open(bed_path, \"w\") as f:     f.write(bed_content)  # Retrieve sequences as a list retrieved_list = store.substrings_from_regions(collection_digest, bed_path)  print(\"Retrieved sequences from BED file:\") for rs in retrieved_list:     print(f\"  {rs.chrom_name}[{rs.start}-{rs.end}]: {rs.sequence}\") In\u00a0[\u00a0]: Copied! <pre># Write retrieved sequences to a FASTA file\noutput_fasta_path = os.path.join(temp_dir, \"output_regions.fa\")\nstore.export_fasta_from_regions(collection_digest, bed_path, output_fasta_path)\n\nprint(f\"Sequences written to: {output_fasta_path}\\n\")\nwith open(output_fasta_path, \"r\") as f:\n    print(\"Content:\")\n    print(f.read())\n</pre> # Write retrieved sequences to a FASTA file output_fasta_path = os.path.join(temp_dir, \"output_regions.fa\") store.export_fasta_from_regions(collection_digest, bed_path, output_fasta_path)  print(f\"Sequences written to: {output_fasta_path}\\n\") with open(output_fasta_path, \"r\") as f:     print(\"Content:\")     print(f.read()) In\u00a0[\u00a0]: Copied! <pre># Create a BED file covering all sequences in the collection\n# Each line is: chrom_name 0 length (whole chromosome)\nall_seqs_bed = os.path.join(temp_dir, \"all_sequences.bed\")\nwith open(all_seqs_bed, \"w\") as f:\n    for seq in collection:\n        f.write(f\"{seq.metadata.name}\\t0\\t{seq.metadata.length}\\n\")\n\nprint(\"BED file covering all sequences:\")\nwith open(all_seqs_bed, \"r\") as f:\n    print(f.read())\n\n# Extract complete FASTA from the collection\nextracted_fasta_path = os.path.join(temp_dir, \"extracted_complete.fa\")\nstore.export_fasta_from_regions(collection_digest, all_seqs_bed, extracted_fasta_path)\n\nprint(f\"\\nExtracted complete FASTA to: {extracted_fasta_path}\")\nwith open(extracted_fasta_path, \"r\") as f:\n    print(\"\\nContent:\")\n    print(f.read())\n</pre> # Create a BED file covering all sequences in the collection # Each line is: chrom_name 0 length (whole chromosome) all_seqs_bed = os.path.join(temp_dir, \"all_sequences.bed\") with open(all_seqs_bed, \"w\") as f:     for seq in collection:         f.write(f\"{seq.metadata.name}\\t0\\t{seq.metadata.length}\\n\")  print(\"BED file covering all sequences:\") with open(all_seqs_bed, \"r\") as f:     print(f.read())  # Extract complete FASTA from the collection extracted_fasta_path = os.path.join(temp_dir, \"extracted_complete.fa\") store.export_fasta_from_regions(collection_digest, all_seqs_bed, extracted_fasta_path)  print(f\"\\nExtracted complete FASTA to: {extracted_fasta_path}\") with open(extracted_fasta_path, \"r\") as f:     print(\"\\nContent:\")     print(f.read()) <p>Alternative: You can also extract sequences programmatically without a BED file:</p> In\u00a0[\u00a0]: Copied! <pre># Extract all sequences from a collection programmatically\nmanual_fasta_path = os.path.join(temp_dir, \"manual_extraction.fa\")\n\nwith open(manual_fasta_path, \"w\") as f:\n    for seq in collection:\n        # Get the sequence data\n        seq_data = store.get_substring(\n            seq.metadata.sha512t24u,\n            0,\n            seq.metadata.length\n        )\n        \n        # Write FASTA header and sequence\n        f.write(f\"&gt;{seq.metadata.name}\\n\")\n        f.write(f\"{seq_data}\\n\")\n\nprint(f\"Manually extracted FASTA to: {manual_fasta_path}\")\nwith open(manual_fasta_path, \"r\") as f:\n    print(\"\\nContent:\")\n    print(f.read())\n</pre> # Extract all sequences from a collection programmatically manual_fasta_path = os.path.join(temp_dir, \"manual_extraction.fa\")  with open(manual_fasta_path, \"w\") as f:     for seq in collection:         # Get the sequence data         seq_data = store.get_substring(             seq.metadata.sha512t24u,             0,             seq.metadata.length         )                  # Write FASTA header and sequence         f.write(f\"&gt;{seq.metadata.name}\\n\")         f.write(f\"{seq_data}\\n\")  print(f\"Manually extracted FASTA to: {manual_fasta_path}\") with open(manual_fasta_path, \"r\") as f:     print(\"\\nContent:\")     print(f.read()) In\u00a0[\u00a0]: Copied! <pre># Save the store to a directory\nsaved_store_path = os.path.join(temp_dir, \"my_refget_store\")\nstore.write_store_to_dir(saved_store_path, \"sequences/%s2/%s.seq\")\n\nprint(f\"Store saved to: {saved_store_path}\")\nprint(f\"\\nStore structure:\")\nfor root, dirs, files in os.walk(saved_store_path):\n    level = root.replace(saved_store_path, '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f\"{indent}{os.path.basename(root)}/\")\n    subindent = ' ' * 2 * (level + 1)\n    for file in files:\n        print(f\"{subindent}{file}\")\n</pre> # Save the store to a directory saved_store_path = os.path.join(temp_dir, \"my_refget_store\") store.write_store_to_dir(saved_store_path, \"sequences/%s2/%s.seq\")  print(f\"Store saved to: {saved_store_path}\") print(f\"\\nStore structure:\") for root, dirs, files in os.walk(saved_store_path):     level = root.replace(saved_store_path, '').count(os.sep)     indent = ' ' * 2 * level     print(f\"{indent}{os.path.basename(root)}/\")     subindent = ' ' * 2 * (level + 1)     for file in files:         print(f\"{subindent}{file}\") In\u00a0[\u00a0]: Copied! <pre># Load the store using load_local method (with lazy loading)\nloaded_store = GlobalRefgetStore.load_local(saved_store_path)\n\nprint(f\"Store loaded from: {saved_store_path}\")\nprint(f\"Loaded store: {loaded_store}\")\n\n# Verify we can retrieve sequences (data loaded on-demand)\nseq = loaded_store.get_substring(seq_digest_chr1, 0, 10)\nprint(f\"\\nRetrieved sequence from loaded store: {seq}\")\n</pre> # Load the store using load_local method (with lazy loading) loaded_store = GlobalRefgetStore.load_local(saved_store_path)  print(f\"Store loaded from: {saved_store_path}\") print(f\"Loaded store: {loaded_store}\")  # Verify we can retrieve sequences (data loaded on-demand) seq = loaded_store.get_substring(seq_digest_chr1, 0, 10) print(f\"\\nRetrieved sequence from loaded store: {seq}\") In\u00a0[\u00a0]: Copied! <pre># Create a cache directory for remote data\ncache_dir = os.path.join(temp_dir, \"refget_cache\")\nos.makedirs(cache_dir, exist_ok=True)\n\nprint(f\"Cache directory: {cache_dir}\")\n\n# Simulate a remote URL (in practice, this would be https://...)\nremote_url = f\"file://{saved_store_path}\"\nprint(f\"Remote URL: {remote_url}\")\n</pre> # Create a cache directory for remote data cache_dir = os.path.join(temp_dir, \"refget_cache\") os.makedirs(cache_dir, exist_ok=True)  print(f\"Cache directory: {cache_dir}\")  # Simulate a remote URL (in practice, this would be https://...) remote_url = f\"file://{saved_store_path}\" print(f\"Remote URL: {remote_url}\") In\u00a0[\u00a0]: Copied! <pre># Load remote store\n# In a real scenario, remote_url would be something like:\n# \"https://s3.amazonaws.com/genomes/hg38\" or \"https://refget.example.com/store\"\nremote_store = GlobalRefgetStore.load_remote(cache_dir, remote_url)\n\nprint(f\"Remote store loaded: {remote_store}\")\nprint(f\"\\nCache directory contents (metadata only at this point):\")\nfor root, dirs, files in os.walk(cache_dir):\n    for file in files:\n        rel_path = os.path.relpath(os.path.join(root, file), cache_dir)\n        print(f\"  - {rel_path}\")\n</pre> # Load remote store # In a real scenario, remote_url would be something like: # \"https://s3.amazonaws.com/genomes/hg38\" or \"https://refget.example.com/store\" remote_store = GlobalRefgetStore.load_remote(cache_dir, remote_url)  print(f\"Remote store loaded: {remote_store}\") print(f\"\\nCache directory contents (metadata only at this point):\") for root, dirs, files in os.walk(cache_dir):     for file in files:         rel_path = os.path.relpath(os.path.join(root, file), cache_dir)         print(f\"  - {rel_path}\") In\u00a0[\u00a0]: Copied! <pre># First access - triggers download and caching\nprint(\"First access (will fetch from remote and cache):\")\nseq1 = remote_store.get_substring(seq_digest_chr1, 0, 10)\nprint(f\"  Sequence: {seq1}\")\n\nprint(f\"\\nCache directory contents (after first access):\")\nfor root, dirs, files in os.walk(cache_dir):\n    for file in files:\n        rel_path = os.path.relpath(os.path.join(root, file), cache_dir)\n        file_size = os.path.getsize(os.path.join(root, file))\n        print(f\"  - {rel_path} ({file_size} bytes)\")\n</pre> # First access - triggers download and caching print(\"First access (will fetch from remote and cache):\") seq1 = remote_store.get_substring(seq_digest_chr1, 0, 10) print(f\"  Sequence: {seq1}\")  print(f\"\\nCache directory contents (after first access):\") for root, dirs, files in os.walk(cache_dir):     for file in files:         rel_path = os.path.relpath(os.path.join(root, file), cache_dir)         file_size = os.path.getsize(os.path.join(root, file))         print(f\"  - {rel_path} ({file_size} bytes)\") In\u00a0[\u00a0]: Copied! <pre># Second access - uses cached data (no network request)\nprint(\"\\nSecond access (uses cache, no network request):\")\nseq2 = remote_store.get_substring(seq_digest_chr1, 5, 15)\nprint(f\"  Sequence: {seq2}\")\n</pre> # Second access - uses cached data (no network request) print(\"\\nSecond access (uses cache, no network request):\") seq2 = remote_store.get_substring(seq_digest_chr1, 5, 15) print(f\"  Sequence: {seq2}\") In\u00a0[\u00a0]: Copied! <pre># Example: Loading a remote genome (pseudocode - URL would need to be real)\n'''\n# Load human genome hg38 from a remote server\ncache_path = \"/data/refget_cache/hg38\"\nremote_url = \"https://refget-server.example.com/hg38\"\n\nhg38_store = GlobalRefgetStore.load_remote(cache_path, remote_url)\n\n# Get chr1 sequence (only chr1 data is downloaded and cached)\nchr1_digest = \"...\"\nsequence = hg38_store.get_substring(chr1_digest, 1000000, 1001000)\n\n# Subsequent accesses use the cached chr1 data\nanother_region = hg38_store.get_substring(chr1_digest, 2000000, 2001000)\n'''\nprint(\"See code cell for real-world usage example\")\n</pre> # Example: Loading a remote genome (pseudocode - URL would need to be real) ''' # Load human genome hg38 from a remote server cache_path = \"/data/refget_cache/hg38\" remote_url = \"https://refget-server.example.com/hg38\"  hg38_store = GlobalRefgetStore.load_remote(cache_path, remote_url)  # Get chr1 sequence (only chr1 data is downloaded and cached) chr1_digest = \"...\" sequence = hg38_store.get_substring(chr1_digest, 1000000, 1001000)  # Subsequent accesses use the cached chr1 data another_region = hg38_store.get_substring(chr1_digest, 2000000, 2001000) ''' print(\"See code cell for real-world usage example\") In\u00a0[\u00a0]: Copied! <pre>import shutil\n\n# Clean up temporary directory\nshutil.rmtree(temp_dir)\nprint(f\"Cleaned up temporary directory: {temp_dir}\")\n</pre> import shutil  # Clean up temporary directory shutil.rmtree(temp_dir) print(f\"Cleaned up temporary directory: {temp_dir}\")"},{"location":"gtars/python/refgetstore/#globalrefgetstore-tutorial","title":"GlobalRefgetStore Tutorial\u00b6","text":"<p>This notebook demonstrates how to use the <code>GlobalRefgetStore</code> class from the <code>gtars.refget</code> module for managing reference genome sequences using the GA4GH refget standard.</p>"},{"location":"gtars/python/refgetstore/#features-covered","title":"Features Covered\u00b6","text":"<ol> <li>Creating and populating a local RefgetStore</li> <li>Retrieving sequences by ID and collection name</li> <li>Getting substrings and BED file regions</li> <li>Extracting complete FASTA from a sequence collection</li> <li>Saving and loading stores</li> <li>Loading remote RefgetStores with lazy loading</li> <li>Working with cached remote sequences</li> </ol>"},{"location":"gtars/python/refgetstore/#setup","title":"Setup\u00b6","text":"<p>First, import the necessary modules:</p>"},{"location":"gtars/python/refgetstore/#part-1-local-refgetstore-full-workflow","title":"Part 1: Local RefgetStore - Full Workflow\u00b6","text":"<p>This section demonstrates the complete workflow for creating and using a local RefgetStore.</p>"},{"location":"gtars/python/refgetstore/#11-create-a-sample-fasta-file","title":"1.1 Create a Sample FASTA File\u00b6","text":"<p>We'll create a temporary FASTA file with two chromosomes:</p>"},{"location":"gtars/python/refgetstore/#12-digest-the-fasta-to-get-collection-information","title":"1.2 Digest the FASTA to Get Collection Information\u00b6","text":"<p>The <code>digest_fasta</code> function computes GA4GH-compliant digests for each sequence:</p>"},{"location":"gtars/python/refgetstore/#13-initialize-and-populate-a-globalrefgetstore","title":"1.3 Initialize and Populate a GlobalRefgetStore\u00b6","text":"<p>Create a store in <code>Encoded</code> mode for efficient storage:</p>"},{"location":"gtars/python/refgetstore/#14-retrieve-sequences-by-id","title":"1.4 Retrieve Sequences by ID\u00b6","text":"<p>Get a complete sequence using its digest:</p>"},{"location":"gtars/python/refgetstore/#15-get-substrings","title":"1.5 Get Substrings\u00b6","text":"<p>Extract specific regions from sequences:</p>"},{"location":"gtars/python/refgetstore/#16-retrieve-regions-from-bed-files","title":"1.6 Retrieve Regions from BED Files\u00b6","text":"<p>Extract multiple regions specified in a BED file:</p>"},{"location":"gtars/python/refgetstore/#17-write-retrieved-sequences-to-fasta","title":"1.7 Write Retrieved Sequences to FASTA\u00b6","text":"<p>Save BED file regions as a new FASTA file:</p>"},{"location":"gtars/python/refgetstore/#18-extract-complete-fasta-from-a-sequence-collection","title":"1.8 Extract Complete FASTA from a Sequence Collection\u00b6","text":"<p>You can reconstruct the entire FASTA file from a sequence collection by creating a BED file that covers all chromosomes:</p>"},{"location":"gtars/python/refgetstore/#19-save-store-to-disk","title":"1.9 Save Store to Disk\u00b6","text":"<p>Persist the store for later use:</p>"},{"location":"gtars/python/refgetstore/#110-load-store-from-disk","title":"1.10 Load Store from Disk\u00b6","text":"<p>Load a previously saved store (with lazy loading):</p>"},{"location":"gtars/python/refgetstore/#part-2-remote-refgetstore-with-lazy-loading","title":"Part 2: Remote RefgetStore with Lazy Loading\u00b6","text":"<p>The <code>load_remote()</code> method allows you to work with RefgetStores hosted on remote servers without downloading everything upfront. Sequence data is fetched on-demand and cached locally.</p>"},{"location":"gtars/python/refgetstore/#21-understanding-remote-stores","title":"2.1 Understanding Remote Stores\u00b6","text":"<p>Key Concepts:</p> <ul> <li>Metadata loaded immediately: Index files (<code>index.json</code>, <code>sequences.farg</code>) are fetched from remote</li> <li>Sequence data loaded on-demand: Actual <code>.seq</code> files are only downloaded when accessed</li> <li>Local caching: Downloaded sequences are cached to avoid re-fetching</li> <li>User-controlled cache: You specify where cached data is stored</li> </ul>"},{"location":"gtars/python/refgetstore/#22-simulating-a-remote-store","title":"2.2 Simulating a Remote Store\u00b6","text":"<p>For this example, we'll use the store we created earlier and treat it as a \"remote\" store using a <code>file://</code> URL. In practice, you'd use <code>http://</code> or <code>https://</code> URLs pointing to an actual remote server.</p>"},{"location":"gtars/python/refgetstore/#23-load-remote-store","title":"2.3 Load Remote Store\u00b6","text":"<p>Load a store from a remote URL with local caching:</p>"},{"location":"gtars/python/refgetstore/#24-access-sequences-triggers-lazy-loading","title":"2.4 Access Sequences (Triggers Lazy Loading)\u00b6","text":"<p>When you request a sequence, it's automatically fetched and cached:</p>"},{"location":"gtars/python/refgetstore/#25-real-world-example-template","title":"2.5 Real-World Example Template\u00b6","text":"<p>Here's how you would use this with an actual remote RefgetStore:</p>"},{"location":"gtars/python/refgetstore/#part-3-comparison-local-vs-remote-loading","title":"Part 3: Comparison - Local vs Remote Loading\u00b6","text":""},{"location":"gtars/python/refgetstore/#when-to-use-each-method","title":"When to Use Each Method\u00b6","text":"Method Use Case Pros Cons <code>load_local(path)</code> Store is already on disk Fast, no network Requires full download <code>load_remote(cache, url)</code> Store hosted remotely Only download what you need Slower first access"},{"location":"gtars/python/refgetstore/#performance-characteristics","title":"Performance Characteristics\u00b6","text":"<p>Local Store:</p> <ul> <li>Initial load: Fast (metadata only)</li> <li>First sequence access: Fast (local disk)</li> <li>Subsequent access: Fast (cached in memory)</li> </ul> <p>Remote Store:</p> <ul> <li>Initial load: 2 HTTP requests (index.json, sequences.farg)</li> <li>First sequence access: 1 HTTP request + cache write</li> <li>Subsequent access: Fast (cached on disk)</li> </ul>"},{"location":"gtars/python/refgetstore/#benefits-of-lazy-loading","title":"Benefits of Lazy Loading\u00b6","text":"<ol> <li>Memory efficient: Only loaded sequences consume memory</li> <li>Bandwidth efficient: Only download what you use</li> <li>Fast startup: No waiting for full genome download</li> <li>Selective caching: Control which sequences to cache locally</li> </ol>"},{"location":"gtars/python/refgetstore/#cleanup","title":"Cleanup\u00b6","text":"<p>Remove temporary files:</p>"},{"location":"gtars/python/refgetstore/#summary","title":"Summary\u00b6","text":"<p>This notebook demonstrated:</p> <ol> <li>\u2705 Creating and populating a RefgetStore from FASTA files</li> <li>\u2705 Retrieving sequences by digest and collection name</li> <li>\u2705 Extracting substrings and BED file regions</li> <li>\u2705 Extracting complete FASTA files from sequence collections</li> <li>\u2705 Saving stores to disk and loading them back</li> <li>\u2705 Loading remote RefgetStores with lazy loading</li> <li>\u2705 On-demand sequence fetching with local caching</li> </ol> <p>For more information, see the gtars documentation.</p>"},{"location":"gtars/r/refgetstore/","title":"RefgetStore","text":"<p>Example: Full <code>GlobalRefgetStore</code> Usage</p> <pre><code>library(gtars)\n\ntemp_dir &lt;- tempdir()\n\n# 1. Prepare a dummy FASTA file\ntemp_fasta_path &lt;- file.path(temp_dir, \"source.fa\")\nfasta_content &lt;- paste(\n  \"&gt;chr1\",\n  \"ATGCATGCATGCAGTCGTAGC\",\n  \"&gt;chr2\", \n  \"GGGGAAAA\",\n  sep = \"\\n\"\n)\nwriteLines(fasta_content, temp_fasta_path)\n\n# 2. Digest the FASTA to get collection info and digest\ncollection &lt;- digest_fasta(temp_fasta_path)\ncollection_digest &lt;- collection@digest\ncat(sprintf(\"Source FASTA digested. Collection digest: %s\\n\", collection_digest))\n\n# 3. Initialize GlobalRefgetStore in Encoded mode\nstore &lt;- global_refget_store(\"encoded\")\ncat(sprintf(\"Initialized store: %s\\n\", store))\n\n# 4. Import FASTA into the store\nimport_fasta(store, temp_fasta_path)\ncat(sprintf(\"FASTA imported into the store.\\n\"))\n\n# 5. Get a sequence by its ID (using the digest from the first sequence in collection)\nseq_digest_chr1 = collection[1]@metadata@sha512t24u\nrecord_chr1 = get_sequence_by_id(store, seq_digest_chr1)\nif (!is.null(record_chr1)) {\n  cat(sprintf(\"Retrieved sequence by ID: %s, length %s\\n\", \n              record_chr1@metadata@name, record_chr1@metadata@length))\n  cat(sprintf(\"  Sequence (full): %s\\n\", \n              get_substring(store, seq_digest_chr1, 0, record_chr1@metadata@length)))\n}\n\n# 6. Get a substring\nsub_seq &lt;- get_substring(store, seq_digest_chr1, 5, 15)\ncat(sprintf(\"Substring from chr1[5:15]: %s\\n\", sub_seq))\n\n# 7. Prepare a BED file for region retrieval\ntemp_bed_path &lt;- file.path(temp_dir, \"test.bed\")\nbed_content &lt;- paste(\n  \"chr1\\t0\\t10\",\n  \"chr2\\t2\\t6\", \n  \"chr_nonexistent\\t0\\t5\",\n  sep = \"\\n\"\n)\nwriteLines(bed_content, temp_bed_path)\n\n# 8. Retrieve sequences from BED file to a list\nretrieved_list &lt;- get_seqs_bed_file_to_vec(store, collection_digest, temp_bed_path)\ncat(\"Retrieved sequences from BED file (as list):\\n\")\nfor (rs in retrieved_list) {\n  print(rs)\n}\n\n# 9. Retrieve sequences from BED file and write to new FASTA\ntemp_output_fa_path &lt;- file.path(temp_dir, \"output.fa\")\nget_seqs_bed_file(store, collection_digest, temp_bed_path, temp_output_fa_path)\ncat(sprintf(\"Retrieved sequences from BED file written to: %s\\n\", temp_output_fa_path))\n\n# 10. Write store to a new directory\ntemp_saved_store_path &lt;- file.path(temp_dir, \"my_refget_store\")\nwrite_store_to_directory(store, temp_saved_store_path, \"{digest_prefix}/{digest}.gz\")\ncat(sprintf(\"Store saved to: %s\\n\", temp_saved_store_path))\n\n# 11. Load store from the directory\nstore_load &lt;- load_local(temp_saved_store_path)\ncat(sprintf(\"Store successfully loaded from: %s\\n\", temp_saved_store_path))\n</code></pre>"},{"location":"manuscripts/gharavi2021/","title":"Embeddings of genomic region sets capture rich biological associations in low dimensions","text":""},{"location":"manuscripts/gharavi2021/#relevant-tutorials","title":"Relevant tutorials","text":"<p>This paper was our first publication showing how to build and evaluate region set embeddings using region-set2vec, based on word2vec.</p> <p>See: train Region2Vec embeddings</p>"},{"location":"manuscripts/gharavi2024/","title":"Joint representation learning for retrieval and annotation of genomic interval sets","text":"<p>Paper: Manuscript at Bioengineering </p>"},{"location":"manuscripts/gharavi2024/#abstract","title":"Abstract","text":"<p>As available genomic interval data increase in scale, we require fast systems to search them. A common approach is simple string matching to compare a search term to metadata, but this is limited by incomplete or inaccurate annotations. An alternative is to compare data directly through genomic region overlap analysis, but this approach leads to challenges like sparsity, high dimensionality, and computational expense. We require novel methods to quickly and flexibly query large, messy genomic interval databases. Here, we develop a genomic interval search system using representation learning. We train numerical embeddings for a collection of region sets simultaneously with their metadata labels, capturing similarity between region sets and their metadata in a low-dimensional space. Using these learned co-embeddings, we develop a system that solves three related information retrieval tasks using embedding distance computations: retrieving region sets related to a user query string, suggesting new labels for database region sets, and retrieving database region sets similar to a query region set. We evaluate these use cases and show that jointly learned representations of region sets and metadata are a promising approach for fast, flexible, and accurate genomic region information retrieval.</p>"},{"location":"manuscripts/gharavi2024/#relevant-tutorials","title":"Relevant tutorials","text":"<p>This paper trained BEDspace models (using StarSpace with BED files). See these tutorials:</p> <ul> <li>How to use BEDSpace to jointly embed regions and metadata</li> </ul>"},{"location":"manuscripts/gu2021/","title":"Bedshift: perturbation of genomic interval sets","text":"<p>Paper: Manuscript at Genome Biology </p>"},{"location":"manuscripts/gu2021/#abstract","title":"Abstract","text":"<p>Functional genomics experiments, like ChIP-Seq or ATAC-Seq, produce results that are summarized as a region set. There is no way to objectively evaluate the effectiveness of region set similarity metrics. We present Bedshift, a tool for perturbing BED files by randomly shifting, adding, and dropping regions from a reference file. The perturbed files can be used to benchmark similarity metrics, as well as for other applications. We highlight differences in behavior between metrics, such as that the Jaccard score is most sensitive to added or dropped regions, while coverage score is most sensitive to shifted regions.</p>"},{"location":"manuscripts/gu2021/#relevant-tutorials","title":"Relevant tutorials","text":"<p>Analysis from the paper is described in these tutorials: </p> <ul> <li>Randomizing BED files with BEDshift</li> </ul>"},{"location":"manuscripts/leroy2024/","title":"Fast clustering and cell-type annotation of scATAC data using pre-trained embeddings","text":"<p>Paper: Manuscript at bioRxiv </p>"},{"location":"manuscripts/leroy2024/#abstract","title":"Abstract","text":"<p>Motivation Data from the single-cell assay for transposase-accessible chromatin using sequencing (scATAC-seq) is now widely available. One major computational challenge is dealing with high dimensionality and inherent sparsity, which is typically addressed by producing lower-dimensional representations of single cells for downstream clustering tasks. Current approaches produce such individual cell embeddings directly through a one-step learning process. Here, we propose an alternative approach by building embedding models pre-trained on reference data. We argue that this provides a more flexible analysis workflow that also has computational performance advantages through transfer learning.</p> <p>Results We implemented our approach in scEmbed, an unsupervised machine learning framework that learns low-dimensional embeddings of genomic regulatory regions to represent and analyze scATAC-seq data. scEmbed performs well in terms of clustering ability and has the key advantage of learning patterns of region co-occurrence that can be transferred to other, unseen datasets. Moreover, pre-trained models on reference data can be exploited to build fast and accurate cell-type annotation systems without the need for other data modalities. scEmbed is implemented in Python and it is available to download from GitHub. We also make our pre-trained models available on huggingface for public use.</p>"},{"location":"manuscripts/leroy2024/#relevant-tutorials","title":"Relevant tutorials","text":"<p>Analysis from the paper is described in these tutorials: </p> <ul> <li>Train single-cell embeddings</li> <li>Populate a vector store</li> <li>Predict cell-types using KNN</li> </ul>"},{"location":"manuscripts/rymuza2024/","title":"Methods for constructing and evaluating consensus genomic interval sets","text":"<p>Paper: Manuscript at bioRxiv </p>"},{"location":"manuscripts/rymuza2024/#abstract","title":"Abstract","text":"<p>The amount of genomic region data continues to increase. Integrating across diverse genomic region sets requires consensus regions, which enable comparing regions across experiments, but also by necessity lose precision in region definitions. We require methods to assess this loss of precision and build optimal consensus region sets.</p> <p>Here, we introduce the concept of flexible intervals and propose 3 novel methods for building consensus region sets, or universes: a coverage cutoff method, a likelihood method, and a Hidden Markov Model. We then propose 3 novel measures for evaluating how well a proposed universe fits a collection of region sets: a base-level overlap score, a region boundary distance score, and a likelihood score. We apply our methods and evaluation approaches to several collections of region sets and show how these methods can be used to evaluate fit of universes and build optimal universes. We describe scenarios where the common approach of merging regions to create consensus leads to undesirable outcomes and provide principled alternatives that provide interoperability of interval data while minimizing loss of resolution.</p>"},{"location":"manuscripts/rymuza2024/#relevant-tutorials","title":"Relevant tutorials","text":"<p>This paper published 2 types of method: 1. Methods to construct a universe, and 2. Methods to evaluate a universe.</p>"},{"location":"manuscripts/rymuza2024/#1-constructing-a-universe","title":"1. Constructing a universe","text":"<p>You can construct a universe either on the command line, or using geniml as a library:</p> <ul> <li>Create consensus peaks with CLI</li> <li>Create consensus peaks with Python</li> </ul>"},{"location":"manuscripts/rymuza2024/#2-evaluating-a-universe","title":"2. Evaluating a universe","text":"<p>The main methods are implemented in the <code>assess-universe</code> model with tutorial:</p> <ul> <li>Assess universe fit tutorial</li> </ul>"},{"location":"manuscripts/zheng2024/","title":"Methods for evaluating unsupervised vector representations of genomic regions","text":"<p>Paper: Manuscript at bioRxiv </p>"},{"location":"manuscripts/zheng2024/#abstract","title":"Abstract","text":"<p>Representation learning models have become a mainstay of modern genomics. These models are trained to yield vector representations, or embeddings, of various biological entities, such as cells, genes, individuals, or genomic regions. Recent applications of unsupervised embedding approaches have been shown to learn relationships among genomic regions that define functional elements in a genome. Unsupervised representation learning of genomic regions is free of the supervision from curated metadata and can condense rich biological knowledge from publicly available data to region embeddings. However, there exists no method for evaluating the quality of these embeddings in the absence of metadata, making it difficult to assess the reliability of analyses based on the embeddings, and to tune model training to yield optimal results. To bridge this gap, we propose four evaluation metrics: the cluster tendency score (CTS), the reconstruction score (RCS), the genome distance scaling score (GDSS), and the neighborhood preserving score (NPS). The CTS and RCS statistically quantify how well region embeddings can be clustered and how well the embeddings preserve information in training data. The GDSS and NPS exploit the biological tendency of regions close in genomic space to have similar biological functions; they measure how much such information is captured by individual region embeddings in a set. We demonstrate the utility of these statistical and biological scores for evaluating unsupervised genomic region embeddings and provide guidelines for learning reliable embeddings.</p>"},{"location":"manuscripts/zheng2024/#relevant-tutorials","title":"Relevant tutorials","text":"<p>Analysis from the paper is described in these tutorials: </p> <ul> <li>How to evaluate embeddings</li> </ul>"}]}