{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEDBASE workflow tutorial\n",
    "\n",
    "This demo demonstrates how to process, analyze, visualize, and serve BED files. The process has 5 steps: First, the [bedmaker](https://github.com/databio/bedmaker) pipeline converts different region data files (bed, bedGraph, bigBed, bigWig, and wig) into BED format and generates bigBed format for each file for visualization in Genome Browser.  An optional step, the [bedqc](https://github.com/databio/bedqc) pipline, flags the BED files that you might not want to include in the downstream analysis.  Second, individual BED files are analyzed using the [bedstat](https://github.com/databio/bedstat) pipeline. Third, BED files are grouped and then analyzed as groups using the [bedbuncher](https://github.com/databio/bedbuncher) pipeline. Fourth, [bedembed](https://github.com/databio/bedembed) uses the StarSpace method to embed the bed files and the meta data, and the distances between the file labels and trained search terms will be calculated with cosine distance. Finally, the BED files, along with statistics, plots, and grouping information, is served via a web interface and RESTful API using the [bedhost](https://github.com/databio/bedhost) package.\n",
    "\n",
    "**Glossary of terms:**\n",
    "\n",
    "- *bedfile*: a tab-delimited file with one genomic region per line. Each genomic region is decribed by 3 required columns: chrom, start and end.\n",
    "- *bedset*: a collection of BED files grouped by with a shared biological, experimental, or logical criterion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Preparation\" data-toc-modified-id=\"1.-Preparation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Preparation</a></span></li><li><span><a href=\"#2.-BEDMAKER:-convert-non-bed-files-into-bed-files-and-generate-bigBed-files-for-genome-browser-tracks\" data-toc-modified-id=\"2.-BEDMAKER:-convert-non-bed-files-into-bed-files-and-generate-bigBed-files-for-genome-browser-tracks-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. BEDMAKER: convert non-bed files into bed files and generate bigBed files for genome browser tracks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-a-PEP-describing-the-files-to-process\" data-toc-modified-id=\"Get-a-PEP-describing-the-files-to-process-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Get a PEP describing the files to process</a></span></li><li><span><a href=\"#Run-bedmaker-on-the-demo-PEP\" data-toc-modified-id=\"Run-bedmaker-on-the-demo-PEP-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Run bedmaker on the demo PEP</a></span></li></ul></li><li><span><a href=\"#OPTIONAL-BEDQC:-flag-bed-files-for-futher-evaluation-to-determine-whether-they-should-be-included-in-the-downstream-analysis\" data-toc-modified-id=\"OPTIONAL-BEDQC:-flag-bed-files-for-futher-evaluation-to-determine-whether-they-should-be-included-in-the-downstream-analysis\"><span class=\"toc-item-num\">&nbsp;&nbsp;</span>OPTIONAL BEDQC: flag bed files for futher evaluation to determine whether they should be included in the downstream analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-a-PEP-describing-the-files-to-process\" data-toc-modified-id=\"Get-a-PEP-describing-the-files-to-process\"><span class=\"toc-item-num\">&nbsp;&nbsp;</span>Get a PEP describing the files to process</a></span></li><li><span><a href=\"#Run-bedqc-on-the-demo-PEP\" data-toc-modified-id=\"Run-bedqc-on-the-demo-PEP\"><span class=\"toc-item-num\">&nbsp;&nbsp;</span>Run bedqc on the demo PEP</a></span></li></ul></li><li><span><a href=\"#3.-BEDSTAT:-Generate-statistics-and-plots-of-BED-files\" data-toc-modified-id=\"3.-BEDSTAT:-Generate-statistics-and-plots-of-BED-files-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. BEDSTAT: Generate statistics and plots of BED files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-a-PEP-describing-the-bedfiles-to-process\" data-toc-modified-id=\"Get-a-PEP-describing-the-bedfiles-to-process-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Get a PEP describing the bedfiles to process</a></span></li><li><span><a href=\"#Install-bedstat-dependencies\" data-toc-modified-id=\"Install-bedstat-dependencies-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Install bedstat dependencies</a></span></li><li><span><a href=\"#Inititiate-a-local-PostgreSQL-instance\" data-toc-modified-id=\"Inititiate-a-local-PostgreSQL-instance-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Inititiate a local PostgreSQL instance</a></span></li><li><span><a href=\"#Run-bedstat--on-the-demo-PEP\" data-toc-modified-id=\"Run-bedstat--on-the-demo-PEP-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Run bedstat  on the demo PEP</a></span></li></ul></li><li><span><a href=\"#4.-BEDBUNCHER:-Create-bedsets-and-their-respective-statistics\" data-toc-modified-id=\"4.-BEDBUNCHER:-Create-bedsets-and-their-respective-statistics-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. BEDBUNCHER: Create bedsets and their respective statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-a-new-PEP-describing-the-bedset-name-and-specific-JSON-query\" data-toc-modified-id=\"Create-a-new-PEP-describing-the-bedset-name-and-specific-JSON-query-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Create a new PEP describing the bedset name and specific JSON query</a></span></li><li><span><a href=\"#Create-outputs-directory-and-install-bedbuncher-CML-dependencies\" data-toc-modified-id=\"Create-outputs-directory-and-install-bedbuncher-CML-dependencies-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Create outputs directory and install bedbuncher CML dependencies</a></span></li><li><span><a href=\"#Run-bedbuncher-using-Looper\" data-toc-modified-id=\"Run-bedbuncher-using-Looper-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Run bedbuncher using Looper</a></span></li></ul></li><li><span><a href=\"#5.-BEDHOST:--Serve-BED-files-and-API-to-explore-pipeline-outputs\" data-toc-modified-id=\"5.-BEDHOST:--Serve-BED-files-and-API-to-explore-pipeline-outputs-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. BEDHOST:  Serve BED files and API to explore pipeline outputs</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation \n",
    "\n",
    "First, we will create a tutorial directory where we'll store the bedbase pipelines and files to be processed. We'll also need to create an environment variable that points to the tutorial directory (we'll need this variable later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir bedbase_tutorial\n",
    "cd bedbase_tutorial\n",
    "export BEDBASE_DATA_PATH_HOST=`pwd`\n",
    "export CODE=`pwd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some example BED files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-06 11:34:36--  http://big.databio.org/example_data/bedbase_tutorial/bed_files.tar.gz\n",
      "Resolving big.databio.org (big.databio.org)... 128.143.245.182, 128.143.245.181\n",
      "Connecting to big.databio.org (big.databio.org)|128.143.245.182|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44549692 (42M) [application/octet-stream]\n",
      "Saving to: ‘bed_files.tar.gz’\n",
      "\n",
      "bed_files.tar.gz    100%[===================>]  42.49M  12.4MB/s    in 3.4s    \n",
      "\n",
      "2021-12-06 11:34:40 (12.4 MB/s) - ‘bed_files.tar.gz’ saved [44549692/44549692]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget http://big.databio.org/example_data/bedbase_tutorial/bed_files.tar.gz     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded files are compressed so we'll need to untar them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bed_files/\n",
      "bed_files/GSE105587_ENCFF018NNF_conservative_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bed_files/GSM2423312_ENCFF155HVK_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE105977_ENCFF617QGK_optimal_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE91663_ENCFF316ASR_peaks_GRCh38.bed.gz\n",
      "bed_files/GSM2423313_ENCFF722AOG_peaks_GRCh38.bed.gz\n",
      "bed_files/GSM2827349_ENCFF196DNQ_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE91663_ENCFF553KIK_optimal_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE91663_ENCFF319TPR_conservative_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE105977_ENCFF937CGY_peaks_GRCh38.bed.gz\n",
      "bed_files/GSM2827350_ENCFF928JXU_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE105977_ENCFF793SZW_conservative_idr_thresholded_peaks_GRCh38.bed.gz\n"
     ]
    }
   ],
   "source": [
    "tar -zxvf bed_files.tar.gz && mv bed_files files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm bed_files.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we'll download a matrix we need to provide if we wish to plot the tissue specificity of our set of genomic ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-06 11:35:27--  http://big.databio.org/open_chromatin_matrix/openSignalMatrix_hg38_percentile99_01_quantNormalized_round4d.txt.gz\n",
      "Resolving big.databio.org (big.databio.org)... 128.143.245.181, 128.143.245.182\n",
      "Connecting to big.databio.org (big.databio.org)|128.143.245.181|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 380989260 (363M) [application/octet-stream]\n",
      "Saving to: ‘openSignalMatrix_hg38_percentile99_01_quantNormalized_round4d.txt.gz’\n",
      "\n",
      "openSignalMatrix_hg 100%[===================>] 363.34M  1.50MB/s    in 2m 50s  \n",
      "\n",
      "2021-12-06 11:38:18 (2.14 MB/s) - ‘openSignalMatrix_hg38_percentile99_01_quantNormalized_round4d.txt.gz’ saved [380989260/380989260]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget http://big.databio.org/open_chromatin_matrix/openSignalMatrix_hg38_percentile99_01_quantNormalized_round4d.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll download the core pipelines and tools needed to complete this tutorial: `bedmaker`, `bedqc`, `bedstat`, `bedbuncher` , `bedhost`, and `bedhost-ui`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bedmaker'...\n",
      "remote: Enumerating objects: 410, done.\u001b[K\n",
      "remote: Counting objects: 100% (241/241), done.\u001b[K\n",
      "remote: Compressing objects: 100% (180/180), done.\u001b[K\n",
      "remote: Total 410 (delta 142), reused 139 (delta 60), pack-reused 169\u001b[K\n",
      "Receiving objects: 100% (410/410), 81.41 KiB | 5.43 MiB/s, done.\n",
      "Resolving deltas: 100% (223/223), done.\n",
      "Cloning into 'bedstat'...\n",
      "remote: Enumerating objects: 750, done.\u001b[K\n",
      "remote: Counting objects: 100% (260/260), done.\u001b[K\n",
      "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
      "remote: Total 750 (delta 120), reused 221 (delta 98), pack-reused 490\u001b[K\n",
      "Receiving objects: 100% (750/750), 165.38 KiB | 5.91 MiB/s, done.\n",
      "Resolving deltas: 100% (351/351), done.\n",
      "Cloning into 'bedbuncher'...\n",
      "remote: Enumerating objects: 611, done.\u001b[K\n",
      "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
      "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
      "remote: Total 611 (delta 66), reused 105 (delta 37), pack-reused 468\u001b[K\n",
      "Receiving objects: 100% (611/611), 113.48 KiB | 4.20 MiB/s, done.\n",
      "Resolving deltas: 100% (319/319), done.\n",
      "Cloning into 'bedhost'...\n",
      "remote: Enumerating objects: 1934, done.\u001b[K\n",
      "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
      "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
      "remote: Total 1934 (delta 58), reused 50 (delta 20), pack-reused 1811\u001b[K\n",
      "Receiving objects: 100% (1934/1934), 454.49 KiB | 6.06 MiB/s, done.\n",
      "Resolving deltas: 100% (1190/1190), done.\n",
      "Cloning into 'bedhost-ui'...\n",
      "remote: Enumerating objects: 1462, done.\u001b[K\n",
      "remote: Counting objects: 100% (825/825), done.\u001b[K\n",
      "remote: Compressing objects: 100% (298/298), done.\u001b[K\n",
      "remote: Total 1462 (delta 595), reused 732 (delta 526), pack-reused 637\u001b[K\n",
      "Receiving objects: 100% (1462/1462), 761.58 KiB | 12.48 MiB/s, done.\n",
      "Resolving deltas: 100% (1092/1092), done.\n"
     ]
    }
   ],
   "source": [
    "git clone -b dev git@github.com:databio/bedbase.git\n",
    "git clone git@github.com:databio/bedmaker\n",
    "git clone git@github.com:databio/bedqc\n",
    "git clone -b validate_genome_assembly git@github.com:databio/bedstat\n",
    "git clone -b validate_genome_assembly git@github.com:databio/bedbuncher\n",
    "git clone git@github.com:databio/bedembed\n",
    "git clone -b dev git@github.com:databio/bedhost\n",
    "git clone git@github.com:databio/bedhost-ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BEDMAKER: convert non-bed files into bed files and generate bigBed files for genome browser tracks\n",
    "\n",
    "### Get a PEP describing the files to process\n",
    "\n",
    "This is a preprocess step to convert non-bed files into bed format using `bedmaker`. Currently supported formats are bedGraph, bigBed, bigWig and wig. `Bedmaker` also generates bigBed files that will be using in Genome Browser. To begin, we'll need some annotation information for our files to load. We'll use the standard [PEP](http://pep.databio.org) format for the annotation, which consists of 1) a sample table (.csv) that annotates the files, and 2) a project config.yaml file that points to the sample annotation sheet. The config file also has other components, such as derived and implied attributes, that in this case point to the files to be processed and whether they are narrowpeak or not. Here is the PEP config file for this example project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pep_version: 2.0.0\n",
      "sample_table: bedstat_annotation_sheet.csv\n",
      "\n",
      "looper:\n",
      "    output-dir: $BEDBASE_DATA_PATH_HOST/outputs/bedmaker_output/bedmaker_pipeline_logs \n",
      "\n",
      "sample_modifiers:\n",
      "  append:\n",
      "    pipeline_interfaces: $CODE/bedmaker/pipeline_interface.yaml\n",
      "    input_file_path: INPUT\n",
      "    output_bed_path: BOUT\n",
      "    output_bigbed_path: $BEDBASE_DATA_PATH_HOST/bigbed_files\n",
      "    narrowpeak: TRUE\n",
      "    rfg_config_path: RFG\n",
      "    protocol: \"make_bed\"\n",
      "  derive:\n",
      "    attributes: [input_file_path, output_bed_path, rfg_config_path]\n",
      "    sources:\n",
      "      INPUT: \"$BEDBASE_DATA_PATH_HOST/files/{file_name}\"\n",
      "      BOUT: \"$BEDBASE_DATA_PATH_HOST/bed_files/{file_name}\" \n",
      "      RFG: \"$REFGENIE\"\n",
      "  imply:\n",
      "    - if:\n",
      "        antibody: [H3K4me3, H3K27me3, H3K27ac, H3K9ac, H4K5ac, H3K4me, H3K36me3, H4K5ac, H3K9ac]\n",
      "      then:\n",
      "        narrowpeak: FALSE\n"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/PEPs/bedmaker_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output bigBed files will be stored in `$BEDBASE_DATA_PATH_HOST/bigbed_files`.bed files will be stored in `$BEDBASE_DATA_PATH_HOST/bed_files`. But We'll need to create a directory where we can store the log and submission files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p outputs/bedmaker_output/bedmaker_pipeline_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we have to initialize environment variable $REFGENIE - the path to the refgenie configuration file. If Refgenie is not initialize, we will have to initialize it localy. use `pip install --user refgenie` to install and add to the PATH with `export PATH=~/.local/bin:$PATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized genome configuration file: /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/genome_config.yaml\n",
      "Created directories:\n",
      " - /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/data\n",
      " - /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/alias\n"
     ]
    }
   ],
   "source": [
    "export REFGENIE='genome_config.yaml'\n",
    "refgenie init -c $REFGENIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step requires `bedToBigBed`. If you don't have it installed, you can download it from [ucsc](http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/bedToBigBed), and add to PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-06 12:12:02--  http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/bedToBigBed\n",
      "Resolving hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)... 128.114.119.163\n",
      "Connecting to hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)|128.114.119.163|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9446624 (9.0M)\n",
      "Saving to: ‘bedToBigBed’\n",
      "\n",
      "bedToBigBed         100%[===================>]   9.01M   784KB/s    in 12s     \n",
      "\n",
      "2021-12-06 12:12:15 (762 KB/s) - ‘bedToBigBed’ saved [9446624/9446624]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/bedToBigBed\n",
    "chmod a+x bedToBigBed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bedmaker on the demo PEP\n",
    "\n",
    "To run bedmaker and the other required pipelines in this tutorial, we will rely on the pipeline submission engine looper, which can be installed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install looper --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looper version: 1.3.1\n",
      "Command: run\n",
      "/home/bnt4me/.local/lib/python3.8/site-packages/divvy/compute.py:150: UserWarning: The '_file_path' property is deprecated and will be removed in a future release. Use ComputingConfiguration[\"__internal\"][\"_file_path\"] instead.\n",
      "  os.path.dirname(self._file_path),\n",
      "/home/bnt4me/.local/lib/python3.8/site-packages/divvy/compute.py:58: UserWarning: The '_file_path' property is deprecated and will be removed in a future release. Use ComputingConfiguration[\"__internal\"][\"_file_path\"] instead.\n",
      "  self.config_file = self._file_path\n",
      "Activating compute package 'local'\n",
      "\u001b[36m## [1 of 11] sample: bedbase_demo_db1; pipeline: BEDMAKER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db1.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db1.sub\n",
      "\u001b[36m## [2 of 11] sample: bedbase_demo_db2; pipeline: BEDMAKER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db2.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db2.sub\n",
      "\u001b[36m## [3 of 11] sample: bedbase_demo_db3; pipeline: BEDMAKER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db3.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db3.sub\n",
      "\u001b[36m## [4 of 11] sample: bedbase_demo_db4; pipeline: BEDMAKER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db4.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db4.sub\n",
      "\u001b[36m## [5 of 11] sample: bedbase_demo_db5; pipeline: BEDMAKER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db5.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db5.sub\n",
      "\u001b[36m## [6 of 11] sample: bedbase_demo_db6; pipeline: BEDMAKER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db6.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db6.sub\n",
      "\u001b[36m## [7 of 11] sample: bedbase_demo_db7; pipeline: BEDMAKER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db7.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db7.sub\n",
      "\u001b[36m## [8 of 11] sample: bedbase_demo_db8; pipeline: BEDMAKER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db8.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db8.sub\n",
      "\u001b[36m## [9 of 11] sample: bedhost_demo_db9; pipeline: BEDMAKER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedhost_demo_db9.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedhost_demo_db9.sub\n",
      "\u001b[36m## [10 of 11] sample: bedbase_demo_db10; pipeline: BEDMAKER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db10.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db10.sub\n",
      "\u001b[36m## [11 of 11] sample: bedbase_demo_db11; pipeline: BEDMAKER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db11.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedmaker_output/bedmaker_pipeline_logs/submission/BEDMAKER_bedbase_demo_db11.sub\n",
      "\n",
      "Looper finished\n",
      "Samples valid for job generation: 11 of 11\n",
      "Commands submitted: 11 of 11\n",
      "Jobs submitted: 11\n"
     ]
    }
   ],
   "source": [
    "looper run bedbase/tutorial_files/PEPs/bedmaker_config.yaml --package local \\\n",
    "--command-extra=\"-R\" > outputs/bedmaker_output/bedmaker_pipeline_logs/looper_logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL BEDQC: flag bed files for futher evaluation to determine whether they should be included in the downstream analysis\n",
    "\n",
    "### Get a PEP describing the files to process\n",
    "\n",
    "This is an optional step to flag bed files for futher evaluation to determine whether they should be included in the downstream analysis using `bedqc`. Currently it flags bed files that are larger than 2G, has over 5 milliom regions, and/or has mean region width less than 10 bp. To begin, we'll need some annotation information for our files to load. We'll use the standard [PEP](http://pep.databio.org) format for the annotation, which consists of 1) a sample table (.csv) that annotates the files, and 2) a project config.yaml file that points to the sample annotation sheet. The config file also has other components, such as derived and implied attributes, that in this case point to the files to be processed and whether they are narrowpeak or not. Here is the PEP config file for this example project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pep_version: 2.0.0\n",
      "sample_table: bedstat_annotation_sheet.csv\n",
      "\n",
      "looper:\n",
      "    output-dir: $BEDBASE_DATA_PATH_HOST/outputs/bedqc_output/bedqc_pipeline_logs \n",
      "\n",
      "sample_modifiers:\n",
      "  append:\n",
      "    pipeline_interfaces: $BEDBASE_DATA_PATH_HOST/bedqc/pipeline_interface.yaml\n",
      "    input_file_path: INPUT\n",
      "    output_dir: $BEDBASE_DATA_PATH_HOST/outputs/bedqc_output/bedqc_pipeline_logs\n",
      "  derive:\n",
      "    attributes: [input_file_path]\n",
      "    sources:\n",
      "      INPUT: \"$BEDBASE_DATA_PATH_HOST/bed_files/{file_name}\" \n",
      "      \n"
     ]
    }
   ],
   "source": [
    "cat ../../bedbase/tutorial_files/PEPs/bedqc_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create a directory where we can store the output `flaged_bed.csv`, log and submission files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p outputs/bedqc_output/bedqc_pipeline_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bedqc on the demo PEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looper version: 1.3.0\n",
      "Command: run\n",
      "/home/bx2ur/.local/lib/python3.8/site-packages/divvy/compute.py:150: UserWarning: The '_file_path' property is deprecated and will be removed in a future release. Use ComputingConfiguration[\"__internal\"][\"_file_path\"] instead.\n",
      "  os.path.dirname(self._file_path),\n",
      "/home/bx2ur/.local/lib/python3.8/site-packages/divvy/compute.py:58: UserWarning: The '_file_path' property is deprecated and will be removed in a future release. Use ComputingConfiguration[\"__internal\"][\"_file_path\"] instead.\n",
      "  self.config_file = self._file_path\n",
      "Activating compute package 'local'\n",
      "\u001b[36m## [1 of 17] sample: bedbase_demo_db1; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db1.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db1.sub\n",
      "\u001b[36m## [2 of 17] sample: bedbase_demo_db2; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db2.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db2.sub\n",
      "\u001b[36m## [3 of 17] sample: bedbase_demo_db3; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db3.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db3.sub\n",
      "\u001b[36m## [4 of 17] sample: bedbase_demo_db4; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db4.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db4.sub\n",
      "\u001b[36m## [5 of 17] sample: bedbase_demo_db5; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db5.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db5.sub\n",
      "\u001b[36m## [6 of 17] sample: bedbase_demo_db6; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db6.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db6.sub\n",
      "\u001b[36m## [7 of 17] sample: bedbase_demo_db7; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db7.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db7.sub\n",
      "\u001b[36m## [8 of 17] sample: bedbase_demo_db8; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db8.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db8.sub\n",
      "\u001b[36m## [9 of 17] sample: bedbase_demo_db9; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db9.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db9.sub\n",
      "\u001b[36m## [10 of 17] sample: bedbase_demo_db10; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db10.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db10.sub\n",
      "\u001b[36m## [11 of 17] sample: bedbase_demo_db11; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db11.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_bedbase_demo_db11.sub\n",
      "\u001b[36m## [12 of 17] sample: encode_test_1; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_encode_test_1.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_encode_test_1.sub\n",
      "\u001b[36m## [13 of 17] sample: encode_test_2; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_encode_test_2.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_encode_test_2.sub\n",
      "\u001b[36m## [14 of 17] sample: encode_test_3; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_encode_test_3.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_encode_test_3.sub\n",
      "\u001b[36m## [15 of 17] sample: encode_test_4; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_encode_test_4.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_encode_test_4.sub\n",
      "\u001b[36m## [16 of 17] sample: encode_test_5; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_encode_test_5.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_encode_test_5.sub\n",
      "\u001b[36m## [17 of 17] sample: encode_test_6; pipeline: BEDQC\u001b[0m\n",
      "Writing script to /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_encode_test_6.sub\n",
      "Job script (n=1; 0.00Gb): /home/bx2ur/Documents/data/bedbase_tutorial/outputs/bedqc_output/bedqc_pipeline_logs/submission/BEDQC_encode_test_6.sub\n",
      "\n",
      "Looper finished\n",
      "Samples valid for job generation: 17 of 17\n",
      "Commands submitted: 17 of 17\n",
      "Jobs submitted: 17\n"
     ]
    }
   ],
   "source": [
    "looper run bedbase/tutorial_files/PEPs/bedqc_config.yaml --package local \\\n",
    "--command-extra=\"-R\" > outputs/bedqc_output/bedqc_pipeline_logs/looper_logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the flaged bedfiles will stored as a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name,detail \n",
      "ENCFF464DKS.bed.gz,['Mean region width is less than 10 bp.'] \n",
      "ENCFF610FVD.bed.gz,['Mean region width is less than 10 bp.'] \n",
      "ENCFF756GON.bed.gz,['Mean region width is less than 10 bp.']\n"
     ]
    }
   ],
   "source": [
    "cat  outputs/bedqc_output/bedqc_pipeline_logs/flaged_bed.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BEDSTAT: Generate statistics and plots of BED files \n",
    "\n",
    "### Get a PEP describing the bedfiles to process\n",
    "\n",
    "The first step is to process the BED files using the `bedstat` pipeline, which computes statistics and makes plots for each individual BED file. To begin, we'll need some annotation information for our BED files to load. We'll use the standard [PEP](http://pep.databio.org) format for the annotation, which consists of 1) a sample table (.csv) that annotates the files, and 2) a project config.yaml file that points to the sample annotation sheet. The config file also has other components, such as derived attributes, that in this case point to the bedfiles to be processed. Here is the PEP config file for this example project. It includes annotation information for each BED file, and also points to the `.bed.gz` files using derived attributes `output_file_path` and `yaml_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pep_version: 2.0.0\n",
      "sample_table: bedstat_annotation_sheet.csv\n",
      "\n",
      "looper:\n",
      "    output-dir: $BEDBASE_DATA_PATH_HOST/outputs/bedstat_output/bedstat_pipeline_logs \n",
      "\n",
      "sample_modifiers:\n",
      "  append:\n",
      "    bedbase_config: $CODE/bedbase/tutorial_files/bedbase_configuration_compose.yaml\n",
      "    pipeline_interfaces: $CODE/bedstat/pipeline_interface.yaml\n",
      "    output_file_path: OUTPUT\n",
      "    yaml_file: SAMPLE_YAML\n",
      "    open_signal_matrix: MATRIX\n",
      "    bigbed:  BIGBED\n",
      "  derive:\n",
      "    attributes: [output_file_path, yaml_file, open_signal_matrix, bigbed]\n",
      "    sources:\n",
      "      OUTPUT: \"$BEDBASE_DATA_PATH_HOST/bed_files/{file_name}\" \n",
      "      SAMPLE_YAML: \"$BEDBASE_DATA_PATH_HOST/outputs/bedstat_output/bedstat_pipeline_logs/submission/{sample_name}_sample.yaml\"\n",
      "      MATRIX: \"$BEDBASE_DATA_PATH_HOST/openSignalMatrix_{genome}_percentile99_01_quantNormalized_round4d.txt.gz\"\n",
      "      BIGBED: \"$BEDBASE_DATA_PATH_HOST/bigbed_files\"\n"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/PEPs/bedstat_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install bedstat dependencies\n",
    "\n",
    "`bedstat` is a [pypiper](http://code.databio.org/pypiper/) pipeline that generates statistics and plots of bedfiles. Additionally, `bedstat` uses [bbconf](https://github.com/databio/bbconf), the bedbase configuration manager which implements convenience methods for interacting with an Elasticsearch database, where our file metadata will be placed. These and the appropriate R dependencies can be installed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "pip install -r bedstat/requirements.txt --user > requirements_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install R dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rscript bedstat/scripts/installRdeps.R > R_deps.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an additional dependency needed by `bedstat` if we wish to calculate and plot the GC content of our bedfiles. Depending on the genome assemblies of the files listed on a PEP, the appropriate BSgenome packages should be installed. The following is an example of how we can do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n",
      "    install.packages(\"BiocManager\")\n",
      "\n",
      "BiocManager::install(\"BSgenome.Hsapiens.UCSC.hg38.masked\")"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/scripts/BSgenome_install.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rscript bedbase/tutorial_files/scripts/BSgenome_install.R > BSgenome.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create a directory where we can store the stats and plots generated by `bedstat`. Additionally, we'll create a directory where we can store log and metadata files that we'll need later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p outputs/bedstat_output/bedstat_pipeline_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use `bbconf`, we'll need to create a minimal configuration.yaml file. The path to this configuration file can be stored in the environment variable `$BEDBASE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:\n",
      "  pipeline_output_path: $BEDBASE_DATA_PATH_HOST/outputs\n",
      "  bedstat_dir: bedstat_output\n",
      "  bedbuncher_dir: bedbuncher_output\n",
      "  remote_url_base: null\n",
      "database:\n",
      "  host: $DB_HOST_URL\n",
      "  port: $POSTGRES_PORT\n",
      "  password: $POSTGRES_PASSWORD\n",
      "  user: $POSTGRES_USER\n",
      "  name: $POSTGRES_DB\n",
      "  dialect: postgresql\n",
      "  driver: psycopg2\n",
      "server:\n",
      "  host: 0.0.0.0\n",
      "  port: 8000\n",
      "remotes:\n",
      "  http:\n",
      "    prefix: http://data.bedbase.org/\n",
      "    description: HTTP compatible path\n",
      "  s3:\n",
      "    prefix: s3://data.bedbase.org/\n",
      "    description: S3 compatible path\n"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/bedbase_configuration_compose.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inititiate a local PostgreSQL instance\n",
    "\n",
    "In addition to generate statistics and plots, `bedstat` inserts JSON formatted metadata into relational [PostgreSQL] database. \n",
    "\n",
    "If you don't have docker installed, you can install it with `sudo apt-get update && apt-get install docker-engine -y`.\n",
    "\n",
    "Now, create a persistent volume to house PostgreSQL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres-data\n"
     ]
    }
   ],
   "source": [
    "docker volume create postgres-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spin up a `postgres` container. Provide required environment variables (need to match the settings in bedbase configuration file) and bind the created docker volume to `/var/lib/postgresql/data` path in the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find image 'postgres:latest' locally\n",
      "latest: Pulling from library/postgres\n",
      "\n",
      "\u001b[1Bb6b2107f: Pulling fs layer \n",
      "\u001b[1B51fa2b56: Pulling fs layer \n",
      "\u001b[1Bb6f96d81: Pulling fs layer \n",
      "\u001b[1Bac832fde: Pulling fs layer \n",
      "\u001b[1Bee1a3f12: Pulling fs layer \n",
      "\u001b[1B3c06319e: Pulling fs layer \n",
      "\u001b[1Ba72764d5: Pulling fs layer \n",
      "\u001b[1B2872ecae: Pulling fs layer \n",
      "\u001b[1Ba31f2e3d: Pulling fs layer \n",
      "\u001b[1B442835e0: Pulling fs layer \n",
      "\u001b[1B05af3390: Pulling fs layer \n",
      "\u001b[1B852bb872: Pulling fs layer \n",
      "\u001b[1B0be11543: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:8f7c3c9b61d82a4a021da5d9618faf056633e089302a726d619fa467c73609e4\n",
      "Status: Downloaded newer image for postgres:latest\n",
      "11bba276e7c48ccdd101d78aacffe85b19283611ebd91572fbd69da06086c698\n"
     ]
    }
   ],
   "source": [
    "docker run -d --name bedbase-postgres -p 5432:5432 -e POSTGRES_PASSWORD=bedbasepassword -e POSTGRES_USER=postgres -e POSTGRES_DB=postgres -v postgres-data:/var/lib/postgresql/data postgres:13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If environment variables are not initialized with function above, We have to initialize them manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "export DB_HOST_URL=localhost\n",
    "export POSTGRES_PORT=5432\n",
    "export POSTGRES_PASSWORD=bedbasepassword\n",
    "export POSTGRES_USER=postgres\n",
    "export POSTGRES_DB=postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bedstat  on the demo PEP\n",
    "\n",
    "In order to establish a modular connection between a project and a pipeline, we'll need to create a [pipeline interface](http://looper.databio.org/en/latest/linking-a-pipeline/) file, which tells looper how to run the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_name: BEDSTAT\n",
      "pipeline_type: sample\n",
      "path: pipeline/bedstat.py\n",
      "input_schema: http://schema.databio.org/pipelines/bedstat.yaml\n",
      "command_template: >\n",
      "  {pipeline.path}\n",
      "  --bedfile {sample.output_file_path}\n",
      "  --genome {sample.genome}\n",
      "  --sample-yaml {sample.yaml_file}\n",
      "  {% if sample.bedbase_config is defined %} --bedbase-config {sample.bedbase_config} {% endif %}\n",
      "  {% if sample.open_signal_matrix is defined %} --open-signal-matrix {sample.open_signal_matrix} {% endif %}\n"
     ]
    }
   ],
   "source": [
    "cat bedstat/pipeline_interface_new.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have properly linked our project to the pipeline of interest, in this case` bedstat`, we simply need to point the `looper run` command our `PEP` config file. Additionally, if the bedbase configuration file location is not stored in the `$BEDBASE` variable, we can pass it to `looper` as an additional argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looper version: 1.3.1\n",
      "Command: run\n",
      "/home/bnt4me/.local/lib/python3.8/site-packages/divvy/compute.py:150: UserWarning: The '_file_path' property is deprecated and will be removed in a future release. Use ComputingConfiguration[\"__internal\"][\"_file_path\"] instead.\n",
      "  os.path.dirname(self._file_path),\n",
      "/home/bnt4me/.local/lib/python3.8/site-packages/divvy/compute.py:58: UserWarning: The '_file_path' property is deprecated and will be removed in a future release. Use ComputingConfiguration[\"__internal\"][\"_file_path\"] instead.\n",
      "  self.config_file = self._file_path\n",
      "Activating compute package 'local'\n",
      "\u001b[36m## [1 of 11] sample: bedbase_demo_db1; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db1.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db1.sub\n",
      "\u001b[36m## [2 of 11] sample: bedbase_demo_db2; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db2.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db2.sub\n",
      "\u001b[36m## [3 of 11] sample: bedbase_demo_db3; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db3.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db3.sub\n",
      "\u001b[36m## [4 of 11] sample: bedbase_demo_db4; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db4.sub\n",
      "Job script (n=1; 0.01Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db4.sub\n",
      "\u001b[36m## [5 of 11] sample: bedbase_demo_db5; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db5.sub\n",
      "Job script (n=1; 0.01Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db5.sub\n",
      "\u001b[36m## [6 of 11] sample: bedbase_demo_db6; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db6.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db6.sub\n",
      "\u001b[36m## [7 of 11] sample: bedbase_demo_db7; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db7.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db7.sub\n",
      "\u001b[36m## [8 of 11] sample: bedbase_demo_db8; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db8.sub\n",
      "Job script (n=1; 0.01Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db8.sub\n",
      "\u001b[36m## [9 of 11] sample: bedhost_demo_db9; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedhost_demo_db9.sub\n",
      "Job script (n=1; 0.01Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedhost_demo_db9.sub\n",
      "\u001b[36m## [10 of 11] sample: bedbase_demo_db10; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db10.sub\n",
      "Job script (n=1; 0.01Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db10.sub\n",
      "\u001b[36m## [11 of 11] sample: bedbase_demo_db11; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db11.sub\n",
      "Job script (n=1; 0.01Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db11.sub\n",
      "\n",
      "Looper finished\n",
      "Samples valid for job generation: 11 of 11\n",
      "Commands submitted: 11 of 11\n",
      "Jobs submitted: 11\n"
     ]
    }
   ],
   "source": [
    "looper run bedbase/tutorial_files/PEPs/bedstat_config.yaml --package local \\\n",
    "--command-extra=\"-R\" > outputs/bedstat_output/bedstat_pipeline_logs/looper_logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for informative purposes, we can inspect how `bedstat` operates on each bedfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute node: cphg-Precision-5560\n",
      "Start time: 2021-12-06 12:17:46\n",
      "### Pipeline run code and environment:\n",
      "\n",
      "*              Command:  `/home/bnt4me/Virginia/bed_maker/bedbase_tutorial/bedstat/pipeline/bedstat.py --bedfile /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/bed_files/GSE105587_ENCFF018NNF_conservative_idr_thresholded_peaks_GRCh38.bed.gz --genome hg38 --sample-yaml /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/bedbase_demo_db1_sample.yaml --bedbase-config /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/bedbase/tutorial_files/bedbase_configuration_compose.yaml --open-signal-matrix /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/openSignalMatrix_hg38_percentile99_01_quantNormalized_round4d.txt.gz --bigbed /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/bigbed_files -R`\n",
      "*         Compute host:  cphg-Precision-5560\n",
      "*          Working dir:  /home/bnt4me/Virginia/bed_maker/bedbase_tutorial\n",
      "*            Outfolder:  /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedstat_output/78c0e4753d04b238fc07e4ebe5a02984/\n",
      "*  Pipeline started at:   (12-06 12:17:46) elapsed: 0.0 _TIME_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head outputs/bedstat_output/bedstat_pipeline_logs/looper_logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the previous steps have been executed, our bedfiles should be available for query on our local Elasticsearch cluster. Files can be queried using the `bedbuncher` pipeline described in the below section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BEDBUNCHER: Create bedsets and their respective statistics \n",
    "\n",
    "### Create a new PEP describing the bedset name and specific JSON query \n",
    "\n",
    "Now that we've processed several individual BED files, we'll turn to the next task: grouping them together into collections of BED files, which we call *bedsets*. For this, we use the `bedbuncher` pipeline, which produces outputs for each bedset, such as a bedset PEP, bedset-level statistics and plots, and an `IGD` database. To run `bedbuncher`, we will need another PEP describing each bedset. Though the annotation sheet below specifies attributes for one bedset, you can create as many as you wish using additional rows. For each bedset, you need to provide the query to retrieve certain collection BED files. \n",
    "\n",
    "The following example PEP shows the attributes we need to provide for each bedset and the config.yaml file that will grab the files needed to run `bedbuncher`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_name,bedset_name,genome,query,operator,query_val,bbconfig_name,bedbase_config\n",
      "sample1,bedsetOver1kRegions,hg38,'regions_no',gt,\"\"\"1000\"\"\",bedbase_configuration_compose,source1\n",
      "sample2,bedsetOver50GCContent,hg38,'gc_content',gt,\"\"\"0.5\"\"\",bedbase_configuration_compose,source1\n",
      "sample3,bedsetUnder500MeanWidth,hg38,'mean_region_width',lt,\"\"\"500\"\"\",bedbase_configuration_compose,source1\n",
      "sample4,bedsetTestSelectCellType,hg38,\"\"\"other::text~~:str_1 or other::text~~:str_2\"\"\",\"\"\"str_1,str_2\"\"\",\"\"\"%GM12878%,%HEK293%\"\"\",bedbase_configuration_compose,source1\n",
      "sample5,bedsetTestSelectGenome,hg38,\"\"\"name=:name_1 or name=:name_2\"\"\",\"\"\"name_1,name_2\"\"\",\"\"\"GSE105587_ENCFF018NNF_conservative_idr_thresholded_peaks_GRCh38,GSE91663_ENCFF553KIK_optimal_idr_thresholded_peaks_GRCh38\"\"\",bedbase_configuration_compose,source1\n",
      "sample6,bedsetTestCellType,hg38,\"\"\"other\"\"\",contains,\"\"\"\"\"{\\\"\"cell_type\\\"\":\\ \\\"\"K562\\\"\"}\"\"\"\"\",bedbase_configuration_compose,source1\n",
      "sample7,bedsetTestSpace,hg38,\"\"\"other\"\"\",contains,\"\"\"\"\"{\\\"\"description\\\"\":\\ \\\"\"IKZF1\\ ChIP-seq\\ on\\ human\\ GM12878\\\"\"}\"\"\"\"\",bedbase_configuration_compose,source1\n",
      "sample8,bedsetTestsSpaceMult,hg38,\"\"\"other::text~~:str_1 or other::text~~:str_2\"\"\",\"\"\"str_1,str_2\"\"\",\"\"\"%IKZF1 ChIP-seq on human GM12878%,%ZEB2 ChIP-seq on human K562 (ENCODE)%\"\"\",bedbase_configuration_compose,source1\n",
      "sample9,bedsetTestSpace2,hg38,\"\"\"other\"\"\",contains,\"\"\"\"\"{\\\"\"description\\\"\":\\ \\\"\"HEK293\\ cell\\ line\\ stably\\ expressing\\ N-terminal\\ tagged\\ eGFP-GLI2\\ under\\ the\\ control\\ of\\ a\\ CMV\\ promoter\\\"\"}\"\"\"\"\",bedbase_configuration_compose,source1\n",
      "sample10,bedsetTestsSpaceMult2,hg38,\"\"\"other::text~~:str_1 or other::text~~:str_2\"\"\",\"\"\"str_1,str_2\"\"\",\"\"\"%ZEB2 ChIP-seq on human K562 (ENCODE)%,%HEK293 cell line stably expressing N-terminal tagged eGFP-GLI2 under the control of a CMV promoter %\"\"\",bedbase_configuration_compose,source1\n"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/PEPs/bedbuncher_query.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pep_version: 2.0.0\n",
      "sample_table: bedbuncher_query.csv\n",
      "\n",
      "looper:\n",
      "    output_dir: $BEDBASE_DATA_PATH_HOST/outputs/bedbuncher_output/bedbuncher_pipeline_logs\n",
      "\n",
      "sample_modifiers:\n",
      "  append:\n",
      "    pipeline_interfaces: $CODE/bedbuncher/pipeline_interface.yaml \n",
      "  derive:\n",
      "    attributes: [bedbase_config]\n",
      "    sources:\n",
      "      source1: $CODE/bedbase/tutorial_files/{bbconfig_name}.yaml\n"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/PEPs/bedbuncher_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `bedbuncher` with arguments defined in the example PEP above will result in a bedset with bedfiles that consist of at least 1000 regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create outputs directory and install bedbuncher command line dependencies\n",
    "\n",
    "We need a folder where we can store bedset related outputs. Though not required, we'll also create a directory where we can store the `bedbuncher` pipeline logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p outputs/bedbuncher_output/bedbuncher_pipeline_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the feats of `bedbuncher` includes [IGD](https://github.com/databio/IGD) database creation from the files in the bedset. `IGD` can be installed by cloning the repository from github, executing the make file to create the binary, and pointing the binary location with the `$PATH` environment variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'IGD'...\n",
      "remote: Enumerating objects: 1297, done.\u001b[K\n",
      "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
      "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
      "remote: Total 1297 (delta 35), reused 40 (delta 17), pack-reused 1230\u001b[K\n",
      "Receiving objects: 100% (1297/1297), 949.45 KiB | 10.79 MiB/s, done.\n",
      "Resolving deltas: 100% (804/804), done.\n"
     ]
    }
   ],
   "source": [
    "git clone git@github.com:databio/IGD\n",
    "cd IGD\n",
    "make > igd_make_log.txt 2>&1\n",
    "cd ..\n",
    "\n",
    "export PATH=$BBTUTORIAL/IGD/bin/:$PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bedbuncher using Looper \n",
    "\n",
    "Once we have cloned the `bedbuncher` repository, set our local Postgres cluster and created the `iGD` binary, we can run the pipeline by pointing `looper run` to the appropriate `PEP` config file. As mentioned earlier, if the path to the bedbase configuration file has been stored in the `$BEDBASE` environment variable, it's not neccesary to pass the `--bedbase-config` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looper version: 1.3.1\n",
      "Command: run\n",
      "/home/bnt4me/.local/lib/python3.8/site-packages/divvy/compute.py:150: UserWarning: The '_file_path' property is deprecated and will be removed in a future release. Use ComputingConfiguration[\"__internal\"][\"_file_path\"] instead.\n",
      "  os.path.dirname(self._file_path),\n",
      "/home/bnt4me/.local/lib/python3.8/site-packages/divvy/compute.py:58: UserWarning: The '_file_path' property is deprecated and will be removed in a future release. Use ComputingConfiguration[\"__internal\"][\"_file_path\"] instead.\n",
      "  self.config_file = self._file_path\n",
      "Activating compute package 'local'\n",
      "\u001b[36m## [1 of 10] sample: sample1; pipeline: BEDBUNCHER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample1.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample1.sub\n",
      "\u001b[36m## [2 of 10] sample: sample2; pipeline: BEDBUNCHER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample2.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample2.sub\n",
      "\u001b[36m## [3 of 10] sample: sample3; pipeline: BEDBUNCHER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample3.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample3.sub\n",
      "\u001b[36m## [4 of 10] sample: sample4; pipeline: BEDBUNCHER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample4.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample4.sub\n",
      "\u001b[36m## [5 of 10] sample: sample5; pipeline: BEDBUNCHER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample5.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample5.sub\n",
      "\u001b[36m## [6 of 10] sample: sample6; pipeline: BEDBUNCHER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample6.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample6.sub\n",
      "\u001b[36m## [7 of 10] sample: sample7; pipeline: BEDBUNCHER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample7.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample7.sub\n",
      "\u001b[36m## [8 of 10] sample: sample8; pipeline: BEDBUNCHER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample8.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample8.sub\n",
      "\u001b[36m## [9 of 10] sample: sample9; pipeline: BEDBUNCHER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample9.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample9.sub\n",
      "\u001b[36m## [10 of 10] sample: sample10; pipeline: BEDBUNCHER\u001b[0m\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample10.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_sample10.sub\n",
      "\n",
      "Looper finished\n",
      "Samples valid for job generation: 10 of 10\n",
      "Commands submitted: 10 of 10\n",
      "Jobs submitted: 10\n"
     ]
    }
   ],
   "source": [
    "looper run  bedbase/tutorial_files/PEPs/bedbuncher_config.yaml  --package local \\\n",
    "--command-extra=\"-R\" > outputs/bedbuncher_output/bedbuncher_pipeline_logs/looper_logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BEDEMBED: \n",
    "\n",
    "### bedembed_train: Uses the StarSpace method to embed the bed files and the meta data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install [StarSpace](https://github.com/facebookresearch/StarSpace) first.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p bedembed/tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install [Boost](http://www.boost.org/) library and specify the path of boost library in makefile in order to run StarSpace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://boostorg.jfrog.io/artifactory/main/release/1.78.0/source/boost_1_78_0.zip\n",
    "unzip boost_1_78_0.zip\n",
    "sudo mv boost_1_78_0 /usr/local/bin\n",
    "cd /usr/local/bin/boost_1_78_0\n",
    "./bootstrap.sh\n",
    "./b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build StarSpace on Mac OS or Linux, use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $$BEDBASE_DATA_PATH_HOST/bedembed/tools\n",
    "git clone https://github.com/facebookresearch/Starspace.git\n",
    "cd Starspace\n",
    "makemake embed_doc\n",
    "make embed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a folder where we can store bedembed related outputs. Though not required, we'll also create a directory where we can store the bedembed pipeline logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p outputs/bedembed_output/bedembed_pipeline_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_starspace=$BEDBASE_DATA_PATH_HOST'/bedembed/tools/Starspace/starspace'\n",
    "path_meta=$BEDBASE_DATA_PATH_HOST'/bedbase/tutorial_files/PEPs/bedstat_annotation_sheet.csv'\n",
    "path_universe= $BEDBASE_DATA_PATH_HOST'/universe_tilelen1000.bed'\n",
    "path_output=$BEDBASE_DATA_PATH_HOST'/outputs/bedembed_output/'\n",
    "assembly='hg38'\n",
    "path_data=$BEDBASE_DATA_PATH_HOST'/bed_files/'\n",
    "label1=\"exp_protocol\"\n",
    "label2=\"cell_type\"\n",
    "no_files=10\n",
    "start_line=0\n",
    "dim=50\n",
    "epochs=20\n",
    "learning_rate=0.001\n",
    "\n",
    "python ./bedembed/pipeline/bedembed_train.py -star $path_starspace -i $path_data -g $assembly -meta $path_meta -univ $path_universe \\\n",
    "-l1 $label1 -l2 label2 -nof $no_files -o $path_output -startline $start_line -dim $dim -epochs $epochs -lr $learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bedembed_test: calculate the distances between file labels and trained search terms\n",
    "\n",
    "### Get a PEP describing the bedfiles to process \n",
    "\n",
    "We'll use the standard [PEP](http://pep.databio.org) format for the annotation, which consists of 1) a sample table (.csv) that annotates the files, and 2) a project config.yaml file that points to the sample annotation sheet. The config file also has other components, such as derived attributes, that in this case point to the bedfiles to be processed. Here is the PEP config file for this example project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedembed_version: 0.0.0\n",
      "sample_table: bedstat_annotation_sheet.csv\n",
      "\n",
      "looper:\n",
      "  output-dir: $BEDBASE_DATA_PATH_HOST/outputs/bedembed_output/bedembed_pipeline_logs \n",
      "sample_modifiers:\n",
      "  append:\n",
      "    bedbase_config: $BEDBASE_DATA_PATH_HOST/bedbase/tutorial_files/bedbase_configuration_compose.yaml\n",
      "    pipeline_interfaces: $BEDBASE_DATA_PATH_HOST/bedembed/pipeline_interface_test.yaml\n",
      "    universe: /project/shefflab/data/StarSpace/universe/universe_tilelen1000.bed\n",
      "    input_file_path: INPUT\n",
      "    output_file_path: $BEDBASE_DATA_PATH_HOST/outputs/bedembed_output\n",
      "    yaml_file: SAMPLE_YAML\n",
      "  derive:\n",
      "    attributes: [yaml_file, input_file_path]\n",
      "    sources:\n",
      "      INPUT: \"/project/shefflab/data/encode/{file_name}\"\n",
      "      SAMPLE_YAML: \"$BEDBASE_DATA_PATH_HOST/outputs/bedembed_output/bedembed_pipeline_logs/submission/{sample_name}_sample.yaml\"\n"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/PEPs/bedembed_test_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bedbuncher using Looper \n",
    "\n",
    "Once we have cloned the `bedembed` repository, set our local postgres cluster, we can run the pipeline by pointing `looper run` to the appropriate `PEP` config file. As mentioned earlier, if the path to the bedbase configuration file is provided, the calculated distances will report to the postgres database, if not it will save as a csv file in the `output_file_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "looper run bedbase/tutorial_files/PEPs/bedembed_test_config.yaml --package local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BEDHOST:  Serve BED files and API to explore pipeline outputs\n",
    "\n",
    "The last part of the tutorial consists on running a local instance of `bedhost` (a REST API for `bedstat` and `bedbuncher` produced outputs) in order to explore plots, statistics and download pipeline outputs. \n",
    "To run `bedhost`, frist use `bedhost-ui` to built the bedhost user interface with React."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd bedhost-ui\n",
    "# Install node modules defined in package.json\n",
    "npm install \n",
    "# Build the app for production to the ./build folder\n",
    "npm run build\n",
    "# copy the contents of the ./build directory to bedhost/bedhost/static/bedhost-ui\n",
    "cp -avr ./build ../bedhost/bedhost/static/bedhost-ui\n",
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run `bedhost`, we'll pip install the package from the previously cloned repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bedhost/. --user > bedhost_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start `bedhost`, we simply need to run the following command passing the location of the bedbase configuration file to the `-c` flag.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving data for columns: ['md5sum']\n",
      "Serving data for columns: ['md5sum']\n",
      "Generating GraphQL schema\n",
      "running bedhost app\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m648505\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47532 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47532 - \"\u001b[1mGET /ui/static/css/2.fa6c921b.chunk.css HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47534 - \"\u001b[1mGET /ui/static/css/main.4620a2c9.chunk.css HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47536 - \"\u001b[1mGET /ui/static/js/2.b0639060.chunk.js HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47534 - \"\u001b[1mGET /ui/static/js/main.56118e82.chunk.js HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47536 - \"\u001b[1mGET /api/bed/all/data/count HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "[(None,), ({'alias': 'hg38', 'digest': '2230c535660fb4774114bfa966a62f823fdb6d21acf138d4'},)]\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47532 - \"\u001b[1mGET /api/bed/genomes HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47534 - \"\u001b[1mGET /api/versions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47538 - \"\u001b[1mGET /ui/bedbase_logo.svg HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47538 - \"\u001b[1mGET /api/bedset/all/data/count HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "Serving data for columns: ['md5sum']\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47538 - \"\u001b[1mGET /api/bed/all/data?ids=md5sum&limit=1 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "Serving data for columns: ['md5sum']\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47538 - \"\u001b[1mGET /api/bedset/all/data?ids=md5sum&limt=1 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47538 - \"\u001b[1mGET /openapi.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47538 - \"\u001b[1mGET /ui/favicon.ico HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "bedhost serve -c  $BEDBASE_DATA_PATH_HOST/bedbase/tutorial_files/bedbase_configuration_compose.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have stored the path to the bedbase config in the environment variable `$BEDBASE` (suggested), it's not neccesary to use said flag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedhost serve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bedhost` API can be opened in the url [http://0.0.0.0:8000](http://0.0.0.0:8000). We can now explore the plots and statistics generated by the `bedstat` and `bedbuncher` pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## or optionally run BEDHOST using containers\n",
    "\n",
    "Alternatively, you can run the application inside a container.\n",
    "\n",
    "For that we'll use [docker compose](https://docs.docker.com/compose/), a tool that makes running multi-contaier Docker applications possible. The `docker-compose.yaml` file defines two services: \n",
    "- `fastapi-api`: runs the fastAPI server \n",
    "- `postgres-db`: runs the PostgeSQL database used by the server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $BEDBASE_DATA_PATH_HOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `BEDBASE_DATA_PATH_HOST` environment variable to point to the host directory with the pipeline results that will be mounted in the container as a volume. \n",
    "\n",
    "The environment variables are passed to the container via `.env` file, which the `docker-compose.yaml` points to for each service. Additionally, you can just export the environment variables before issuing the `docker-compose` command.\n",
    "When you set the same environment variable in multiple files, here’s the priority used by Compose to choose which value to use:\n",
    "\n",
    "1. Compose file\n",
    "2. Shell environment variables\n",
    "3. Environment file\n",
    "4. Dockerfile\n",
    "4. Variable is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling postgres-db (postgres:)...\n",
      "latest: Pulling from library/postgres\n",
      "Digest: sha256:8f7c3c9b61d82a4a021da5d9618faf056633e089302a726d619fa467c73609e4\n",
      "Status: Downloaded newer image for postgres:latest\n",
      "Recreating postgreSQL-bedbase ... \n",
      "\u001b[1BRecreating fastAPI-bedbase    ... mdone\u001b[0m\n",
      "\u001b[1BAttaching to postgreSQL-bedbase, fastAPI-bedbase\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m \n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m PostgreSQL Database directory appears to contain a database; Skipping initialization\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m \n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m 2020-11-02 23:10:28.883 UTC [1] LOG:  starting PostgreSQL 13.0 (Debian 13.0-1.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m 2020-11-02 23:10:28.885 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m 2020-11-02 23:10:28.885 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m 2020-11-02 23:10:28.891 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m 2020-11-02 23:10:28.901 UTC [25] LOG:  database system was shut down at 2020-11-02 23:03:14 UTC\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m 2020-11-02 23:10:28.909 UTC [1] LOG:  database system is ready to accept connections\n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m wait-for-it.sh: waiting 60 seconds for postgres-db:5432\n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m wait-for-it.sh: postgres-db:5432 is available after 0 seconds\n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m DEBU 2020-11-02 23:10:30,246 | bedhost:est:265 > Configured logger 'bedhost' using logmuse v0.2.6 \n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m DEBU 23:10:30 | bbconf:est:265 > Configured logger 'bbconf' using logmuse v0.2.6 \n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m DEBU 23:10:30 | bbconf:bbconf:105 > Established connection with PostgreSQL: postgres-db \n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m DEBU 2020-11-02 23:10:30,299 | bedhost:main:503 > Determined React UI path: /app/bedhost/static/bedhost-ui \n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m INFO 2020-11-02 23:10:30,299 | bedhost:main:510 > running bedhost app \n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m INFO:     Started server process [1]\n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m INFO:     Waiting for application startup.\n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m INFO:     Application startup complete.\n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "Stopping fastAPI-bedbase      ... \n",
      "Stopping postgreSQL-bedbase   ... \n",
      "\u001b[1Bping postgreSQL-bedbase   ... \u001b[32mdone\u001b[0m"
     ]
    }
   ],
   "source": [
    "cd bedhost; docker-compose up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "329.797px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
