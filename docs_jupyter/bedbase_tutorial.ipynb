{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEDBASE workflow tutorial\n",
    "\n",
    "This demo demonstrates how to process, analyze, visualize, and serve BED files. The process has 3 steps: First, individual BED files are analyzed using the [bedstat](https://github.com/databio/bedstat) pipeline. Second, BED files are grouped and then analyzed as groups using the [bedbuncher](https://github.com/databio/bedbuncher) pipeline. Finally, the BED files, along with statistics, plots, and grouping information, is served via a web interface and RESTful API using the [bedhost](https://github.com/databio/bedhost/tree/master) package.\n",
    "\n",
    "**Glossary of terms:**\n",
    "\n",
    "- *bedfile*: a tab-delimited file with one genomic region per line. Each genomic region is decribed by 3 required columns: chrom, start and end.\n",
    "- *bedset*: a collection of BED files grouped by with a shared biological, experimental, or logical criterion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Preparation\" data-toc-modified-id=\"1.-Preparation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Preparation</a></span></li><li><span><a href=\"#2.-BEDSTAT:-Generate-statistics-and-plots-of-BED-files\" data-toc-modified-id=\"2.-BEDSTAT:-Generate-statistics-and-plots-of-BED-files-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. BEDSTAT: Generate statistics and plots of BED files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-a-PEP-describing-the-bedfiles-to-process\" data-toc-modified-id=\"Get-a-PEP-describing-the-bedfiles-to-process-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Get a PEP describing the bedfiles to process</a></span></li><li><span><a href=\"#Install-bedstat-dependencies\" data-toc-modified-id=\"Install-bedstat-dependencies-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Install bedstat dependencies</a></span></li><li><span><a href=\"#Inititiate-a-local-elasticsearch-cluster\" data-toc-modified-id=\"Inititiate-a-local-elasticsearch-cluster-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Inititiate a local elasticsearch cluster</a></span></li><li><span><a href=\"#Run-bedstat--on-the-demo-PEP\" data-toc-modified-id=\"Run-bedstat--on-the-demo-PEP-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Run bedstat  on the demo PEP</a></span></li></ul></li><li><span><a href=\"#3.-BEDBUNCHER:-Create-bedsets-and-their-respective-statistics\" data-toc-modified-id=\"3.-BEDBUNCHER:-Create-bedsets-and-their-respective-statistics-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. BEDBUNCHER: Create bedsets and their respective statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-a-new-PEP-describing-the-bedset-name-and-specific-JSON-query\" data-toc-modified-id=\"Create-a-new-PEP-describing-the-bedset-name-and-specific-JSON-query-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create a new PEP describing the bedset name and specific JSON query</a></span></li><li><span><a href=\"#Create-outputs-directory-and-install-bedbuncher-CML-dependencies\" data-toc-modified-id=\"Create-outputs-directory-and-install-bedbuncher-CML-dependencies-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Create outputs directory and install bedbuncher CML dependencies</a></span></li><li><span><a href=\"#Run-bedbuncher-using-Looper\" data-toc-modified-id=\"Run-bedbuncher-using-Looper-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Run bedbuncher using Looper</a></span></li></ul></li><li><span><a href=\"#4.-BEDHOST:--Serve-BED-files-and-API-to-explore-pipeline-outputs\" data-toc-modified-id=\"4.-BEDHOST:--Serve-BED-files-and-API-to-explore-pipeline-outputs-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. BEDHOST:  Serve BED files and API to explore pipeline outputs</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation \n",
    "\n",
    "First, we will create a tutorial directory where we'll store the bedbase pipelines and files to be processed. We'll also need to create an environment variable that points to the tutorial directory (we'll need this variable later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir bedbase_tutorial\n",
    "cd bedbase_tutorial\n",
    "export BBTUTORIAL=`pwd`\n",
    "export BEDBASE_DATA_PATH_HOST=`pwd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some example BED files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-08 14:00:26--  http://big.databio.org/example_data/bedbase_tutorial/bed_files.tar.gz\n",
      "Resolving big.databio.org (big.databio.org)... 128.143.245.182, 128.143.245.181\n",
      "Connecting to big.databio.org (big.databio.org)|128.143.245.182|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44549692 (42M) [application/octet-stream]\n",
      "Saving to: ‘bed_files.tar.gz’\n",
      "\n",
      "bed_files.tar.gz    100%[===================>]  42.49M  6.60MB/s    in 11s     \n",
      "\n",
      "2021-11-08 14:00:38 (3.77 MB/s) - ‘bed_files.tar.gz’ saved [44549692/44549692]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget http://big.databio.org/example_data/bedbase_tutorial/bed_files.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded files are compressed so we'll need to untar them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bed_files/\n",
      "bed_files/GSE105587_ENCFF018NNF_conservative_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bed_files/GSM2423312_ENCFF155HVK_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE105977_ENCFF617QGK_optimal_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE91663_ENCFF316ASR_peaks_GRCh38.bed.gz\n",
      "bed_files/GSM2423313_ENCFF722AOG_peaks_GRCh38.bed.gz\n",
      "bed_files/GSM2827349_ENCFF196DNQ_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE91663_ENCFF553KIK_optimal_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE91663_ENCFF319TPR_conservative_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE105977_ENCFF937CGY_peaks_GRCh38.bed.gz\n",
      "bed_files/GSM2827350_ENCFF928JXU_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE105977_ENCFF793SZW_conservative_idr_thresholded_peaks_GRCh38.bed.gz\n"
     ]
    }
   ],
   "source": [
    "tar -zxvf bed_files.tar.gz\n",
    "rm -rf bed_files.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we'll download a matrix we need to provide if we wish to plot the tissue specificity of our set of genomic ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-08 14:00:53--  http://big.databio.org/open_chromatin_matrix/openSignalMatrix_hg38_percentile99_01_quantNormalized_round4d.txt.gz\n",
      "Resolving big.databio.org (big.databio.org)... 128.143.245.181, 128.143.245.182\n",
      "Connecting to big.databio.org (big.databio.org)|128.143.245.181|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 380989260 (363M) [application/octet-stream]\n",
      "Saving to: ‘openSignalMatrix_hg38_percentile99_01_quantNormalized_round4d.txt.gz’\n",
      "\n",
      "openSignalMatrix_hg 100%[===================>] 363.34M  4.34MB/s    in 87s     \n",
      "\n",
      "2021-11-08 14:02:20 (4.20 MB/s) - ‘openSignalMatrix_hg38_percentile99_01_quantNormalized_round4d.txt.gz’ saved [380989260/380989260]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget http://big.databio.org/open_chromatin_matrix/openSignalMatrix_hg38_percentile99_01_quantNormalized_round4d.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll download the core pipelines and tools needed to complete this tutorial: `bedstat`, `bedbuncher` , `bedhost`, and `bedhost-ui`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bedbase'...\n",
      "remote: Enumerating objects: 342, done.\u001b[K\n",
      "remote: Counting objects: 100% (342/342), done.\u001b[K\n",
      "remote: Compressing objects: 100% (220/220), done.\u001b[K\n",
      "remote: Total 342 (delta 158), reused 246 (delta 84), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (342/342), 521.47 KiB | 3.95 MiB/s, done.\n",
      "Resolving deltas: 100% (158/158), done.\n",
      "Cloning into 'bedstat'...\n",
      "remote: Enumerating objects: 750, done.\u001b[K\n",
      "remote: Counting objects: 100% (260/260), done.\u001b[K\n",
      "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
      "remote: Total 750 (delta 120), reused 221 (delta 98), pack-reused 490\u001b[K\n",
      "Receiving objects: 100% (750/750), 165.38 KiB | 2.71 MiB/s, done.\n",
      "Resolving deltas: 100% (351/351), done.\n",
      "Cloning into 'bedbuncher'...\n",
      "remote: Enumerating objects: 611, done.\u001b[K\n",
      "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
      "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
      "remote: Total 611 (delta 66), reused 105 (delta 37), pack-reused 468\u001b[K\n",
      "Receiving objects: 100% (611/611), 113.48 KiB | 2.84 MiB/s, done.\n",
      "Resolving deltas: 100% (319/319), done.\n",
      "Cloning into 'bedhost'...\n",
      "remote: Enumerating objects: 1920, done.\u001b[K\n",
      "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
      "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
      "remote: Total 1920 (delta 49), reused 37 (delta 14), pack-reused 1811\u001b[K\n",
      "Receiving objects: 100% (1920/1920), 453.51 KiB | 3.06 MiB/s, done.\n",
      "Resolving deltas: 100% (1181/1181), done.\n",
      "Cloning into 'bedhost-ui'...\n",
      "remote: Enumerating objects: 1462, done.\u001b[K\n",
      "remote: Counting objects: 100% (825/825), done.\u001b[K\n",
      "remote: Compressing objects: 100% (298/298), done.\u001b[K\n",
      "remote: Total 1462 (delta 595), reused 732 (delta 526), pack-reused 637\u001b[K\n",
      "Receiving objects: 100% (1462/1462), 761.58 KiB | 3.10 MiB/s, done.\n",
      "Resolving deltas: 100% (1092/1092), done.\n"
     ]
    }
   ],
   "source": [
    "git clone -b dev git@github.com:databio/bedbase\n",
    "git clone -b dev git@github.com:databio/bedstat\n",
    "git clone -b dev git@github.com:databio/bedbuncher\n",
    "git clone -b dev git@github.com:databio/bedhost\n",
    "git clone -b dev git@github.com:databio/bedhost-ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BEDSTAT: Generate statistics and plots of BED files \n",
    "\n",
    "### Get a PEP describing the bedfiles to process\n",
    "\n",
    "The first step is to process the BED files using the `bedstat` pipeline, which computes statistics and makes plots for each individual BED file. To begin, we'll need some annotation information for our BED files to load. We'll use the standard [PEP](https://pepkit.github.io/) format for the annotation, which consists of 1) a sample table (.csv) that annotates the files, and 2) a project config.yaml file that points to the sample annotation sheet. The config file also has other components, such as derived attributes, that in this case point to the bedfiles to be processed. Here is the PEP config file for this example project. It includes annotation information for each BED file, and also points to the `.bed.gz` files using derived attributes `output_file_path` and `yaml_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pep_version: 2.0.0\n",
      "sample_table: bedstat_annotation_sheet.csv\n",
      "\n",
      "looper:\n",
      "  output-dir: $BBTUTORIAL/outputs/bedstat_output/bedstat_pipeline_logs \n",
      "\n",
      "sample_modifiers:\n",
      "  append:\n",
      "    bedbase_config: $BBTUTORIAL/bedbase/tutorial_files/bedbase_configuration.yaml\n",
      "    pipeline_interfaces: $BBTUTORIAL/bedstat/pipeline_interface.yaml\n",
      "    output_file_path: OUTPUT\n",
      "    yaml_file: SAMPLE_YAML\n",
      "    open_signal_matrix: MATRIX\n",
      "  derive:\n",
      "    attributes: [output_file_path, yaml_file, open_signal_matrix]\n",
      "    sources:\n",
      "      OUTPUT: \"$BBTUTORIAL/bed_files/{file_name}\" \n",
      "      SAMPLE_YAML: \"$BBTUTORIAL/outputs/bedstat_output/bedstat_pipeline_logs/submission/{sample_name}_sample.yaml\"\n",
      "      MATRIX: \"$BBTUTORIAL/openSignalMatrix_{genome}_percentile99_01_quantNormalized_round4d.txt.gz\"\n"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/PEPs/bedstat_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install bedstat dependencies\n",
    "\n",
    "`bedstat` is a [pypiper](http://code.databio.org/pypiper/) pipeline that generates statistics and plots of bedfiles. Additionally, `bedstat` uses [bbconf](https://github.com/databio/bbconf), the bedbase configuration manager which implements convenience methods for interacting with an Elasticsearch database, where our file metadata will be placed. These and the appropriate R dependencies can be installed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r bedstat/requirements/requirements-all.txt  --user > requirements_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install R dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required package: R.utils\n",
      "Loading required package: R.oo\n",
      "Loading required package: R.methodsS3\n",
      "R.methodsS3 v1.8.1 (2020-08-26 16:20:06 UTC) successfully loaded. See ?R.methodsS3 for help.\n",
      "R.oo v1.24.0 (2020-08-26 16:11:58 UTC) successfully loaded. See ?R.oo for help.\n",
      "\n",
      "Attaching package: ‘R.oo’\n",
      "\n",
      "The following object is masked from ‘package:R.methodsS3’:\n",
      "\n",
      "    throw\n",
      "\n",
      "The following objects are masked from ‘package:methods’:\n",
      "\n",
      "    getClasses, getMethods\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    attach, detach, load, save\n",
      "\n",
      "R.utils v2.11.0 (2021-09-26 08:30:02 UTC) successfully loaded. See ?R.utils for help.\n",
      "\n",
      "Attaching package: ‘R.utils’\n",
      "\n",
      "The following object is masked from ‘package:utils’:\n",
      "\n",
      "    timestamp\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    cat, commandArgs, getOption, inherits, isOpen, nullfile, parse,\n",
      "    warnings\n",
      "\n",
      "Loading required package: BiocManager\n",
      "Bioconductor version '3.13' is out-of-date; the current release version '3.14'\n",
      "  is available with R version '4.1'; see https://bioconductor.org/install\n",
      "Loading required package: optparse\n",
      "Loading required package: devtools\n",
      "Loading required package: usethis\n",
      "\n",
      "Attaching package: ‘devtools’\n",
      "\n",
      "The following object is masked from ‘package:BiocManager’:\n",
      "\n",
      "    install\n",
      "\n",
      "The following objects are masked from ‘package:R.oo’:\n",
      "\n",
      "    check, unload\n",
      "\n",
      "Loading required package: GenomicRanges\n",
      "Loading required package: stats4\n",
      "Loading required package: BiocGenerics\n",
      "Loading required package: parallel\n",
      "\n",
      "Attaching package: ‘BiocGenerics’\n",
      "\n",
      "The following objects are masked from ‘package:parallel’:\n",
      "\n",
      "    clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,\n",
      "    clusterExport, clusterMap, parApply, parCapply, parLapply,\n",
      "    parLapplyLB, parRapply, parSapply, parSapplyLB\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    IQR, mad, sd, var, xtabs\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    anyDuplicated, append, as.data.frame, basename, cbind, colnames,\n",
      "    dirname, do.call, duplicated, eval, evalq, Filter, Find, get, grep,\n",
      "    grepl, intersect, is.unsorted, lapply, Map, mapply, match, mget,\n",
      "    order, paste, pmax, pmax.int, pmin, pmin.int, Position, rank,\n",
      "    rbind, Reduce, rownames, sapply, setdiff, sort, table, tapply,\n",
      "    union, unique, unsplit, which.max, which.min\n",
      "\n",
      "Loading required package: S4Vectors\n",
      "\n",
      "Attaching package: ‘S4Vectors’\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    expand.grid, I, unname\n",
      "\n",
      "Loading required package: IRanges\n",
      "\n",
      "Attaching package: ‘IRanges’\n",
      "\n",
      "The following object is masked from ‘package:R.oo’:\n",
      "\n",
      "    trim\n",
      "\n",
      "Loading required package: GenomeInfoDb\n",
      "Loading required package: GenomicFeatures\n",
      "Loading required package: AnnotationDbi\n",
      "Loading required package: Biobase\n",
      "Welcome to Bioconductor\n",
      "\n",
      "    Vignettes contain introductory material; view with\n",
      "    'browseVignettes()'. To cite Bioconductor, see\n",
      "    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n",
      "\n",
      "Loading required package: ensembldb\n",
      "Loading required package: AnnotationFilter\n",
      "\n",
      "Attaching package: 'ensembldb'\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    filter\n",
      "\n",
      "Loading required package: LOLA\n",
      "Loading required package: BSgenome\n",
      "Loading required package: Biostrings\n",
      "Loading required package: XVector\n",
      "\n",
      "Attaching package: 'Biostrings'\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    strsplit\n",
      "\n",
      "Loading required package: rtracklayer\n",
      "Loading required package: GenomicDistributions\n",
      "Loading required package: GenomicDistributionsData\n",
      "snapshotDate(): 2021-05-18\n"
     ]
    }
   ],
   "source": [
    "Rscript bedstat/scripts/installRdeps.R > R_deps.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an additional dependency needed by `bedstat` if we wish to calculate and plot the GC content of our bedfiles. Depending on the genome assemblies of the files listed on a PEP, the appropriate BSgenome packages should be installed. The following is an example of how we can do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n",
      "    install.packages(\"BiocManager\")\n",
      "\n",
      "BiocManager::install(\"BSgenome.Hsapiens.UCSC.hg38.masked\")"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/scripts/BSgenome_install.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n",
      "'?repositories' for details\n",
      "\n",
      "replacement repositories:\n",
      "    CRAN: https://cloud.r-project.org\n",
      "\n",
      "Bioconductor version 3.13 (BiocManager 1.30.16), R 4.1.1 (2021-08-10)\n",
      "Installation paths not writeable, unable to update packages\n",
      "  path: /usr/lib/R/library\n",
      "  packages:\n",
      "    nlme, spatial\n",
      "Old packages: 'backports', 'broom', 'cli', 'cpp11', 'crayon', 'crosstalk',\n",
      "  'generics', 'gert', 'glue', 'labelled', 'rlang', 'sessioninfo', 'snow',\n",
      "  'tibble', 'tinytex', 'tzdb', 'usethis', 'uuid', 'xfun'\n",
      "Warning message:\n",
      "package(s) not installed when version(s) same as current; use `force = TRUE` to\n",
      "  re-install: 'BSgenome.Hsapiens.UCSC.hg38.masked' \n"
     ]
    }
   ],
   "source": [
    "Rscript bedbase/tutorial_files/scripts/BSgenome_install.R > BSgenome.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create a directory where we can store the stats and plots generated by `bedstat`. Additionally, we'll create a directory where we can store log and metadata files that we'll need later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p outputs/bedstat_output/bedstat_pipeline_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use `bbconf`, we'll need to create a minimal configuration.yaml file. The path to this configuration file can be stored in the environment variable `$BEDBASE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:\n",
      "  pipeline_output_path: $BEDBASE_DATA_PATH_HOST/outputs\n",
      "  bedstat_dir: bedstat_output\n",
      "  bedbuncher_dir: bedbuncher_output\n",
      "  remote_url_base: null\n",
      "database:\n",
      "  host: $DB_HOST_URL\n",
      "  port: $POSTGRES_PORT\n",
      "  password: $POSTGRES_PASSWORD\n",
      "  user: $POSTGRES_USER\n",
      "  name: $POSTGRES_DB\n",
      "  dialect: postgresql\n",
      "  driver: psycopg2\n",
      "server:\n",
      "  host: 0.0.0.0\n",
      "  port: 8000\n"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/bedbase_configuration_compose.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inititiate a local PostgreSQL instance\n",
    "\n",
    "In addition to generate statistics and plots, `bedstat` inserts JSON formatted metadata into relational [PostgreSQL] database. \n",
    "\n",
    "If you don't have docker installed, you can install it with `sudo apt-get update && apt-get install docker-engine -y`.\n",
    "\n",
    "Now, create a persistent volume to house PostgreSQL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres-data\n"
     ]
    }
   ],
   "source": [
    "docker volume create postgres-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spin up a `postgres` container. Provide required environment variables (need to match the settings in bedbase configuration file) and bind the created docker volume to `/var/lib/postgresql/data` path in the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3c8d5d0bf8e1d248d6ac668907ad50c1434df330d629b250f91112b05a80a25c\n"
     ]
    }
   ],
   "source": [
    "docker run -d --name bedbase-postgres -p 5432:5432 -e POSTGRES_PASSWORD=bedbasepassword -e POSTGRES_USER=postgres -e POSTGRES_DB=postgres -v postgres-data:/var/lib/postgresql/data postgres:13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above command might add environment variables, if it didn't happen run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "export DB_HOST_URL='localhost'\n",
    "export POSTGRES_PORT=5432\n",
    "export POSTGRES_PASSWORD='bedbasepassword'\n",
    "export POSTGRES_USER='postgres'\n",
    "export POSTGRES_DB='postgres'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bedstat  on the demo PEP\n",
    "To run `bedstat` and the other required pipelines in this tutorial, we will rely on the pipeline submission engine [looper](http://looper.databio.org/en/latest/), which can be installed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: looper in /home/bnt4me/.local/lib/python3.8/site-packages (1.3.1)\n",
      "Requirement already satisfied: ubiquerg>=0.5.2 in /home/bnt4me/.local/lib/python3.8/site-packages (from looper) (0.6.1)\n",
      "Requirement already satisfied: attmap>=0.12.7 in /home/bnt4me/.local/lib/python3.8/site-packages (from looper) (0.13.0)\n",
      "Requirement already satisfied: eido>=0.1.3 in /home/bnt4me/.local/lib/python3.8/site-packages (from looper) (0.1.5)\n",
      "Requirement already satisfied: pyyaml>=3.12 in /usr/lib/python3/dist-packages (from looper) (5.3.1)\n",
      "Requirement already satisfied: divvy>=0.5.0 in /home/bnt4me/.local/lib/python3.8/site-packages (from looper) (0.5.0)\n",
      "Requirement already satisfied: peppy>=0.31.0 in /home/bnt4me/.local/lib/python3.8/site-packages (from looper) (0.31.1)\n",
      "Requirement already satisfied: jinja2 in /home/bnt4me/.local/lib/python3.8/site-packages (from looper) (3.0.2)\n",
      "Requirement already satisfied: colorama>=0.3.9 in /usr/lib/python3/dist-packages (from looper) (0.4.3)\n",
      "Requirement already satisfied: pandas>=0.20.2 in /home/bnt4me/.local/lib/python3.8/site-packages (from looper) (1.3.4)\n",
      "Requirement already satisfied: logmuse>=0.2.0 in /home/bnt4me/.local/lib/python3.8/site-packages (from looper) (0.2.7)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /home/bnt4me/.local/lib/python3.8/site-packages (from eido>=0.1.3->looper) (4.1.0)\n",
      "Requirement already satisfied: yacman>=0.5.0 in /home/bnt4me/.local/lib/python3.8/site-packages (from divvy>=0.5.0->looper) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bnt4me/.local/lib/python3.8/site-packages (from jinja2->looper) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/bnt4me/.local/lib/python3.8/site-packages (from pandas>=0.20.2->looper) (1.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas>=0.20.2->looper) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas>=0.20.2->looper) (2019.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/bnt4me/.local/lib/python3.8/site-packages (from jsonschema>=3.0.1->eido>=0.1.3->looper) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/bnt4me/.local/lib/python3.8/site-packages (from jsonschema>=3.0.1->eido>=0.1.3->looper) (0.18.0)\n",
      "Requirement already satisfied: oyaml in /home/bnt4me/.local/lib/python3.8/site-packages (from yacman>=0.5.0->divvy>=0.5.0->looper) (1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install looper --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to establish a modular connection between a project and a pipeline, we'll need to create a [pipeline interface](http://looper.databio.org/en/latest/linking-a-pipeline/) file, which tells looper how to run the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_name: BEDSTAT\n",
      "pipeline_type: sample\n",
      "var_templates:\n",
      "  path: \"{looper.piface_dir}/pipeline/bedstat.py\"\n",
      "input_schema: http://schema.databio.org/pipelines/bedstat.yaml\n",
      "pre_submit:\n",
      "  python_functions:\n",
      "    - looper.write_sample_yaml\n",
      "command_template: >\n",
      "  {pipeline.var_templates.path}\n",
      "  --bedfile {sample.output_file_path}\n",
      "  --genome {sample.genome}\n",
      "  --sample-yaml {sample.yaml_file}\n",
      "  {% if sample.bedbase_config is defined %} --bedbase-config {sample.bedbase_config} {% endif %}\n",
      "  {% if sample.open_signal_matrix is defined %} --open-signal-matrix {sample.open_signal_matrix} {% endif %}\n",
      "  {% if sample.bigbed is defined %} --bigbed {sample.bigbed} {% endif %}\n"
     ]
    }
   ],
   "source": [
    "cat bedstat/pipeline_interface.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have properly linked our project to the pipeline of interest, in this case` bedstat`, we simply need to point the `looper run` command our `PEP` config file. Additionally, if the bedbase configuration file location is not stored in the `$BEDBASE` variable, we can pass it to `looper` as an additional argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looper version: 1.3.1\n",
      "Command: run\n",
      "/home/bnt4me/.local/lib/python3.8/site-packages/divvy/compute.py:150: UserWarning: The '_file_path' property is deprecated and will be removed in a future release. Use ComputingConfiguration[\"__internal\"][\"_file_path\"] instead.\n",
      "  os.path.dirname(self._file_path),\n",
      "/home/bnt4me/.local/lib/python3.8/site-packages/divvy/compute.py:58: UserWarning: The '_file_path' property is deprecated and will be removed in a future release. Use ComputingConfiguration[\"__internal\"][\"_file_path\"] instead.\n",
      "  self.config_file = self._file_path\n",
      "Activating compute package 'local'\n",
      "\u001b[36m## [1 of 11] sample: bedbase_demo_db1; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db1.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db1.sub\n",
      "\u001b[36m## [2 of 11] sample: bedbase_demo_db2; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db2.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db2.sub\n",
      "\u001b[36m## [3 of 11] sample: bedbase_demo_db3; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db3.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db3.sub\n",
      "\u001b[36m## [4 of 11] sample: bedbase_demo_db4; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db4.sub\n",
      "Job script (n=1; 0.01Gb): /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db4.sub\n",
      "\u001b[36m## [5 of 11] sample: bedbase_demo_db5; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db5.sub\n",
      "Job script (n=1; 0.01Gb): /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db5.sub\n",
      "\u001b[36m## [6 of 11] sample: bedbase_demo_db6; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db6.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db6.sub\n",
      "\u001b[36m## [7 of 11] sample: bedbase_demo_db7; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db7.sub\n",
      "Job script (n=1; 0.00Gb): /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db7.sub\n",
      "\u001b[36m## [8 of 11] sample: bedbase_demo_db8; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db8.sub\n",
      "Job script (n=1; 0.01Gb): /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db8.sub\n",
      "\u001b[36m## [9 of 11] sample: bedhost_demo_db9; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedhost_demo_db9.sub\n",
      "Job script (n=1; 0.01Gb): /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedhost_demo_db9.sub\n",
      "\u001b[36m## [10 of 11] sample: bedbase_demo_db10; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db10.sub\n",
      "Job script (n=1; 0.01Gb): /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db10.sub\n",
      "\u001b[36m## [11 of 11] sample: bedbase_demo_db11; pipeline: BEDSTAT\u001b[0m\n",
      "Calling pre-submit function: looper.write_sample_yaml\n",
      "Writing script to /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db11.sub\n",
      "Job script (n=1; 0.01Gb): /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db11.sub\n",
      "\n",
      "Looper finished\n",
      "Samples valid for job generation: 11 of 11\n",
      "Commands submitted: 11 of 11\n",
      "Jobs submitted: 11\n"
     ]
    }
   ],
   "source": [
    "looper run bedbase/tutorial_files/PEPs/bedstat_config.yaml --package local \\\n",
    "--command-extra=\"-R\" > outputs/bedstat_output/bedstat_pipeline_logs/looper_logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if there is error: \"ImportError: cannot import name 'get_logger' from 'peppy.utils'\": reinstall looper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for informative purposes, we can inspect how `bedstat` operates on each bedfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute node: cphg-Precision-5560\n",
      "Start time: 2021-11-08 14:15:14\n",
      "### Pipeline run code and environment:\n",
      "\n",
      "*              Command:  `/home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/bedstat/pipeline/bedstat.py --bedfile /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/bed_files/GSE105587_ENCFF018NNF_conservative_idr_thresholded_peaks_GRCh38.bed.gz --genome hg38 --sample-yaml /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/bedbase_demo_db1_sample.yaml --bedbase-config /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/bedbase/tutorial_files/bedbase_configuration_compose.yaml --open-signal-matrix /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/openSignalMatrix_hg38_percentile99_01_quantNormalized_round4d.txt.gz --bigbed /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/bigbed_files -R`\n",
      "*         Compute host:  cphg-Precision-5560\n",
      "*          Working dir:  /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial\n",
      "*            Outfolder:  /home/bnt4me/Virginia/bed_maker_new/bedbase_tutorial/outputs/bedstat_output/78c0e4753d04b238fc07e4ebe5a02984/\n",
      "*  Pipeline started at:   (11-08 14:15:14) elapsed: 0.0 _TIME_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head outputs/bedstat_output/bedstat_pipeline_logs/looper_logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the previous steps have been executed, our bedfiles should be available for query on our local Elasticsearch cluster. Files can be queried using the `bedbuncher` pipeline described in the below section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BEDBUNCHER: Create bedsets and their respective statistics \n",
    "\n",
    "### Create a new PEP describing the bedset name and specific JSON query \n",
    "\n",
    "Now that we've processed several individual BED files, we'll turn to the next task: grouping them together into collections of BED files, which we call *bedsets*. For this, we use the `bedbuncher` pipeline, which produces outputs for each bedset, such as a bedset PEP, bedset-level statistics and plots, and an `IGD` database. To run `bedbuncher`, we will need another PEP describing each bedset. Though the annotation sheet below specifies attributes for one bedset, you can create as many as you wish using additional rows. For each bedset, you need to provide the query to retrieve certain collection BED files. \n",
    "\n",
    "The following example PEP shows the attributes we need to provide for each bedset and the config.yaml file that will grab the files needed to run `bedbuncher`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_name,bedset_name,query,bbconfig_name,bedbase_config\n",
      "sample1,bedsetOver1kRegions,'regions_no>1000',bedbase_configuration,source1\n",
      "sample2,bedsetOver50GCContent,'gc_content>0.5',bedbase_configuration,source1\n",
      "sample3,bedsetUnder500MeanWidth,'mean_region_width<500',bedbase_configuration,source1"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/PEPs/bedbuncher_query.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pep_version: 2.0.0\n",
      "sample_table: bedbuncher_query.csv\n",
      "\n",
      "looper:\n",
      "    output_dir: $BBTUTORIAL/outputs/bedbuncher_output/bedbuncher_pipeline_logs\n",
      "\n",
      "sample_modifiers:\n",
      "  append:\n",
      "    pipeline_interfaces: $BBTUTORIAL/bedbuncher/pipeline_interface.yaml \n",
      "  derive:\n",
      "    attributes: [bedbase_config]\n",
      "    sources:\n",
      "      source1: $BBTUTORIAL/bedbase/tutorial_files/{bbconfig_name}.yaml\n"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/PEPs/bedbuncher_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `bedbuncher` with arguments defined in the example PEP above will result in a bedset with bedfiles that consist of at least 1000 regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create outputs directory and install bedbuncher command line dependencies\n",
    "\n",
    "We need a folder where we can store bedset related outputs. Though not required, we'll also create a directory where we can store the `bedbuncher` pipeline logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p outputs/bedbuncher_output/bedbuncher_pipeline_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the feats of `bedbuncher` includes [IGD](https://github.com/databio/IGD) database creation from the files in the bedset. `IGD` can be installed by cloning the repository from github, executing the make file to create the binary, and pointing the binary location with the `$PATH` environment variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'IGD'...\n"
     ]
    }
   ],
   "source": [
    "git clone git@github.com:databio/IGD\n",
    "cd IGD\n",
    "make > igd_make_log.txt 2>&1\n",
    "cd ..\n",
    "\n",
    "export PATH=$BBTUTORIAL/IGD/bin/:$PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bedbuncher using Looper \n",
    "\n",
    "Once we have cloned the `bedbuncher` repository, set our local Elasticsearch cluster and created the `iGD` binary, we can run the pipeline by pointing `looper run` to the appropriate `PEP` config file. As mentioned earlier, if the path to the bedbase configuration file has been stored in the `$BEDBASE` environment variable, it's not neccesary to pass the `--bedbase-config` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: outputs/bedbuncher_output/bedbuncher_pipeline_logs/looper_logs.txt: No such file or directory\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "looper run  bedbase/tutorial_files/PEPs/bedbuncher_config.yaml  --package local \\\n",
    "--command-extra=\"-R\" > outputs/bedbuncher_output/bedbuncher_pipeline_logs/looper_logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BEDHOST:  Serve BED files and API to explore pipeline outputs\n",
    "\n",
    "The last part of the tutorial consists on running a local instance of `bedhost` (a REST API for `bedstat` and `bedbuncher` produced outputs) in order to explore plots, statistics and download pipeline outputs. \n",
    "To run `bedhost`, frist use `bedhost-ui` to built the bedhost user interface with React."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd bedhost-ui\n",
    "# Install node modules defined in package.json\n",
    "npm install \n",
    "# Build the app for production to the ./build folder\n",
    "npm run build\n",
    "# copy the contents of the ./build directory to bedhost/bedhost/static/bedhost-ui\n",
    "cp -avr ./build ../bedhost/bedhost/static/bedhost-ui\n",
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run `bedhost`, we'll pip install the package from the previously cloned repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bedhost/. --user > bedhost_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start `bedhost`, we simply need to run the following command passing the location of the bedbase configuration file to the `-c` flag.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedhost serve -c  $BBTUTORIAL/bedbase/tutorial_files/bedbase_configuration.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have stored the path to the bedbase config in the environment variable `$BEDBASE` (suggested), it's not neccesary to use said flag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedhost serve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bedhost` API can be opened in the url [http://0.0.0.0:8000](http://0.0.0.0:8000). We can now explore the plots and statistics generated by the `bedstat` and `bedbuncher` pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## or optionally run BEDHOST using containers\n",
    "\n",
    "Alternatively, you can run the application inside a container.\n",
    "\n",
    "For that we'll use [docker compose](https://docs.docker.com/compose/), a tool that makes running multi-contaier Docker applications possible. The `docker-compose.yaml` file defines two services: \n",
    "- `fastapi-api`: runs the fastAPI server \n",
    "- `postgres-db`: runs the PostgeSQL database used by the server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $BBTUTORIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `BEDBASE_DATA_PATH_HOST` environment variable to point to the host directory with the pipeline results that will be mounted in the container as a volume. \n",
    "\n",
    "The environment variables are passed to the container via `.env` file, which the `docker-compose.yaml` points to for each service. Additionally, you can just export the environment variables before issuing the `docker-compose` command.\n",
    "When you set the same environment variable in multiple files, here’s the priority used by Compose to choose which value to use:\n",
    "\n",
    "1. Compose file\n",
    "2. Shell environment variables\n",
    "3. Environment file\n",
    "4. Dockerfile\n",
    "4. Variable is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "export BEDBASE_DATA_PATH_HOST=$BBTUTORIAL # should be named like this from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling postgres-db (postgres:)...\n",
      "latest: Pulling from library/postgres\n",
      "Digest: sha256:8f7c3c9b61d82a4a021da5d9618faf056633e089302a726d619fa467c73609e4\n",
      "Status: Downloaded newer image for postgres:latest\n",
      "Recreating postgreSQL-bedbase ... \n",
      "\u001b[1BRecreating fastAPI-bedbase    ... mdone\u001b[0m\n",
      "\u001b[1BAttaching to postgreSQL-bedbase, fastAPI-bedbase\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m \n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m PostgreSQL Database directory appears to contain a database; Skipping initialization\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m \n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m 2020-11-02 23:10:28.883 UTC [1] LOG:  starting PostgreSQL 13.0 (Debian 13.0-1.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m 2020-11-02 23:10:28.885 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m 2020-11-02 23:10:28.885 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m 2020-11-02 23:10:28.891 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m 2020-11-02 23:10:28.901 UTC [25] LOG:  database system was shut down at 2020-11-02 23:03:14 UTC\n",
      "\u001b[33mpostgreSQL-bedbase |\u001b[0m 2020-11-02 23:10:28.909 UTC [1] LOG:  database system is ready to accept connections\n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m wait-for-it.sh: waiting 60 seconds for postgres-db:5432\n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m wait-for-it.sh: postgres-db:5432 is available after 0 seconds\n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m DEBU 2020-11-02 23:10:30,246 | bedhost:est:265 > Configured logger 'bedhost' using logmuse v0.2.6 \n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m DEBU 23:10:30 | bbconf:est:265 > Configured logger 'bbconf' using logmuse v0.2.6 \n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m DEBU 23:10:30 | bbconf:bbconf:105 > Established connection with PostgreSQL: postgres-db \n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m DEBU 2020-11-02 23:10:30,299 | bedhost:main:503 > Determined React UI path: /app/bedhost/static/bedhost-ui \n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m INFO 2020-11-02 23:10:30,299 | bedhost:main:510 > running bedhost app \n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m INFO:     Started server process [1]\n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m INFO:     Waiting for application startup.\n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m INFO:     Application startup complete.\n",
      "\u001b[36mfastAPI-bedbase |\u001b[0m INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "Stopping fastAPI-bedbase      ... \n",
      "Stopping postgreSQL-bedbase   ... \n",
      "\u001b[1Bping postgreSQL-bedbase   ... \u001b[32mdone\u001b[0m"
     ]
    }
   ],
   "source": [
    "cd bedhost; docker-compose up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "329.797px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
