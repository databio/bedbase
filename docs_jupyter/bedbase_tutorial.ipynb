{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEDBASE workflow tutorial\n",
    "\n",
    "This demo demonstrates how to process, analyze, visualize, and serve BED files. The process has 3 steps: First, individual BED files are analyzed using the [bedstat](https://github.com/databio/bedstat) pipeline. Second, BED files are grouped and then analyzed as groups using the [bedbuncher](https://github.com/databio/bedbuncher) pipeline. Finally, the BED files, along with statistics, plots, and grouping information, is served via a web interface and RESTful API using the [bedhost](https://github.com/databio/bedhost/tree/master) package.\n",
    "\n",
    "**Glossary of terms:**\n",
    "\n",
    "- *bedfile*: a tab-delimited file with one genomic region per line. Each genomic region is decribed by 3 required columns: chrom, start and end.\n",
    "- *bedset*: a collection of BED files grouped by with a shared biological, experimental, or logical criterion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Preparation\" data-toc-modified-id=\"1.-Preparation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Preparation</a></span></li><li><span><a href=\"#2.-BEDSTAT:-Generate-statistics-and-plots-of-BED-files\" data-toc-modified-id=\"2.-BEDSTAT:-Generate-statistics-and-plots-of-BED-files-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. BEDSTAT: Generate statistics and plots of BED files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-a-PEP-describing-the-bedfiles-to-process\" data-toc-modified-id=\"Get-a-PEP-describing-the-bedfiles-to-process-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Get a PEP describing the bedfiles to process</a></span></li><li><span><a href=\"#Install-bedstat-dependencies\" data-toc-modified-id=\"Install-bedstat-dependencies-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Install bedstat dependencies</a></span></li><li><span><a href=\"#Inititiate-a-local-elasticsearch-cluster\" data-toc-modified-id=\"Inititiate-a-local-elasticsearch-cluster-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Inititiate a local elasticsearch cluster</a></span></li><li><span><a href=\"#Run-bedstat--on-the-demo-PEP\" data-toc-modified-id=\"Run-bedstat--on-the-demo-PEP-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Run bedstat  on the demo PEP</a></span></li></ul></li><li><span><a href=\"#3.-BEDBUNCHER:-Create-bedsets-and-their-respective-statistics\" data-toc-modified-id=\"3.-BEDBUNCHER:-Create-bedsets-and-their-respective-statistics-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. BEDBUNCHER: Create bedsets and their respective statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-a-new-PEP-describing-the-bedset-name-and-specific-JSON-query\" data-toc-modified-id=\"Create-a-new-PEP-describing-the-bedset-name-and-specific-JSON-query-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create a new PEP describing the bedset name and specific JSON query</a></span></li><li><span><a href=\"#Create-outputs-directory-and-install-bedbuncher-CML-dependencies\" data-toc-modified-id=\"Create-outputs-directory-and-install-bedbuncher-CML-dependencies-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Create outputs directory and install bedbuncher CML dependencies</a></span></li><li><span><a href=\"#Run-bedbuncher-using-Looper\" data-toc-modified-id=\"Run-bedbuncher-using-Looper-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Run bedbuncher using Looper</a></span></li></ul></li><li><span><a href=\"#4.-BEDHOST:--Serve-BED-files-and-API-to-explore-pipeline-outputs\" data-toc-modified-id=\"4.-BEDHOST:--Serve-BED-files-and-API-to-explore-pipeline-outputs-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. BEDHOST:  Serve BED files and API to explore pipeline outputs</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation \n",
    "\n",
    "First, we will create a tutorial directory where we'll store the bedbase pipelines and files to be processed. We'll also need to create an environment variable that points to the tutorial directory (we'll need this variable later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir bedbase_tutorial\n",
    "cd bedbase_tutorial\n",
    "export BBTUTORIAL=`pwd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some example BED files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-08 11:25:06--  http://big.databio.org/example_data/bedbase_tutorial/bed_files.tar.gz\n",
      "Resolving big.databio.org (big.databio.org)... 128.143.245.182, 128.143.245.181\n",
      "Connecting to big.databio.org (big.databio.org)|128.143.245.182|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44549692 (42M) [application/octet-stream]\n",
      "Saving to: ‘bed_files.tar.gz’\n",
      "\n",
      "bed_files.tar.gz    100%[===================>]  42.49M  17.9MB/s    in 2.4s    \n",
      "\n",
      "2020-06-08 11:25:09 (17.9 MB/s) - ‘bed_files.tar.gz’ saved [44549692/44549692]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget http://big.databio.org/example_data/bedbase_tutorial/bed_files.tar.gz     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded files are compressed so we'll need to untar them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bed_files/\n",
      "bed_files/GSE105587_ENCFF018NNF_conservative_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bed_files/GSM2423312_ENCFF155HVK_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE105977_ENCFF617QGK_optimal_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE91663_ENCFF316ASR_peaks_GRCh38.bed.gz\n",
      "bed_files/GSM2423313_ENCFF722AOG_peaks_GRCh38.bed.gz\n",
      "bed_files/GSM2827349_ENCFF196DNQ_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE91663_ENCFF553KIK_optimal_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE91663_ENCFF319TPR_conservative_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE105977_ENCFF937CGY_peaks_GRCh38.bed.gz\n",
      "bed_files/GSM2827350_ENCFF928JXU_peaks_GRCh38.bed.gz\n",
      "bed_files/GSE105977_ENCFF793SZW_conservative_idr_thresholded_peaks_GRCh38.bed.gz\n"
     ]
    }
   ],
   "source": [
    "tar -zxvf bed_files.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we'll download a matrix we need to provide if we wish to plot the tissue specificity of our set of genomic ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-08 11:25:42--  http://big.databio.org/open_chromatin_matrix/openSignalMatrix_hg38_quantileNormalized_round4.txt.gz\n",
      "Resolving big.databio.org (big.databio.org)... 128.143.245.181, 128.143.245.182\n",
      "Connecting to big.databio.org (big.databio.org)|128.143.245.181|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 235485550 (225M) [application/octet-stream]\n",
      "Saving to: ‘openSignalMatrix_hg38_quantileNormalized_round4.txt.gz’\n",
      "\n",
      "openSignalMatrix_hg 100%[===================>] 224.58M  3.80MB/s    in 54s     \n",
      "\n",
      "2020-06-08 11:26:36 (4.13 MB/s) - ‘openSignalMatrix_hg38_quantileNormalized_round4.txt.gz’ saved [235485550/235485550]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget http://big.databio.org/open_chromatin_matrix/openSignalMatrix_hg38_percentile99_01_quantNormalized_round4d.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll download the 3 core pipelines and tools needed to complete this tutorial: `bedstat`, `bedbuncher` and `bedhost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bedbase'...\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 27 (delta 2), reused 27 (delta 2), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (27/27), 12.75 KiB | 12.75 MiB/s, done.\n",
      "Resolving deltas: 100% (2/2), done.\n",
      "Cloning into 'bedstat'...\n",
      "remote: Enumerating objects: 490, done.\u001b[K\n",
      "remote: Total 490 (delta 0), reused 0 (delta 0), pack-reused 490\u001b[K\n",
      "Receiving objects: 100% (490/490), 70.39 KiB | 1.50 MiB/s, done.\n",
      "Resolving deltas: 100% (231/231), done.\n",
      "Cloning into 'bedbuncher'...\n",
      "remote: Enumerating objects: 154, done.\u001b[K\n",
      "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
      "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
      "remote: Total 350 (delta 84), reused 104 (delta 43), pack-reused 196\u001b[K\n",
      "Receiving objects: 100% (350/350), 71.34 KiB | 2.97 MiB/s, done.\n",
      "Resolving deltas: 100% (192/192), done.\n",
      "Cloning into 'bedhost'...\n",
      "remote: Enumerating objects: 325, done.\u001b[K\n",
      "remote: Counting objects: 100% (325/325), done.\u001b[K\n",
      "remote: Compressing objects: 100% (215/215), done.\u001b[K\n",
      "remote: Total 836 (delta 201), reused 207 (delta 103), pack-reused 511\u001b[K\n",
      "Receiving objects: 100% (836/836), 250.50 KiB | 4.25 MiB/s, done.\n",
      "Resolving deltas: 100% (534/534), done.\n"
     ]
    }
   ],
   "source": [
    "git clone git@github.com:databio/bedbase\n",
    "git clone git@github.com:databio/bedstat\n",
    "git clone git@github.com:databio/bedbuncher\n",
    "git clone git@github.com:databio/bedhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BEDSTAT: Generate statistics and plots of BED files \n",
    "\n",
    "### Get a PEP describing the bedfiles to process\n",
    "\n",
    "The first step is to process the BED files using the `bedstat` pipeline, which computes statistics and makes plots for each individual BED file. To begin, we'll need some annotation information for our BED files to load. We'll use the standard [PEP](https://pepkit.github.io/) format for the annotation, which consists of 1) a sample table (.csv) that annotates the files, and 2) a project config.yaml file that points to the sample annotation sheet. The config file also has other components, such as derived attributes, that in this case point to the bedfiles to be processed. Here is the PEP config file for this example project. It includes annotation information for each BED file, and also points to the `.bed.gz` files using derived attributes `output_file_path` and `yaml_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pep_version: 2.0.0\n",
      "sample_table: bedstat_annotation_sheet.csv\n",
      "\n",
      "looper:\n",
      "    output-dir: $BBTUTORIAL/outputs/bedstat_output/bedstat_pipeline_logs \n",
      "\n",
      "sample_modifiers:\n",
      "  append:\n",
      "    bedbase_config: $BBTUTORIAL/bedbase/tutorial_files/bedbase_configuration.yaml\n",
      "    pipeline_interfaces: $BBTUTORIAL/bedstat/pipeline_interface_new.yaml\n",
      "    output_file_path: OUTPUT\n",
      "    yaml_file: SAMPLE_YAML\n",
      "    open_signal_matrix: MATRIX\n",
      "  derive:\n",
      "    attributes: [output_file_path, yaml_file, open_signal_matrix]\n",
      "    sources:\n",
      "      OUTPUT: \"$BBTUTORIAL/bed_files/{file_name}\" \n",
      "      SAMPLE_YAML: \"$BBTUTORIAL/outputs/bedstat_pipeline_logs/submission/{sample_name}.yaml\"\n",
      "      MATRIX: \"$BBTUTORIAL/openSignalMatrix_{genome}_quantileNormalized_round4.txt.gz\""
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/PEPs/bedstat_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install bedstat dependencies\n",
    "\n",
    "`bedstat` is a [pypiper](http://code.databio.org/pypiper/) pipeline that generates statistics and plots of bedfiles. Additionally, `bedstat` uses [bbconf](https://github.com/databio/bbconf), the bedbase configuration manager which implements convenience methods for interacting with an Elasticsearch database, where our file metadata will be placed. These and the appropriate R dependencies can be installed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "pip install -r bedstat/requirements.txt --user > requirements_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install R dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rscript bedstat/scripts/installRdeps.R > R_deps.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an additional dependency needed by `bedstat` if we wish to calculate and plot the GC content of our bedfiles. Depending on the genome assemblies of the files listed on a PEP, the appropriate BSgenome packages should be installed. The following is an example of how we can do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n",
      "    install.packages(\"BiocManager\")\n",
      "\n",
      "BiocManager::install(\"BSgenome.Hsapiens.UCSC.hg38.masked\")"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/scripts/BSgenome_install.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rscript bedbase/tutorial_files/scripts/BSgenome_install.R > BSgenome.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create a directory where we can store the stats and plots generated by `bedstat`. Additionally, we'll create a directory where we can store log and metadata files that we'll need later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p outputs/bedstat_output/bedstat_pipeline_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use `bbconf`, we'll need to create a minimal configuration.yaml file. The path to this configuration file can be stored in the environment variable `$BEDBASE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:\n",
      "  bedstat_output: $BBTUTORIAL/outputs/bedstat_output\n",
      "  bedbuncher_output: $BBTUTORIAL/outputs/bedbuncher_output\n",
      "\n",
      "database:\n",
      "  host: localhost\n",
      "  bed_index: bed_index\n",
      "  bedset_index: bedset_index\n",
      "\n",
      "server:\n",
      "  host: 0.0.0.0\n",
      "  port: 8000\n"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/bedbase_configuration.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inititiate a local PostgreSQL instance\n",
    "\n",
    "In addition to generate statistics and plots, `bedstat` inserts JSON formatted metadata into relational [PostgreSQL] database. \n",
    "\n",
    "If you don't have docker installed, you can install it with `sudo apt-get update && apt-get install docker-engine -y`.\n",
    "\n",
    "Now, create a persistent volume to house PostgreSQL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es-data\n"
     ]
    }
   ],
   "source": [
    "docker volume create postgres-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spin up a `postgres` container. Provide required environment variables (need to match the settings in bedbase configuration file) and bind the created docker volume to `/var/lib/postgresql/data` path in the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1107272f4eda0c5cff262d71881600e76cff4a83c3438f6a786e89e17c6b72f\n"
     ]
    }
   ],
   "source": [
    "docker run -d --name bedbase-postgres -p 5432:5432 -e POSTGRES_PASSWORD=bedbasepassword -e POSTGRES_USER=postgres -e POSTGRES_DB=postgres -v postgres-data:/var/lib/postgresql/data postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bedstat  on the demo PEP\n",
    "To run `bedstat` and the other required pipelines in this tutorial, we will rely on the pipeline submission engine [looper](http://looper.databio.org/en/latest/), which can be installed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install looper --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to establish a modular connection between a project and a pipeline, we'll need to create a [pipeline interface](http://looper.databio.org/en/latest/linking-a-pipeline/) file, which tells looper how to run the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_name: BEDSTAT\n",
      "pipeline_type: sample\n",
      "path: pipeline/bedstat.py\n",
      "input_schema: http://schema.databio.org/pipelines/bedstat.yaml\n",
      "command_template: >\n",
      "  {pipeline.path}\n",
      "  --bedfile {sample.output_file_path}\n",
      "  --genome {sample.genome}\n",
      "  --sample-yaml {sample.yaml_file}\n",
      "  {% if sample.bedbase_config is defined %} --bedbase-config {sample.bedbase_config} {% endif %}\n",
      "  {% if sample.open_signal_matrix is defined %} --open-signal-matrix {sample.open_signal_matrix} {% endif %}\n"
     ]
    }
   ],
   "source": [
    "cat bedstat/pipeline_interface_new.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have properly linked our project to the pipeline of interest, in this case` bedstat`, we simply need to point the `looper run` command our `PEP` config file. Additionally, if the bedbase configuration file location is not stored in the `$BEDBASE` variable, we can pass it to `looper` as an additional argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looper version: 1.2.0\n",
      "Command: run\n",
      "Activating compute package 'local'\n",
      "\u001b[36m## [1 of 11] sample: bedbase_demo_db1; pipeline: BEDSTAT\u001b[0m\n",
      "Writing script to /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db1.sub\n",
      "Job script (n=1; 0.00Gb): /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db1.sub\n",
      "Established connection with Elasticsearch: localhost\n",
      "\u001b[36m## [2 of 11] sample: bedbase_demo_db2; pipeline: BEDSTAT\u001b[0m\n",
      "Writing script to /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db2.sub\n",
      "Job script (n=1; 0.00Gb): /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db2.sub\n",
      "Established connection with Elasticsearch: localhost\n",
      "\u001b[36m## [3 of 11] sample: bedbase_demo_db3; pipeline: BEDSTAT\u001b[0m\n",
      "Writing script to /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db3.sub\n",
      "Job script (n=1; 0.00Gb): /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db3.sub\n",
      "Established connection with Elasticsearch: localhost\n",
      "\u001b[36m## [4 of 11] sample: bedbase_demo_db4; pipeline: BEDSTAT\u001b[0m\n",
      "Writing script to /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db4.sub\n",
      "Job script (n=1; 0.01Gb): /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db4.sub\n",
      "Established connection with Elasticsearch: localhost\n",
      "\u001b[36m## [5 of 11] sample: bedbase_demo_db5; pipeline: BEDSTAT\u001b[0m\n",
      "Writing script to /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db5.sub\n",
      "Job script (n=1; 0.01Gb): /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db5.sub\n",
      "Established connection with Elasticsearch: localhost\n",
      "\u001b[36m## [6 of 11] sample: bedbase_demo_db6; pipeline: BEDSTAT\u001b[0m\n",
      "Writing script to /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db6.sub\n",
      "Job script (n=1; 0.00Gb): /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db6.sub\n",
      "Established connection with Elasticsearch: localhost\n",
      "\u001b[36m## [7 of 11] sample: bedbase_demo_db7; pipeline: BEDSTAT\u001b[0m\n",
      "Writing script to /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db7.sub\n",
      "Job script (n=1; 0.00Gb): /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db7.sub\n",
      "Established connection with Elasticsearch: localhost\n",
      "\u001b[36m## [8 of 11] sample: bedbase_demo_db8; pipeline: BEDSTAT\u001b[0m\n",
      "Writing script to /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db8.sub\n",
      "Job script (n=1; 0.01Gb): /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db8.sub\n",
      "Established connection with Elasticsearch: localhost\n",
      "\u001b[36m## [9 of 11] sample: bedhost_demo_db9; pipeline: BEDSTAT\u001b[0m\n",
      "Writing script to /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedhost_demo_db9.sub\n",
      "Job script (n=1; 0.01Gb): /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedhost_demo_db9.sub\n",
      "Established connection with Elasticsearch: localhost\n",
      "\u001b[36m## [10 of 11] sample: bedbase_demo_db10; pipeline: BEDSTAT\u001b[0m\n",
      "Writing script to /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db10.sub\n",
      "Job script (n=1; 0.01Gb): /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db10.sub\n",
      "Established connection with Elasticsearch: localhost\n",
      "\u001b[36m## [11 of 11] sample: bedbase_demo_db11; pipeline: BEDSTAT\u001b[0m\n",
      "Writing script to /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db11.sub\n",
      "Job script (n=1; 0.01Gb): /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/BEDSTAT_bedbase_demo_db11.sub\n",
      "Established connection with Elasticsearch: localhost\n",
      "\n",
      "Looper finished\n",
      "Samples valid for job generation: 11 of 11\n",
      "Commands submitted: 11 of 11\n",
      "Jobs submitted: 11\n"
     ]
    }
   ],
   "source": [
    "looper run bedbase/tutorial_files/PEPs/bedstat_config.yaml --package local \\\n",
    "--command-extra=\"-R\" > outputs/bedstat_output/bedstat_pipeline_logs/looper_logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for informative purposes, we can inspect how `bedstat` operates on each bedfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute node: cphg-51ksmr2\n",
      "Start time: 2020-06-08 11:28:33\n",
      "### Pipeline run code and environment:\n",
      "\n",
      "*              Command:  `/home/jev4xy/Desktop/bedbase_tutorial/bedstat/pipeline/bedstat.py --bedfile /home/jev4xy/Desktop/bedbase_tutorial/bed_files/GSE105587_ENCFF018NNF_conservative_idr_thresholded_peaks_GRCh38.bed.gz --genome hg38 --sample-yaml /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/bedstat_pipeline_logs/submission/bedbase_demo_db1.yaml --bedbase-config /home/jev4xy/Desktop/bedbase_tutorial/bedbase/tutorial_files/bedbase_configuration.yaml --open-signal-matrix /home/jev4xy/Desktop/bedbase_tutorial/openSignalMatrix_hg38_quantileNormalized_round4.txt.gz -R`\n",
      "*         Compute host:  cphg-51ksmr2\n",
      "*          Working dir:  /home/jev4xy/Desktop/bedbase_tutorial\n",
      "*            Outfolder:  /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedstat_output/78c0e4753d04b238fc07e4ebe5a02984/\n",
      "*  Pipeline started at:   (06-08 11:28:33) elapsed: 0.0 _TIME_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head outputs/bedstat_output/bedstat_pipeline_logs/looper_logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the previous steps have been executed, our bedfiles should be available for query on our local Elasticsearch cluster. Files can be queried using the `bedbuncher` pipeline described in the below section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BEDBUNCHER: Create bedsets and their respective statistics \n",
    "\n",
    "### Create a new PEP describing the bedset name and specific JSON query \n",
    "\n",
    "Now that we've processed several individual BED files, we'll turn to the next task: grouping them together into collections of BED files, which we call *bedsets*. For this, we use the `bedbuncher` pipeline, which produces outputs for each bedset, such as a bedset PEP, bedset-level statistics and plots, and an `IGD` database. To run `bedbuncher`, we will need another PEP describing each bedset. Though the annotation sheet below specifies attributes for one bedset, you can create as many as you wish using additional rows. For each bedset, you need to provide the name of a JSON file containing the query to retrieve certain BED files. The JSONquery_name string will be used to build a sample derived attribute that points to the location of the JSON file. \n",
    "\n",
    "The following example PEP shows the attributes we need to provide for each bedset and the config.yaml file that will grab the files needed to run `bedbuncher`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_name,bedset_name,JSONquery_name,bbconfig_name,JSONquery_path,bedbase_config\n",
      "bedset1,bedbase_demo_bedset,test_query,bedbase_configuration,source1,source2\n"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/PEPs/bedbuncher_query.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pep_version: 2.0.0\n",
      "sample_table: bedbuncher_query.csv\n",
      "\n",
      "looper:\n",
      "    output_dir: $BBTUTORIAL/outputs/bedbuncher_output/bedbuncher_pipeline_logs\n",
      "\n",
      "sample_modifiers:\n",
      "  append:\n",
      "    pipeline_interfaces: $BBTUTORIAL/bedbuncher/pipeline_interface.yaml \n",
      "  derive:\n",
      "    attributes: [JSONquery_path, bedbase_config]\n",
      "    sources:\n",
      "      source1: $BBTUTORIAL/bedbuncher/tests/{JSONquery_name}.json\n",
      "      source2: $BBTUTORIAL/bedbase/tutorial_files/{bbconfig_name}.yaml\n"
     ]
    }
   ],
   "source": [
    "cat bedbase/tutorial_files/PEPs/bedbuncher_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elasticsearch query format is complex and is difficult to create by hand. However, if you have a bedhost instance running, the JSON file contents can be generated using the bedhost graphical search interface. Just specify search criteria of interest and click \"Get Elasticsearch query\" button. This will generate a query string that looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"bool\": {\n",
      "    \"must\": [\n",
      "      {\n",
      "        \"range\": {\n",
      "          \"gc_content\": {\n",
      "            \"gt\": 0.1\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cat bedbuncher/tests/test_query.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create outputs directory and install bedbuncher CML dependencies\n",
    "\n",
    "We need a folder where we can store bedset related outputs. Though not required, we'll also create a directory where we can store the `bedbuncher` pipeline logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p outputs/bedbuncher_output/bedbuncher_pipeline_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the feats of `bedbuncher` includes [IGD](https://github.com/databio/IGD) database creation from the files in the bedset. `IGD` can be installed by cloning the repository from github, executing the make file to create the binary, and pointing the binary location with the `$PATH` environment variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'IGD'...\n"
     ]
    }
   ],
   "source": [
    "git clone git@github.com:databio/IGD\n",
    "cd IGD\n",
    "make > igd_make_log.txt 2>&1\n",
    "cd ..\n",
    "\n",
    "export PATH=$BBTUTORIAL/IGD/bin/:$PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bedbuncher using Looper \n",
    "\n",
    "Once we have cloned the `bedbuncher` repository, set our local Elasticsearch cluster and created the `iGD` binary, we can run the pipeline by pointing `looper run` to the appropriate `PEP` config file. As mentioned earlier, if the path to the bedbase configuration file has been stored in the `$BEDBASE` environment variable, it's not neccesary to pass the `--bedbase-config` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looper version: 1.2.0\n",
      "Command: run\n",
      "Activating compute package 'local'\n",
      "\u001b[36m## [1 of 1] sample: bedset1; pipeline: BEDBUNCHER\u001b[0m\n",
      "Writing script to /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_bedset1.sub\n",
      "Job script (n=1; 0.00Gb): /home/jev4xy/Desktop/bedbase_tutorial/outputs/bedbuncher_output/bedbuncher_pipeline_logs/submission/BEDBUNCHER_bedset1.sub\n",
      "\n",
      "Looper finished\n",
      "Samples valid for job generation: 1 of 1\n",
      "Commands submitted: 1 of 1\n",
      "Jobs submitted: 1\n"
     ]
    }
   ],
   "source": [
    "looper run  bedbase/tutorial_files/PEPs/bedbuncher_config.yaml  --package local \\\n",
    "--command-extra=\"-R\" > outputs/bedbuncher_output/bedbuncher_pipeline_logs/looper_logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BEDHOST:  Serve BED files and API to explore pipeline outputs\n",
    "\n",
    "The last part of the tutorial consists on running a local instance of `bedhost` (a REST API for `bedstat` and `bedbuncher` produced outputs) in order to explore plots, statistics and download pipeline outputs. To run `bedhost`, we'll pip install the package from the previously cloned repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bedhost/. --user > bedhost_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start `bedhost`, we simply need to run the following command passing the location of the bedbase configuration file to the `-c` flag.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedhost serve -c  $BBTUTORIAL/bedbase/tutorial_files/bedbase_configuration.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have stored the path to the bedbase config in the environment variable `$BEDBASE` (suggested), it's not neccesary to use said flag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedhost serve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bedhost` API can be opened in the url [http://0.0.0.0:8000](http://0.0.0.0:8000). We can now explore the plots and statistics generated by the `bedstat` and `bedbuncher` pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "329.797px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
