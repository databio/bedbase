{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to https://docs.bedbase.org","text":"<p>This site hosts developer and user documentation for components of BEDbase and related tools, notably including geniml, our package for machine learning on genomic intervals. Use the tab navigation above to find the project of interest.</p> <p>You can access the main BEDbase interface at https://bedbase.org.</p>"},{"location":"changelog/","title":"Changelogs for BEDbase","text":"<p>BEDbase is not versioned as a whole; instead, you will need to look at the changelogs for the individual components.</p>"},{"location":"citations/","title":"Manuscripts describing BEDbase and geniml","text":"<p>Components of BEDbase and geniml have been published independently. Here, we showcase the tutorials organized by the manuscript that published them, so if you're reading a paper you can more easily find the relevant features. Find the citation of interest on the left to find links to tutorials published as part of each manuscript.</p>"},{"location":"citations/#how-to-cite-bedbase-and-geniml","title":"How to cite BEDbase and geniml","text":"<p>Thanks for citing us! If you use BEDbase, geniml, or their components in your research, here are papers you can cite.</p> If you use... Please cite ... <code>region2vec</code> embeddings Gharavi et al. (2021) Bioinformatics <code>bedspace</code> search and embeddings Gharavi et al. (2024) Bioengineering <code>scEmbed</code> single-cell embedding framework LeRoy et al. (2023) bioRxiv <code>geniml</code> region set evaluations Zheng et al. (2023) bioRxiv <code>geniml hmm</code> module Rymuza et al. (2023) bioRxiv <code>bedbase</code> database Unpublished"},{"location":"citations/#full-citation-information-for-manuscripts","title":"Full citation information for manuscripts","text":"<li>Gharavi et al. (2024). Joint representation learning for retrieval and annotation of genomic interval sets Bioengineering.  DOI: 10.3390/bioengineering11030263</li> <li>Zheng et al. (2023). Methods for evaluating unsupervised vector representations of genomic regions bioRxiv.  DOI: 10.1101/2023.08.28.555137</li> <li>Xue et al. (2023). Opportunities and challenges in sharing and reusing genomic interval data Frontiers in Genetics.  DOI: 10.3389/fgene.2023.1155809</li> <li>Rymuza et al. (2023). Methods for constructing and evaluating consensus genomic interval sets bioRxiv.  DOI: 10.1101/2023.08.03.551899</li> <li>LeRoy et al. (2023). Fast clustering and cell-type annotation of scATACdata with pre-trained embeddings bioRxiv.  DOI: 10.1101/2023.08.01.551452</li> <li>Gu et al. (2021). Bedshift: perturbation of genomic interval sets Genome Biology.  DOI: 10.1186/s13059-021-02440-w</li> <li>Gharavi et al. (2021). Embeddings of genomic region sets capture rich biological associations in low dimensions Bioinformatics.  DOI: 10.1093/bioinformatics/btab439</li>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>Pull requests or issues are welcome.</p> <ul> <li>After adding tests in <code>tests</code> for a new feature or a bug fix, please run the test suite.</li> <li>To do so, the only additional dependencies needed beyond those for the package can be  installed with:</li> </ul> <p><code>pip install -r requirements/requirements-all.txt</code></p> <ul> <li>Once those are installed, the tests can be run with <code>pytest</code>. Alternatively,  <code>python setup.py test</code> can be used.</li> </ul>"},{"location":"bbconf/","title":"BBConf","text":"bbconf <p>BEDbase project configuration package (agent)</p>"},{"location":"bbconf/#what-is-this","title":"What is this?","text":"<p><code>bbconf</code> is a configuration and management tool for BEDbase, facilitating the reading of configuration files,  setting up connections to PostgreSQL, PEPhub, S3, and Qdrant databases, managing file paths, and storing transformer models.  It formalizes communication pathways for pipelines and downstream tools, ensuring seamless interaction.\"</p>"},{"location":"bbconf/#installation","title":"Installation","text":"<p>To install <code>bbconf</code> use this command: </p> <pre><code>pip install bbconf\n</code></pre> <p>or, install the latest version from the GitHub repository:</p> <pre><code>pip install git+https://github.com/databio/bbconf.git\n</code></pre>"},{"location":"bbconf/bbc_api/","title":"Guide","text":""},{"location":"bbconf/bbc_api/#docs-in-progress-stay-tuned-for-updates-were-working-hard-to-bring-you-valuable-content-soon","title":"\ud83d\udea7 Docs in progress! Stay tuned for updates. We're working hard to bring you valuable content soon!","text":""},{"location":"bbconf/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format.</p>"},{"location":"bbconf/changelog/#051-2024-04-09","title":"[0.5.1] - 2024-04-09","text":""},{"location":"bbconf/changelog/#changed","title":"Changed","text":"<ul> <li>updated qdrant uploader</li> <li>bedset bedfile list query improvement</li> <li>other minor fixes in uploading</li> </ul>"},{"location":"bbconf/changelog/#050-2024-04-08","title":"[0.5.0] - 2024-04-08","text":""},{"location":"bbconf/changelog/#changed_1","title":"Changed","text":"<ul> <li>Rebuild bbconf</li> <li>Introduced new DB schema</li> <li>Added bbagent that will be used to interact with the database</li> <li>Updated config schema</li> <li>Added new functionality to the bbagent</li> <li>New tests</li> </ul>"},{"location":"bbconf/changelog/#042-2024-03-12","title":"[0.4.2] - 2024-03-12","text":""},{"location":"bbconf/changelog/#change","title":"Change","text":"<ul> <li>Updated logger</li> <li>Updated requirements</li> <li>Added <code>upload_status</code> column to the <code>bedfile</code> table</li> </ul>"},{"location":"bbconf/changelog/#041-2024-01-01","title":"[0.4.1] - 2024-01-01","text":""},{"location":"bbconf/changelog/#fix","title":"Fix","text":"<ul> <li>Requirements</li> </ul>"},{"location":"bbconf/changelog/#040-2023-12-18","title":"[0.4.0] - 2023-12-18","text":""},{"location":"bbconf/changelog/#change_1","title":"Change","text":"<ul> <li>bbconf to use pipestat v0.6.0 and SQLModel</li> <li>Fixed tests</li> </ul>"},{"location":"bbconf/changelog/#added","title":"Added","text":"<ul> <li><code>qdrant</code> search, insert and update functionality</li> <li>functions that return results in the DRS format for both bed and bedhost. DRS</li> </ul>"},{"location":"bbconf/changelog/#030-2022-08-18","title":"[0.3.0] - 2022-08-18","text":""},{"location":"bbconf/changelog/#change_2","title":"Change","text":"<ul> <li>update select_bedfiles_for_distance</li> <li>update database table schema</li> </ul>"},{"location":"bbconf/changelog/#021-2021-11-11","title":"[0.2.1] - 2021-11-11","text":""},{"location":"bbconf/changelog/#fix_1","title":"Fix","text":"<ul> <li>attempt to fix database connection error</li> </ul>"},{"location":"bbconf/changelog/#020-2021-10-25","title":"[0.2.0] - 2021-10-25","text":"<p>This release introduces backwards incompatible changes </p>"},{"location":"bbconf/changelog/#changed_2","title":"Changed","text":"<ul> <li>switched to object-relational mapping approach (ORM) for database interface</li> </ul>"},{"location":"bbconf/changelog/#011-2021-04-15","title":"[0.1.1] - 2021-04-15","text":""},{"location":"bbconf/changelog/#added_1","title":"Added","text":"<ul> <li>added new fields in the bedfiles and bedsets schema</li> </ul>"},{"location":"bbconf/changelog/#010-2021-02-22","title":"[0.1.0] - 2021-02-22","text":"<p>This release introduces backwards incompatible changes</p>"},{"location":"bbconf/changelog/#changed_3","title":"Changed","text":"<ul> <li><code>BedBaseConf</code> backend (database) to PostgreSQL</li> <li>complete <code>BedBaseConf</code> class redesign</li> </ul>"},{"location":"bbconf/changelog/#002-2020-05-28","title":"[0.0.2] - 2020-05-28","text":""},{"location":"bbconf/changelog/#added_2","title":"Added","text":"<ul> <li>index deleting methods:<ul> <li><code>delete_bedsets_index</code></li> <li><code>delete_bedfiles_index</code></li> </ul> </li> <li>multiple new keys constants</li> </ul>"},{"location":"bbconf/changelog/#changed_4","title":"Changed","text":"<ul> <li>make <code>search_bedfiles</code> and <code>search_bedsets</code> methods return all hits by default instead of just 10. Parametrize it.</li> <li>added more arguments to <code>insert_bedfiles_data</code> and <code>insert_bedsets_data</code> method interfaces: <code>doc_id</code> and <code>force_update</code></li> <li>Elasticsearch documents are inserted into the indices more securily, <code>insert_*</code> methods prevent documents duplication</li> </ul>"},{"location":"bbconf/changelog/#001-2020-02-05","title":"[0.0.1] - 2020-02-05","text":""},{"location":"bbconf/changelog/#added_3","title":"Added","text":"<ul> <li>initial project release</li> </ul>"},{"location":"bbconf/overview/","title":"DEMO of the bbconf module","text":"<p><code>bbconf</code> is a configuration and management tool for BEDbase, facilitating the reading of configuration file, setting up connections to PostgreSQL, PEPhub, S3, and Qdrant databases, managing file paths, and storing transformer models.</p>"},{"location":"bbconf/overview/#introduction","title":"Introduction","text":"<p><code>bbconf</code> is divided into 2 main modules: - <code>bbconf.config_parser</code> - reads the configuration file and sets up connections to databases.  <code>BedBaseConfig</code> class is used to store the passwords, configuration, connection objects, and other information.</p> <ul> <li><code>bbconf.modules</code> - contains modules for managing <code>bed_files</code>, <code>bedsets</code>, and other common functionalities. Users will mainly use this mudule because it provides classes with methods for managing the database.</li> </ul>"},{"location":"bbconf/overview/#example","title":"Example:","text":""},{"location":"bbconf/overview/#1-init-the-bedbaseagent-class","title":"1) Init the <code>BedBaseAgent</code> class","text":"<p><pre><code>from bbconf.bbagent import BedBaseAgent\n\nbbagent = BedBaseAgent(bbconf_file_path)\n</code></pre> Where <code>bbconf_file_path</code> is the path to the configuration file. How to create a configuration file is described in the configuration section.</p>"},{"location":"bbconf/overview/#upload-a-bedfile-to-the-database","title":"Upload a bedfile to the database","text":"<pre><code>    bbagent.bed.add(\n        identifier=bed_metadata.bed_digest,\n        stats=stats.model_dump(exclude_unset=True),\n        metadata=other_metadata,\n        plots=plots.model_dump(exclude_unset=True),\n        files=files.model_dump(exclude_unset=True),\n        classification=classification.model_dump(exclude_unset=True),\n        upload_qdrant=upload_qdrant,\n        upload_pephub=upload_pephub,\n        upload_s3=upload_s3,\n        local_path=outfolder,\n        overwrite=force_overwrite,\n        nofail=True,\n    )\n</code></pre>"},{"location":"bbconf/overview/#get-a-bedfile-from-the-database","title":"Get a bedfile from the database","text":"<pre><code>bed = bbagent.bed.get(identifier=bed_id, full=True,)\n</code></pre>"},{"location":"bbconf/overview/#get-a-bedset-from-the-database","title":"Get a bedset from the database","text":"<pre><code>bedset = bbagent.bedset.get(identifier=bedset_id, full=True,)\n</code></pre>"},{"location":"bbconf/overview/#user-can-access-credentials-and-other-configurations-from-the-bedbaseconfig-class","title":"User can access credentials and other configurations from the <code>BedBaseConfig</code> class","text":"<p>e.g. to get pephub namespace used in config you can use the following code:</p> <pre><code>bbagent.config._config[\"pephub\"][\"namespace\"]\n</code></pre> <p>Full API of bbconf can be found here</p>"},{"location":"bedbase/","title":"BEDbase","text":"<p>BEDbase is a unifying platform for aggregating, analyzing and serving genomic region data as BED files. Input files are processed by a series of Python pipelines. The output of these pipelines is displayed through a RESTful API where users can access BED files along with useful statistics and plots. A project to aggregate, analyze, and serve genomic regions better (aka BED files).</p>"},{"location":"bedbase/#services","title":"Services","text":"<p>Deployed public instance: https://bedbase.org/</p> <p>Documentation: https://docs.bedbase.org/bedhost</p> <p>API: https://api.bedbase.org/</p> <p>API dev: https://api-dev.bedbase.org/</p> <p>UI: https://bedbase.org/</p> <p>UI dev: https://dev.bedhost.pages.dev/</p> <p>Source Code: https://github.com/databio/bedhost/</p> <p>Object store, production https://data2.bedbase.org/ - base URL for cloudflare/backblaze</p>"},{"location":"bedbase/#tutorial","title":"Tutorial","text":"<p>There's a tutorial for bedbase in the docs_jupyter folder (probably outdated).</p>"},{"location":"bedbase/#components","title":"Components","text":"<ul> <li>bedboss: Main BEDbase processing pipeline and managing tool, combining bedqc, bedmaker, bedstat, and bedbuncher</li> <li>bbconf: BEDbase configuration package (core of the BEDbase stack)</li> <li>bedhost: FastAPI application with API for accessing data</li> <li>bedhost-ui: Front-end user interface built with React</li> <li>bedbase.org repository: Repository for deploying the bedhost container to AWS.</li> <li>all_geo_beds: A subfolder, is the scripts to download all bed files on GEO using geofetch and build a backend to host the metadata using bedstat</li> <li>geniml: Machine learning for genomic intervals</li> </ul>"},{"location":"bedbase/bedbase-api-user-guide/","title":"BEDbase API user guide","text":""},{"location":"bedbase/bedbase-api-user-guide/#introduction","title":"Introduction","text":""},{"location":"bedbase/bedbase-api-user-guide/#data-types","title":"Data types","text":"<p>BEDbase stores two types of data, which we call records. They are 1. BEDs, and 2. BEDsets. BEDsets are simply collections of BEDs. Each record in the database is either a BED or a BEDset.</p>"},{"location":"bedbase/bedbase-api-user-guide/#endpoint-organization","title":"Endpoint organization","text":"<p>The endpoints are divided into 3 groups:</p> <ol> <li><code>/bed</code> endpoints are used to interact with metadata for BED records.</li> <li><code>/bedset</code> endpoints are used to interact with metadata for BEDset records.</li> <li><code>/objects</code> endpoints are used to download metadata and get URLs to retrieve the underlying data itself. These endpoints implement the GA4GH DRS standard.</li> </ol> <p>Therefore, to get information and statistics about BED or BEDset records, or what is contained in the database, look through the <code>/bed</code> and <code>/bedset</code> endpoints. But if you need to write a tool that gets the actual underlying files, then you'll need to use the <code>/objects</code> endpoints. The type of identifiers used in each case differ.</p>"},{"location":"bedbase/bedbase-api-user-guide/#record-identifiers-vs-object-identifiers","title":"Record identifiers vs. object identifiers","text":"<p>Each record has an identifier. For example, <code>eaf9ee97241f300f1c7e76e1f945141f</code> is a BED identifier. You can use this identifier for the metadata endpoints. To download files, you'll need something slightly different -- you need an object identifier. This is because each BED record includes multiple files, such as the original BED file, the BigBed file, analysis plots, and so on. To download a file, you will construct what we call the <code>object_id</code>, which identifies the specific file.</p>"},{"location":"bedbase/bedbase-api-user-guide/#how-to-construct-object-identifiers","title":"How to construct object identifiers","text":"<p>Object IDs take the form <code>&lt;record_type&gt;.&lt;record_identifier&gt;.&lt;result_id&gt;</code>. An example of an object_id for a BED file is <code>bed.eaf9ee97241f300f1c7e76e1f945141f.bedfile</code></p> <p>So, you can get information about this object like this:</p> <p><code>GET</code> /objects/bed.eaf9ee97241f300f1c7e76e1f945141f.bedfile</p> <p>Or, you can get a URL to download the actual file with:</p> <p><code>GET</code> /objects/bed.eaf9ee97241f300f1c7e76e1f945141f.bedfile/access/http</p>"},{"location":"bedbase/bedbase-ui-user-guide/","title":"BEDbase public instance","text":"<p>The public BEDbase instance at bedbase.org hosts public BED files.</p> <p>\ud83d\udea7 Documentation under construction.</p>"},{"location":"bedbase/bedbase-ui-user-guide/#resource-links","title":"Resource links","text":"<ul> <li>Documentation: https://docs.bedbase.org/</li> <li>Deployed public instance UI: https://bedbase.org/</li> <li>Dev UI: https://dev.bedhost.pages.dev/</li> <li>API: https://api.bedbase.org/</li> <li>Dev API: https://api-dev.bedbase.org/</li> </ul>"},{"location":"bedbase/bedbase-ui-user-guide/#finding-relevant-bed-files","title":"Finding relevant BED files","text":"<p>The best way to locate data is to use the search interface on the bedbase.org home page. This search interface is smart. It relies on our Text2BED models, which allow you to search the genome using natural language. We have previously computed embeddings for each BED file in BEDbase, and then you can search them.</p>"},{"location":"bedbase/bedbase-ui-user-guide/#downloading-data","title":"Downloading data","text":"<p>From the search interface, you can add results to your cart, then download all the files in your cart.</p>"},{"location":"bedbase/bedbase-ui-user-guide/#bedsets","title":"BEDsets","text":"<p>BEDsets are collections of BED files. BEDbase holds tens of thousands of files, which span reference genome assemblies. You aren't likely to want to use all the data for one project. BEDsets provide a way to group together a subset of the files in BEDbase for a particular purpose. In the future, users will be able to create their own BEDsets.</p>"},{"location":"bedboss/","title":"BEDboss","text":"<p>A command-line and manager tool for calculating statistics for region set files (BED files) and managing them in the BEDbase database.</p>"},{"location":"bedboss/#main-features","title":"Main features:","text":"<p>1) bedmaker - pipeline to convert supported file types into BED format and bigBed format.  2) bedqc - pipeline to flag bed files for further evaluation to determine whether they should be included in the downstream analysis.  3) bedstat - pipeline for obtaining statistics about bed files.  4) bedbuncher - pipeline designed to create bedsets (sets of BED files) that will be retrieved from bedbase.  5) index - pipeline to create vectors of bedfiles and insert them into vector database for further search.  6) Other delete and update tools that manage bed and bedset files in the BEDbase database. </p> <p>Mainly pipelines are intended to be run from command line but nevertheless,  they are also available as a python function, so that user can implement them to his own code (e.g. automatic uploading tools).</p>"},{"location":"bedboss/#installation","title":"Installation","text":"<p>To install <code>bedboss</code> use this command:  <pre><code>pip install bedboss\n</code></pre> or install the latest version from the GitHub repository: <pre><code>pip install git+https://github.com/databio/bedboss.git\n</code></pre></p>"},{"location":"bedboss/#bedboss-dependencies","title":"BEDboss dependencies","text":"<p>Before running any of the pipelines, you need to install the required dependencies.</p> <p>To check if all dependencies are installed, you can run the following command:</p> <pre><code>bedboss check-requirements\n</code></pre> <p>All dependencies can be using this how to documentation: How to install dependencies</p>"},{"location":"bedboss/#bedbase-configuration-file","title":"BEDbase configuration file","text":"<p>To run most of the pipelines, you need to create a BEDbase configuration file.</p> <p>How to create a BEDbase configuration file is described in the configuration section.</p>"},{"location":"bedboss/#pipelines-information","title":"Pipelines information","text":""},{"location":"bedboss/#bedmaker","title":"bedmaker","text":"<p>bedmaker - pipeline to convert supported file types* into BED format and bigBed format. Currently supported formats:</p> <ul> <li>bedGraph</li> <li>bigBed</li> <li>bigWig</li> <li>wig</li> </ul>"},{"location":"bedboss/#bedqc","title":"bedqc","text":"<p>flag bed files for further evaluation to determine whether they should be included in the downstream analysis.  Currently, it flags bed files that are larger than 2G, has over 5 milliom regions, and/or has mean region width less than 10 bp. This threshold can be changed in bedqc function arguments.</p>"},{"location":"bedboss/#bedstat","title":"bedstat","text":"<p>pipeline for obtaining statistics about bed files</p> <p>It produces BED file Statistics:</p> <ul> <li>GC content.The average GC content of the region set. </li> <li>Number of regions. The total number of regions in the BED file. </li> <li>Median TSS distance. The median absolute distance to the Transcription Start Sites (TSS)</li> <li>Mean region width. The average region width of the region set.</li> <li>Exon percentage.  The percentage of the regions in the BED file that are annotated as exon. </li> <li>Intron percentage.    The percentage of the regions in the BED file that are annotated as intron.</li> <li>Promoter proc percentage. The percentage of the regions in the BED file that are annotated as promoter-prox.</li> <li>Intergenic percentage. The percentage of the regions in the BED file that are annotated as intergenic.</li> <li>Promoter core percentage. The percentage of the regions in the BED file that are annotated as promoter-core.</li> <li>5' UTR percentage. The percentage of the regions in the BED file that are annotated as 5'-UTR.</li> <li>3' UTR percentage. The percentage of the regions in the BED file that are annotated as 3'-UTR.</li> </ul>"},{"location":"bedboss/#bedbuncher","title":"bedbuncher","text":"<p>Pipeline designed to create bedsets (sets of BED files) that will be retrieved from bedbase.</p> <p>Example bedsets:</p> <ul> <li>Bed files from the AML database.</li> <li>Bed files from the Excluderanges database.</li> <li>Bed files from the LOLA database http://lolaweb.databio.org/</li> </ul> <p>Bedbuncher calculates statistics: - Bedset statistics (currenty means and standard deviations).</p>"},{"location":"bedboss/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format.</p>"},{"location":"bedboss/changelog/#021-2024-04-09","title":"[0.2.1] - 2024-04-09","text":""},{"location":"bedboss/changelog/#changed","title":"Changed","text":"<ul> <li>small naming tweaks</li> </ul>"},{"location":"bedboss/changelog/#added","title":"Added","text":"<ul> <li>added requirement check to cli</li> </ul>"},{"location":"bedboss/changelog/#020-2024-04-08","title":"[0.2.0] - 2024-04-08","text":""},{"location":"bedboss/changelog/#changed_1","title":"Changed","text":"<ul> <li>moved all uploading functionality to the <code>bbconf</code> package</li> </ul>"},{"location":"bedboss/changelog/#added_1","title":"Added","text":"<ul> <li>added commands for indexing bedfiles</li> <li>added commands for deleting bedfiles and bedsets</li> </ul>"},{"location":"bedboss/changelog/#010-2024-01-26","title":"[0.1.0] - 2024-01-26","text":""},{"location":"bedboss/changelog/#added_2","title":"Added","text":"<ul> <li>Initial alpha release</li> </ul>"},{"location":"bedboss/how-to-configure/","title":"How to create bedbase config file","text":""},{"location":"bedboss/how-to-configure/#bedbase-config-file-is-yaml-file-with-4-parts","title":"Bedbase config file is yaml file with 4 parts:","text":"<ul> <li>paths and vector models</li> <li>relational database credentials</li> <li>qdrant credentials</li> <li>server information</li> <li>remote info</li> <li>pephub info</li> <li>s3 credentials</li> </ul>"},{"location":"bedboss/how-to-configure/#example","title":"Example:","text":"<pre><code>path:\n  remote_url_base: http://data.bedbase.org/\n  region2vec: databio/r2v-encode-hg38\n  vec2vec: databio/v2v-geo-hg38\n  text2vec: sentence-transformers/all-MiniLM-L6-v2\ndatabase:\n  host: $POSTGRES_HOST\n  port: 5432\n  password: $POSTGRES_PASSWORD\n  user: $POSTGRES_USER\n  database: bedbase2\nqdrant:\n  host: $QDRANT_HOST\n  port: 6333\n  api_key: $QDRANT_API_KEY\n  collection: bedbase2\nserver:\n  host: 0.0.0.0\n  port: 8000\ns3:\n  endpoint_url: $AWS_ENDPOINT_URL\n  aws_access_key_id: $AWS_ACCESS_KEY_ID\n  aws_secret_access_key: $AWS_SECRET_ACCESS_KEY\n  bucket: bedbase\nphc:\n  namespace: databio\n  name: bedbase_all\n  tag: default\naccess_methods:\n  http:\n    type: \"https\"\n    description: HTTP compatible path\n    prefix: https://data2.bedbase.org/\n  s3:\n    type: \"s3\"\n    description: S3 compatible path\n    prefix: s3://data2.bedbase.org/\n  local:\n    type: \"https\"\n    description: How to serve local files.\n    prefix: /static/\n</code></pre>"},{"location":"bedboss/how-to-create-database/","title":"How to create BEDbase database","text":"<p>To run bedboss and upload data to the database we need to create postgres database, or use existing one.</p>"},{"location":"bedboss/how-to-create-database/#to-create-local-database","title":"To create local database:","text":"<p>We are initiating postgres db in docker. If you don't have docker installed, you can install it with  <pre><code>sudo apt-get update &amp;&amp; apt-get install docker-engine -y\n</code></pre></p> <p>Now, create a persistent volume to house PostgreSQL data:</p> <pre><code>docker volume create postgres-data\n</code></pre> <pre><code>docker run -d --name bedbase-postgres -p 5432:5432 \\\n  -e POSTGRES_PASSWORD=bedbasepassword \\\n  -e POSTGRES_USER=postgres \\\n  -e POSTGRES_DB=postgres \\\n  -v postgres-data:/var/lib/postgresql/data \\\n  postgres:13\n</code></pre> <p>Now we have created docker and can run pipelines. To connect to the database, change your credentials in your configuration file.</p>"},{"location":"bedboss/how-to-install-requirements/","title":"How to install R dependencies","text":"<ol> <li>Install bedboss</li> <li>Install R: https://cran.r-project.org/bin/linux/ubuntu/fullREADME.html</li> <li>Download this script: installRdeps.R</li> <li>Install dependencies by running this command in your terminal: <code>Rscript installRdeps.R</code></li> <li>Run <code>bedboss check-requirements</code> to check if everything was installed correctly.</li> </ol>"},{"location":"bedboss/how-to-install-requirements/#how-to-install-regionset-conversion-tools","title":"How to install regionset conversion tools:","text":"<ul> <li>bedToBigBed: http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/bedToBigBed</li> <li>bigBedToBed: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/bigBedToBed</li> <li>bigWigToBedGraph: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/bigWigToBedGraph</li> <li>wigToBigWig: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/wigToBigWig</li> </ul>"},{"location":"bedboss/usage/","title":"Usage reference","text":"<p>BEDboss is command-line tool-manager and a set of tools for working with BED files and BEDbase. Main components of BEDboss are: 1) Pipelines for processing BED files: bedmaker, bedqc, and bedstats. 2) Indexing of the Bed files in bedbase 3) Managing bed and bedsets in the database</p> <p>Here you can see the command-line usage instructions for the main bedboss command and for each subcommand:</p>"},{"location":"bedboss/usage/#bedboss-help","title":"<code>bedboss --help</code>","text":"<pre><code> Usage: bedboss [OPTIONS] COMMAND [ARGS]...                                                                                                                                                                                             \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version             -v                                       App version                                                                                                                                                           \u2502\n\u2502 --install-completion          [bash|zsh|fish|powershell|pwsh]  Install completion for the specified shell. [default: None]                                                                                                           \u2502\n\u2502 --show-completion             [bash|zsh|fish|powershell|pwsh]  Show completion for the specified shell, to copy it or customize the installation. [default: None]                                                                    \u2502\n\u2502 --help                                                         Show this message and exit.                                                                                                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 check-requirements                              check installed R packages                                                                                                                                                           \u2502\n\u2502 delete-bed                                      Delete bed from the bedbase database                                                                                                                                                 \u2502\n\u2502 delete-bedset                                   Delete BedSet from the bedbase database                                                                                                                                              \u2502\n\u2502 init-config                                     Initialize the new, sample configuration file                                                                                                                                        \u2502\n\u2502 make-bed                                        Create a bed files form a [bigwig, bedgraph, bed, bigbed, wig] file                                                                                                                  \u2502\n\u2502 make-bedset                                     Create a bedset from a pep file, and insert it to the bedbase database.                                                                                                              \u2502\n\u2502 make-bigbed                                     Create a bigbed files form a bed file                                                                                                                                                \u2502\n\u2502 reindex                                         Reindex the bedbase database and insert all files to the qdrant database.                                                                                                            \u2502\n\u2502 run-all                                         Run all the bedboss pipeline for a single bed file                                                                                                                                   \u2502\n\u2502 run-pep                                         Run the all bedboss pipeline for a bed files in a PEP                                                                                                                                \u2502\n\u2502 run-qc                                          Run the quality control for a bed file                                                                                                                                               \u2502\n\u2502 run-stats                                       Create the statistics for a single bed file.                                                                                                                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-check-requirements-help","title":"<code>bedboss check-requirements --help</code>","text":"<pre><code> Usage: bedboss check-requirements [OPTIONS]                                                                                                                                                                                            \n\n check installed R packages                                                                                                                                                                                                             \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                                                                                                                                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-delete-bed-help","title":"<code>bedboss delete-bed --help</code>","text":"<pre><code> Usage: bedboss delete-bed [OPTIONS]                                                                                                                                                                                                    \n\n Delete bed from the bedbase database                                                                                                                                                                                                   \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --sample-id        TEXT  Sample ID [default: None] [required]                                                                                                                                                                     \u2502\n\u2502 *  --config           TEXT  Path to the bedbase config file [default: None] [required]                                                                                                                                               \u2502\n\u2502    --help                   Show this message and exit.                                                                                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-delete-bedset-help","title":"<code>bedboss delete-bedset --help</code>","text":"<pre><code> Usage: bedboss delete-bedset [OPTIONS]                                                                                                                                                                                                 \n\n Delete BedSet from the bedbase database                                                                                                                                                                                                \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --identifier        TEXT  BedSet ID [default: None] [required]                                                                                                                                                                    \u2502\n\u2502 *  --config            TEXT  Path to the bedbase config file [default: None] [required]                                                                                                                                              \u2502\n\u2502    --help                    Show this message and exit.                                                                                                                                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-init-config-help","title":"<code>bedboss init-config --help</code>","text":"<pre><code> Usage: bedboss init-config [OPTIONS]                                                                                                                                                                                                   \n\n Initialize the new, sample configuration file                                                                                                                                                                                          \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --outfolder        TEXT  Path to the output folder [default: None] [required]                                                                                                                                                     \u2502\n\u2502    --help                   Show this message and exit.                                                                                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-make-bed-help","title":"<code>bedboss make-bed --help</code>","text":"<pre><code> Usage: bedboss make-bed [OPTIONS]                                                                                                                                                                                                      \n\n Create a bed files form a [bigwig, bedgraph, bed, bigbed, wig] file                                                                                                                                                                    \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --input-file                        TEXT  Path to the input file [default: None] [required]                                                                                                                                       \u2502\n\u2502 *  --input-type                        TEXT  Type of the input file. Options are: bigwig, bedgraph, bed, bigbed, wig [default: None] [required]                                                                                      \u2502\n\u2502 *  --outfolder                         TEXT  Path to the output folder [default: None] [required]                                                                                                                                    \u2502\n\u2502 *  --genome                            TEXT  Genome name. Example: 'hg38' [default: None] [required]                                                                                                                                 \u2502\n\u2502    --rfg-config                        TEXT  Path to the rfg config file [default: None]                                                                                                                                             \u2502\n\u2502    --narrowpeak     --no-narrowpeak          Is the input file a narrowpeak file? [default: no-narrowpeak]                                                                                                                           \u2502\n\u2502    --chrom-sizes                       TEXT  Path to the chrom sizes file [default: None]                                                                                                                                            \u2502\n\u2502    --multi          --no-multi               Run multiple samples [default: no-multi]                                                                                                                                                \u2502\n\u2502    --recover        --no-recover             Recover from previous run [default: recover]                                                                                                                                            \u2502\n\u2502    --dirty          --no-dirty               Run without removing existing files [default: no-dirty]                                                                                                                                 \u2502\n\u2502    --help                                    Show this message and exit.                                                                                                                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-make-bedset-help","title":"<code>bedboss make-bedset --help</code>","text":"<pre><code> Usage: bedboss make-bedset [OPTIONS]                                                                                                                                                                                                   \n\n Create a bedset from a pep file, and insert it to the bedbase database.                                                                                                                                                                \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --pep                                        TEXT  PEP file. Local or remote path [default: None] [required]                                                                                                                      \u2502\n\u2502 *  --outfolder                                  TEXT  Path to the output folder [default: None] [required]                                                                                                                           \u2502\n\u2502 *  --bedbase-config                             TEXT  Path to the bedbase config file [default: None] [required]                                                                                                                     \u2502\n\u2502 *  --bedset-name                                TEXT  Name of the bedset [default: None] [required]                                                                                                                                  \u2502\n\u2502    --heavy              --no-heavy                    Run the heavy version of the pipeline [default: no-heavy]                                                                                                                      \u2502\n\u2502    --force-overwrite    --no-force-overwrite          Force overwrite the output files [default: no-force-overwrite]                                                                                                                 \u2502\n\u2502    --upload-s3          --no-upload-s3                Upload to S3 [default: no-upload-s3]                                                                                                                                           \u2502\n\u2502    --upload-pephub      --no-upload-pephub            Upload to PEPHub [default: no-upload-pephub]                                                                                                                                   \u2502\n\u2502    --no-fail            --no-no-fail                  Do not fail on error [default: no-no-fail]                                                                                                                                     \u2502\n\u2502    --help                                             Show this message and exit.                                                                                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-make-bigbed-help","title":"<code>bedboss make-bigbed --help</code>","text":"<pre><code> Usage: bedboss make-bigbed [OPTIONS]                                                                                                                                                                                                   \n\n Create a bigbed files form a bed file                                                                                                                                                                                                  \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bed-file                       TEXT  Path to the input file [default: None] [required]                                                                                                                                          \u2502\n\u2502 *  --bed-type                       TEXT  bed type to be used for bigBed file generation 'bed{bedtype}+{n}' [Default: None] (e.g bed3+1) [default: None] [required]                                                                  \u2502\n\u2502 *  --outfolder                      TEXT  Path to the output folder [default: None] [required]                                                                                                                                       \u2502\n\u2502 *  --genome                         TEXT  Genome name. Example: 'hg38' [default: None] [required]                                                                                                                                    \u2502\n\u2502    --rfg-config                     TEXT  Path to the rfg config file [default: None]                                                                                                                                                \u2502\n\u2502    --chrom-sizes                    TEXT  Path to the chrom sizes file [default: None]                                                                                                                                               \u2502\n\u2502    --multi          --no-multi            Run multiple samples [default: no-multi]                                                                                                                                                   \u2502\n\u2502    --recover        --no-recover          Recover from previous run [default: recover]                                                                                                                                               \u2502\n\u2502    --dirty          --no-dirty            Run without removing existing files [default: no-dirty]                                                                                                                                    \u2502\n\u2502    --help                                 Show this message and exit.                                                                                                                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-reindex-help","title":"<code>bedboss reindex --help</code>","text":"<pre><code> Usage: bedboss reindex [OPTIONS]                                                                                                                                                                                                       \n\n Reindex the bedbase database and insert all files to the qdrant database.                                                                                                                                                              \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bedbase-config        TEXT  Path to the bedbase config file [default: None] [required]                                                                                                                                          \u2502\n\u2502    --help                        Show this message and exit.                                                                                                                                                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-run-all-help","title":"<code>bedboss run-all --help</code>","text":"<pre><code> Usage: bedboss run-all [OPTIONS]                                                                                                                                                                                                       \n\n Run all the bedboss pipeline for a single bed file                                                                                                                                                                                     \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --input-file                                    TEXT  Path to the input file [default: None] [required]                                                                                                                           \u2502\n\u2502 *  --input-type                                    TEXT  Type of the input file. Options are: bigwig, bedgraph, bed, bigbed, wig [default: None] [required]                                                                          \u2502\n\u2502 *  --outfolder                                     TEXT  Path to the output folder [default: None] [required]                                                                                                                        \u2502\n\u2502 *  --genome                                        TEXT  Genome name. Example: 'hg38' [default: None] [required]                                                                                                                     \u2502\n\u2502 *  --bedbase-config                                TEXT  Path to the bedbase config file [default: None] [required]                                                                                                                  \u2502\n\u2502    --rfg-config                                    TEXT  Path to the rfg config file [default: None]                                                                                                                                 \u2502\n\u2502    --narrowpeak            --no-narrowpeak               Is the input file a narrowpeak file? [default: no-narrowpeak]                                                                                                               \u2502\n\u2502    --check-qc              --no-check-qc                 Check the quality of the input file? [default: check-qc]                                                                                                                    \u2502\n\u2502    --chrom-sizes                                   TEXT  Path to the chrom sizes file [default: None]                                                                                                                                \u2502\n\u2502    --open-signal-matrix                            TEXT  Path to the open signal matrix file [default: None]                                                                                                                         \u2502\n\u2502    --ensdb                                         TEXT  Path to the EnsDb database file [default: None]                                                                                                                             \u2502\n\u2502    --just-db-commit        --no-just-db-commit           Just commit to the database? [default: no-just-db-commit]                                                                                                                   \u2502\n\u2502    --force-overwrite       --no-force-overwrite          Force overwrite the output files [default: no-force-overwrite]                                                                                                              \u2502\n\u2502    --upload-qdrant         --no-upload-qdrant            Upload to Qdrant [default: no-upload-qdrant]                                                                                                                                \u2502\n\u2502    --upload-s3             --no-upload-s3                Upload to S3 [default: no-upload-s3]                                                                                                                                        \u2502\n\u2502    --upload-pephub         --no-upload-pephub            Upload to PEPHub [default: no-upload-pephub]                                                                                                                                \u2502\n\u2502    --multi                 --no-multi                    Run multiple samples [default: no-multi]                                                                                                                                    \u2502\n\u2502    --recover               --no-recover                  Recover from previous run [default: recover]                                                                                                                                \u2502\n\u2502    --dirty                 --no-dirty                    Run without removing existing files [default: no-dirty]                                                                                                                     \u2502\n\u2502    --help                                                Show this message and exit.                                                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-run-pep-help","title":"<code>bedboss run-pep --help</code>","text":"<pre><code> Usage: bedboss run-pep [OPTIONS]                                                                                                                                                                                                       \n\n Run the all bedboss pipeline for a bed files in a PEP                                                                                                                                                                                  \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --pep                                        TEXT  PEP file. Local or remote path [default: None] [required]                                                                                                                      \u2502\n\u2502 *  --outfolder                                  TEXT  Path to the output folder [default: None] [required]                                                                                                                           \u2502\n\u2502 *  --bedbase-config                             TEXT  Path to the bedbase config file [default: None] [required]                                                                                                                     \u2502\n\u2502    --create-bedset      --no-create-bedset            Create a new bedset [default: no-create-bedset]                                                                                                                                \u2502\n\u2502    --bedset-heavy       --no-bedset-heavy             Run the heavy version of the bedbuncher pipeline [default: no-bedset-heavy]                                                                                                    \u2502\n\u2502    --bedset-id                                  TEXT  Bedset ID [default: None]                                                                                                                                                      \u2502\n\u2502    --rfg-config                                 TEXT  Path to the rfg config file [default: None]                                                                                                                                    \u2502\n\u2502    --check-qc           --no-check-qc                 Check the quality of the input file? [default: check-qc]                                                                                                                       \u2502\n\u2502    --ensdb                                      TEXT  Path to the EnsDb database file [default: None]                                                                                                                                \u2502\n\u2502    --just-db-commit     --no-just-db-commit           Just commit to the database? [default: no-just-db-commit]                                                                                                                      \u2502\n\u2502    --force-overwrite    --no-force-overwrite          Force overwrite the output files [default: no-force-overwrite]                                                                                                                 \u2502\n\u2502    --upload-qdrant      --no-upload-qdrant            Upload to Qdrant [default: no-upload-qdrant]                                                                                                                                   \u2502\n\u2502    --upload-s3          --no-upload-s3                Upload to S3 [default: no-upload-s3]                                                                                                                                           \u2502\n\u2502    --upload-pephub      --no-upload-pephub            Upload to PEPHub [default: no-upload-pephub]                                                                                                                                   \u2502\n\u2502    --no-fail            --no-no-fail                  Do not fail on error [default: no-no-fail]                                                                                                                                     \u2502\n\u2502    --multi              --no-multi                    Run multiple samples [default: no-multi]                                                                                                                                       \u2502\n\u2502    --recover            --no-recover                  Recover from previous run [default: recover]                                                                                                                                   \u2502\n\u2502    --dirty              --no-dirty                    Run without removing existing files [default: no-dirty]                                                                                                                        \u2502\n\u2502    --help                                             Show this message and exit.                                                                                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-run-qc-help","title":"<code>bedboss run-qc --help</code>","text":"<pre><code> Usage: bedboss run-qc [OPTIONS]                                                                                                                                                                                                        \n\n Run the quality control for a bed file                                                                                                                                                                                                 \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bed-file                             TEXT     Path to the bed file to check the quality control on. [default: None] [required]                                                                                                  \u2502\n\u2502 *  --outfolder                            TEXT     Path to the output folder [default: None] [required]                                                                                                                              \u2502\n\u2502    --max-file-size                        INTEGER  Maximum file size threshold to pass the quality [default: 2147483648]                                                                                                             \u2502\n\u2502    --max-region-number                    INTEGER  Maximum number of regions threshold to pass the quality [default: 5000000]                                                                                                        \u2502\n\u2502    --min-region-width                     INTEGER  Minimum region width threshold to pass the quality [default: 10]                                                                                                                  \u2502\n\u2502    --multi                --no-multi               Run multiple samples [default: no-multi]                                                                                                                                          \u2502\n\u2502    --recover              --no-recover             Recover from previous run [default: recover]                                                                                                                                      \u2502\n\u2502    --dirty                --no-dirty               Run without removing existing files [default: no-dirty]                                                                                                                           \u2502\n\u2502    --help                                          Show this message and exit.                                                                                                                                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/usage/#bedboss-run-stats-help","title":"<code>bedboss run-stats --help</code>","text":"<pre><code> Usage: bedboss run-stats [OPTIONS]                                                                                                                                                                                                     \n\n Create the statistics for a single bed file.                                                                                                                                                                                           \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --bed-file                                     TEXT  Path to the bed file [default: None] [required]                                                                                                                              \u2502\n\u2502 *  --genome                                       TEXT  Genome name. Example: 'hg38' [default: None] [required]                                                                                                                      \u2502\n\u2502 *  --outfolder                                    TEXT  Path to the output folder [default: None] [required]                                                                                                                         \u2502\n\u2502    --ensdb                                        TEXT  Path to the EnsDb database file [default: None]                                                                                                                              \u2502\n\u2502    --open-signal-matrix                           TEXT  Path to the open signal matrix file [default: None]                                                                                                                          \u2502\n\u2502    --just-db-commit        --no-just-db-commit          Just commit to the database? [default: no-just-db-commit]                                                                                                                    \u2502\n\u2502    --multi                 --no-multi                   Run multiple samples [default: no-multi]                                                                                                                                     \u2502\n\u2502    --recover               --no-recover                 Recover from previous run [default: recover]                                                                                                                                 \u2502\n\u2502    --dirty                 --no-dirty                   Run without removing existing files [default: no-dirty]                                                                                                                      \u2502\n\u2502    --help                                               Show this message and exit.                                                                                                                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"bedboss/tutorials/bedbuncher_tutorial/","title":"BEDbuncher tutorial","text":""},{"location":"bedboss/tutorials/bedbuncher_tutorial/#bedbuncher","title":"BEDbuncher","text":"<p>Bedbuncher is used to create bedset of bed files in the bedbase database.</p>"},{"location":"bedboss/tutorials/bedbuncher_tutorial/#1-create-bedbase-config-file","title":"1) Create bedbase config file","text":"<p>How to create config file: configuration section.</p>"},{"location":"bedboss/tutorials/bedbuncher_tutorial/#2-create-pep-with-bed-file-record-identifiers","title":"2) Create pep with bed file record identifiers.","text":"<p>To do so, you need to create a PEP with the following fields: sample_name (where sample_name is record_identifier), or <code>sample_name</code> + <code>record_identifier</code> e.g. sample_table:</p> sample_name record_identifier sample1 asdf3215f34 sample2 a23452f34tf"},{"location":"bedboss/tutorials/bedbuncher_tutorial/#3-run-bedboss-bunch","title":"3) Run bedboss bunch","text":""},{"location":"bedboss/tutorials/bedbuncher_tutorial/#from-command-line","title":"From command line","text":"<pre><code>bedboss bunch \\\n  --bedbase-config path/to/bedbase_config.yaml \\\n  --bedset-name bedset1 \\\n  --pep path/to/pep.yaml \\\n  --outfolder path/to/output/dir \\\n  --heavy \\\n  --upload-pephub \\\n  --upload-s3 \n</code></pre>"},{"location":"bedboss/tutorials/bedbuncher_tutorial/#run-bedboss-bunch-from-within-python","title":"Run bedboss bunch from within Python","text":"<pre><code>from bedboss.bedbuncher.bedbuncher import run_bedbuncher_form_pep\n\nrun_bedbuncher_form_pep(\n    bedbase_config=bedbase_config,\n    bedset_pep=pep,\n    output_folder=outfolder,\n    bedset_name=bedset_name,\n    heavy=heavy,\n    upload_pephub=upload_pephub,\n    upload_s3=upload_s3,\n    no_fail=no_fail,\n    force_overwrite=force_overwrite,\n    )\n</code></pre>"},{"location":"bedboss/tutorials/bedclassifier_tutorial/","title":"BEDclassifier tutorial","text":""},{"location":"bedboss/tutorials/bedclassifier_tutorial/#tutorial-in-progress-stay-tuned-for-updates-were-working-hard-to-bring-you-valuable-content-soon","title":"\ud83d\udea7 Tutorial in progress! Stay tuned for updates. We're working hard to bring you valuable content soon!","text":""},{"location":"bedboss/tutorials/bedindex_tutorial/","title":"BEDindex tutorial","text":""},{"location":"bedboss/tutorials/bedindex_tutorial/#indexing-to-qdrant-database","title":"Indexing to qdrant database","text":""},{"location":"bedboss/tutorials/bedindex_tutorial/#1-create-bedbase-config-file","title":"1. Create bedbase config file","text":"<p>How to create a BEDbase configuration file is described in the configuration section.</p>"},{"location":"bedboss/tutorials/bedindex_tutorial/#2-run-bedboss-index","title":"2. Run bedboss index","text":""},{"location":"bedboss/tutorials/bedindex_tutorial/#from-command-line","title":"From command line","text":"<pre><code>bedboss reindex --bedbase-config path/to/bedbase_config.yaml\n</code></pre> <p>After running this comman all files that are in the database and weren't indexed will be indexed to qdrant database.</p>"},{"location":"bedboss/tutorials/bedindex_tutorial/#from-within-python","title":"From within Python","text":"<pre><code>from bedboss.qdrant_index.qdrant_index import add_to_qdrant\n\nadd_to_qdrant(config=bedbase_config)\n</code></pre>"},{"location":"bedboss/tutorials/bedmaker_tutorial/","title":"BEDmaker tutorial","text":""},{"location":"bedboss/tutorials/bedmaker_tutorial/#bedmaker","title":"BEDmaker","text":"<p>The BEDmaker is a tool that allows you to convert various file types into BED format and bigBed format. Currently supported formats are: - bed  - bigBed - bigWig - wig</p> <p>Before running pipeline first, you have to install bedboss and check if bedmaker requirements are satisfied. To do so, you can run the next command: <pre><code>bedboss check-requirements\n</code></pre></p>"},{"location":"bedboss/tutorials/bedmaker_tutorial/#run-bedmaker-from-command-line","title":"Run BEDmaker from command line","text":"<pre><code>bedboss make-bed \\\n    --input-file path/to/input/file \\\n    --input-type bed\\\n    --outfolder path/to/output/dir \\\n    --genome hg38\n</code></pre>"},{"location":"bedboss/tutorials/bedmaker_tutorial/#run-bedmaker-from-within-python","title":"Run BEDmaker from within Python","text":"<pre><code>from bedboss.bedmaker.bedmaker import make_all\n\nmake_all(\n    input_file=\"path/to/input/file\",\n    input_type=\"bed\",\n    output_path=\"path/to/output/dir\",\n    genome=\"hg38\",\n)\n</code></pre>"},{"location":"bedboss/tutorials/bedms_tutorial/","title":"BEDMS tutorial","text":""},{"location":"bedboss/tutorials/bedms_tutorial/#bedms","title":"BEDMS","text":"<p>BEDMS (BED Metadata Standardizer) is a tool desgined to standardize genomics and epigenomics metadata attributes according to user-selected or user-trained schemas. BEDMS ensures consistency and FAIRness of metadata across different platforms.  Users can interact with BEDMS either through Python or via PEPhub choosing from predefined schemas provided by the tool. Additionally, BEDMS allows users to create and train custom schemas as per their project requirements. For detailed information on the available schemas, please visit HuggingFace. </p>"},{"location":"bedboss/tutorials/bedms_tutorial/#installation","title":"Installation","text":"<p>To install bedms use this command:</p> <pre><code>pip install bedms\n</code></pre> <p>or install the latest version from the GitHub repository:</p> <pre><code>pip install git+https://github.com/databio/bedms.git\n</code></pre>"},{"location":"bedboss/tutorials/bedms_tutorial/#usage","title":"Usage","text":"<p>BEDMS can be used to standardize metadata attributes based on available schemas, train models on custom schemas, and standardize attributes based on the custom schema models.</p>"},{"location":"bedboss/tutorials/bedms_tutorial/#standardizing-based-on-available-schemas","title":"Standardizing based on available schemas","text":"<p>If you want to standardize the attributes in your PEP based on our available schemas, you can either visit PEPhub or using Python:</p> <p><pre><code>from bedms import AttrStandardizer\n\nmodel = AttrStandardizer(\n    repo_id=\"databio/attribute-standardizer-model6\", model_name=\"encode\"\n)\nresults = model.standardize(pep=\"geo/gse228634:default\")\n\nprint(results) #Dictionary of suggested predictions with their confidence: {'attr_1':{'prediction_1': 0.70, 'prediction_2':0.30}}\n</code></pre> In the above example, we have provided the <code>repo_id</code> which is the path to the repository that holds the models on HuggingFace. The <code>model_name</code> selection can vary based on your choice of schema. You can view the schemas on PEPhub for encode, fairtracks, and bedbase. For standardization, you need to provide the path to your PEP which in the above example is <code>pep=\"geo/gse228634:default\"</code>.</p>"},{"location":"bedboss/tutorials/bedms_tutorial/#training-custom-schemas","title":"Training custom schemas","text":"<p>If you want to train your custom schema-based models, you would need two things to get started:     1. Training sets      2. HuggingFace model and associated files</p>"},{"location":"bedboss/tutorials/bedms_tutorial/#training-sets","title":"Training sets","text":"<p>To develop training sets, follow the step by step protocol mentioned below:   </p> <ol> <li> <p>Select what attributes would be most suitable for your project metadata. For example, here are some attributes that you might choose: <pre><code>sample_name: Name of the sample_name\nassembly: Genome assembly (e.g. hg38)\nspecies_name: Scientific name of the species \n</code></pre></p> </li> <li> <p>Fetch training data from ontologies, publications and other available surces to make two directories: <code>values_directory</code> and <code>headers_directory</code>. <code>values_directory</code> has all the values associated with that attribute while the <code>headers_directory</code> has various synonyms for the attribute names.      The directory structure would look like this:     <pre><code>values_directory/\n    values_1.csv\n    values_2.csv\n    values_3.csv\n    .\n    .\n    values_1000.csv\n\nheaders_directory/\n    headers_1.csv\n    headers_2.csv\n    headers_3.csv\n    .\n    .\n    values_1000.csv\n</code></pre>         To see an example of what a <code>values_*.csv</code> and <code>headers_*.csv</code> might look like, you can check our sample csv files on PEPhub: sample_bedms_values_1.csv and sample_bedms-headers_1.csv.     While these are only samples and are not information dense, we recommend having large vocabulary for the training files for both the <code>values_directory</code> and <code>headers_directory</code>. To get a better understanding of the training data that we trained BEDMS on, you can visit this link</p> </li> <li> <p>Once your training sets are ready, you can make a directory for your schema in your HuggingFace repository. If the name of your schema is <code>new_schema</code> and the name of your repository is <code>new_repo</code>, this is what the directory structure will look like:     <pre><code>new_repo/\n    new_schema/\n        new_schema_design.yaml #This has the schema design defining the attributes with their data types and descriptions\n</code></pre></p> </li> <li> <p>You can now start training your model using the <code>AttrStandardizerTrainer</code> module. For this, you would need a <code>training_config.yaml</code>. Please follow the config file schema given here.</p> <p>To instantiate <code>AttrStandardizerTrainer</code> class:</p> <p><pre><code>from bedms.train import TrainStandardizer\n\ntrainer = TrainStandardizer(\"training_config.yaml\")\n</code></pre> To load the datasets and encode them:</p> <pre><code>train_data, val_data, test_data, label_encoder, vectorizer = trainer.load_data()\n</code></pre> <p>To train the custom model:</p> <pre><code>trainer.train()\n</code></pre> <p>To test the custom model:</p> <pre><code>test_results_dict = trainer.test() #Dictionary with Precision, Recall, and F1 values\n</code></pre> <p>To generate visualizations such as Learning Curves, Confusion Matrices, and ROC Curve:</p> <p><pre><code>acc_fig, loss_fig, conf_fig, roc_fig = trainer.plot_visualizations() \n</code></pre> Where <code>acc_fig</code> is Accuracy Curve figure object, <code>loss_fig</code> is Loss Curve figure object, <code>conf_fig</code> is the Confusion Matrix figure object, and <code>roc_fig</code> is the ROC Curve figure object. </p> </li> <li> <p>After your model is trained, you will have three files for it (paths to which you mentioned in the <code>training_config.yaml</code>):         i. <code>model_pth</code> : Path to your model. Let us assume it is named <code>model_new_schema.pth</code>.         ii. <code>label_encoder_pth</code>: Path to the Label Encoder. Let us assume it is named <code>label_encoder_new_schema.pkl</code>.         iii. <code>vectorizer_pth</code>: Path to the Vectorizer. Let us assume it is named <code>vectorizer_new_schema.pkl</code>.     Upload these files to your HuggingFace repository in the directory you had made earlier <code>new_repo/new_schema</code>.     Now, your HuggingFace repository would look something like this:</p> <pre><code>new_repo/\n    new_schema/\n        new_schema_design.yaml #This has the schema design defining the attributes with their data types and descriptions\n        model_new_schema.pth \n        label_encoder_new_schema.pkl\n        vectorizer.pkl\n</code></pre> </li> <li> <p>Yay, you're just one step away from standardizing metadata according to your custom schema. You would need to add a config file with the parameters you trained your model on to the <code>new_schema/</code> directory. Name this config file as <code>config_new_schema.yaml</code>. The config file should have the following keys:     <pre><code>params:\ninput_size_bow: None\nembedding_size: None\nhidden_size: None\noutput_size: None\ndropout_prob: None\n</code></pre>     Provide the values that you trained your model on. Now, the completely trained repository should have the following structure:</p> <p><pre><code>new_repo/\n    new_schema/\n        new_schema_design.yaml #This has the schema design defining the attributes with their data types and descriptions\n        model_new_schema.pth \n        label_encoder_new_schema.pkl\n        vectorizer.pkl\n        config_new_schema.yaml\n</code></pre> Before moving on to standardization, confirm that all the above files are present in your repository.</p> </li> </ol>"},{"location":"bedboss/tutorials/bedms_tutorial/#standardizing-on-custom-schema-models","title":"Standardizing on custom schema models","text":"<p>For standardizing on custom schema model, instantiate <code>AttrStandardizer</code> and provide the repo_id:</p> <pre><code>from bedms import AttrStandardizer\n\nmodel = AttrStandardizer(\n    repo_id=\"new_repo\", model_name=\"new_schema\"\n)\nresults = model.standardize(pep=\"geo/gse228634:default\")\n\nprint(results) #Dictionary of suggested predictions with their confidence: {'attr_1':{'prediction_1': 0.70, 'prediction_2':0.30}}\n</code></pre>"},{"location":"bedboss/tutorials/bedqc_tutorial/","title":"BEDqc tutorial","text":""},{"location":"bedboss/tutorials/bedqc_tutorial/#bedqc","title":"BEDqc","text":"<p>BEDqc is a tool for quality control of BED files.  As for now, it checks: - maximum file size,  - maximum number of regions,  - minimum region width threshold</p>"},{"location":"bedboss/tutorials/bedqc_tutorial/#run-bedqc-from-command-line","title":"Run BEDqc from command line","text":"<pre><code>bedboss qc \\\n    --bed-file path/to/bedfile.bed \\\n    --outfolder path/to/output/dir\n</code></pre> <p>Run BEDqc from within Python <pre><code>from bedboss import bedqc\n\nbedqc.run_bedqc(\n    bedfile=\"path/to/bedfile.bed\",\n    outfolder=\"path/to/output/dir\"\n    max_file_size=1000000, # optional\n    max_number_of_regions=1000, # optional\n    min_region_width=10, # optional\n)\n</code></pre></p> <p>If file won't pass the quality control, it will raise an error. and add this information to the log file.</p>"},{"location":"bedboss/tutorials/bedstat_tutorial/","title":"BEDstats","text":"<p>BEDstats is a tool that calculates the statistics of a BED file and provides plots to visualize the results.</p> <p>It produces BED file Statistics:</p> <ul> <li>GC content.The average GC content of the region set. </li> <li>Number of regions. The total number of regions in the BED file. </li> <li>Median TSS distance. The median absolute distance to the Transcription Start Sites (TSS)</li> <li>Mean region width. The average region width of the region set.</li> <li>Exon percentage.  The percentage of the regions in the BED file that are annotated as exon. </li> <li>Intron percentage.    The percentage of the regions in the BED file that are annotated as intron.</li> <li>Promoter proc percentage. The percentage of the regions in the BED file that are annotated as promoter-prox.</li> <li>Intergenic percentage. The percentage of the regions in the BED file that are annotated as intergenic.</li> <li>Promoter core percentage. The percentage of the regions in the BED file that are annotated as promoter-core.</li> <li>5' UTR percentage. The percentage of the regions in the BED file that are annotated as 5'-UTR.</li> <li>3' UTR percentage. The percentage of the regions in the BED file that are annotated as 3'-UTR.</li> </ul>"},{"location":"bedboss/tutorials/bedstat_tutorial/#step-1-install-all-dependencies","title":"Step 1: Install all dependencies","text":"<p>First you have to install bedboss and check if all requirements are satisfied.  To do so, you can run next command: <pre><code>bedboss check-requirements\n</code></pre> If requirements are not satisfied, you will see the list of missing packages.</p>"},{"location":"bedboss/tutorials/bedstat_tutorial/#step-2-run-bedstats","title":"Step 2: Run bedstats","text":""},{"location":"bedboss/tutorials/bedstat_tutorial/#run-bedstats-from-command-line","title":"Run BEDstats from command line","text":"<pre><code>bedboss stats \\\n    --bedfile path/to/bedfile.bed \\\n    --outfolder path/to/output/dir \\\n    --genome hg38 \\\n</code></pre>"},{"location":"bedboss/tutorials/bedstat_tutorial/#run-bedstats-from-within-python","title":"Run BEDstats from within Python","text":"<pre><code>from bedboss import bedstats\n\nbedstat(\n    bedfile=\"path/to/bedfile.bed\",\n    outfolder=\"path/to/output/dir\",\n    genome=\"hg19\",\n    )\n</code></pre> <p>After running BEDstats, you will find the following files in the output directory + all statistics will be saved in output file.</p>"},{"location":"bedboss/tutorials/tutorial_all/","title":"BEDboss-all pipeline","text":""},{"location":"bedboss/tutorials/tutorial_all/#bedboss-run-all","title":"Bedboss run-all","text":"<p>Bedboss run-all is intended to run on ONE sample (bed file) and run all bedboss pipelines:  bedmaker (+ bedclassifier + bedqc) -&gt; bedstat. After that optionally it can run bedbuncher, qdrant indexing and upload metadata to PEPhub.</p>"},{"location":"bedboss/tutorials/tutorial_all/#step-1-install-all-dependencies","title":"Step 1: Install all dependencies","text":"<p>First you have to install bedboss and check if all requirements are satisfied.  To do so, you can run next command: <pre><code>bedboss check-requirements\n</code></pre> If requirements are not satisfied, you will see the list of missing packages.</p>"},{"location":"bedboss/tutorials/tutorial_all/#step-2-create-bedconfyaml-file","title":"Step 2: Create bedconf.yaml file","text":"<p>To run bedboss, you need to create a bedconf.yaml file with configuration.  Detail instructions are in the configuration section.</p>"},{"location":"bedboss/tutorials/tutorial_all/#step-3-run-bedboss","title":"Step 3: Run bedboss","text":"<p>To run bedboss, you need to run the next command: <pre><code>bedboss all \\\n    --bedbase-config bedconf.yaml \\\n    --input-file path/to/bedfile.bed \\\n    --outfolder path/to/output/dir \\\n    --input-type bed \\\n    --genome hg38 \\\n</code></pre></p> <p>Above command will run bedboss on the bed file and create a bedstat file in the output directory. It contains only required parameters. For more details, please check the usage section.</p> <p>By default, results will be uploaded only to the PostgreSQL database.</p> <ul> <li>To upload results to PEPhub, you need to make the <code>databio</code> org available on GitHub, then login to PEPhub, and add the <code>--upload-pephub</code> flag to the command.</li> <li>To upload results to Qdrant, you need to add the <code>--upload-qdrant</code> flag to the command.</li> <li>To upload actual files to S3, you need to add the <code>--upload-s3</code> flag to the command, and before uploading, you have to set up all necessary environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_ENDPOINT_URL.</li> <li>To ignore errors and continue processing, you need to add the <code>--no-fail</code> flag to the command.</li> </ul>"},{"location":"bedboss/tutorials/tutorial_all/#run-bedboss-all-from-within-python","title":"Run bedboss all from within Python","text":"<p>To run bedboss all from within Python, instead of using the command line in the step #3, you can use the following code:</p> <pre><code>from bedboss import bedboss\n\nbedboss.run_all(\n    name=\"sample1\",\n    input_file=\"path/to/bedfile.bed\",\n    input_type=\"bed\",\n    outfolder=\"path/to/output/dir\",\n    genome=\"hg38\",\n    bedbase_config=\"bedconf.yaml\",\n    other_metadata=None, # optional\n    upload_pephub=True, # optional\n    upload_qdrant=True, # optional\n    upload_s3=True, # optional\n)\n</code></pre>"},{"location":"bedboss/tutorials/tutorial_run_pep/","title":"BEDboss run-pep","text":""},{"location":"bedboss/tutorials/tutorial_run_pep/#bedboss-run-pep","title":"Bedboss run-pep","text":"<p>Bedboss insert is designed to process each sample in the provided PEP.  The PEP can be provided either as a path to config file or as a registry path of the PEPhub.</p>"},{"location":"bedboss/tutorials/tutorial_run_pep/#step-1-install-all-dependencies","title":"Step 1: Install all dependencies","text":"<p>First, you have to install bedboss and check if all requirements are satisfied.  To do so, you can run the following command: <pre><code>bedboss check-requirements\n</code></pre> If requirements are not satisfied, you will see the list of missing packages.</p>"},{"location":"bedboss/tutorials/tutorial_run_pep/#step-2-create-bedconfyaml-file","title":"Step 2: Create bedconf.yaml file","text":"<p>To run bedboss run-pep, you need to create a bedconf.yaml file with configuration.  Detailed instructions are in the configuration section.</p>"},{"location":"bedboss/tutorials/tutorial_run_pep/#step-3-create-pep-with-bed-files","title":"Step 3: Create PEP with bed files.","text":"<p>BEDboss PEP should contain next fields: sample_name, input_file, input_type, genome. Before running bedboss, you need to validate provided PEP with bedboss_insert schema. The easiest way to do so is to use PEPhub, where you create a new PEP and validate it with the schema. Example PEP: https://pephub.databio.org/databio/excluderanges?tag=default</p>"},{"location":"bedboss/tutorials/tutorial_run_pep/#step-4-run-bedboss-insert","title":"Step 4: Run bedboss insert","text":"<p>To run bedboss insert , you need to run the next command: <pre><code>bedboss insert \\\n    --bedbase-config bedconf.yaml \\\n    --pep path/to/pep.yaml \\\n    --outfolder path/to/output/dir\n</code></pre></p> <p>Above command will run bedboss on the bed file and create a file with statistics in the output directory.  It contains only required parameters. For more details, please check the usage section.</p> <p>By default, results will be uploaded only to the PostgreSQL database.</p> <ul> <li>To upload results to PEPhub, you need to make the <code>databio</code> org available on GitHub, then login to PEPhub, and add the <code>--upload-pephub</code> flag to the command.</li> <li>To upload results to Qdrant, you need to add the <code>--upload-qdrant</code> flag to the command.</li> <li>To upload actual files to S3, you need to add the <code>--upload-s3</code> flag to the command, and before uploading, you have to set up all necessary environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_ENDPOINT_URL.</li> <li>To create a bedset of provided pep files, you need to add the <code>--create-bedset</code> flag to the command.</li> <li>To ignore errors and continue processing, you need to add the <code>--no-fail</code> flag to the command.</li> </ul>"},{"location":"bedboss/tutorials/tutorial_run_pep/#run-bedboss-insert-from-within-python","title":"Run bedboss insert from within Python","text":"<p>To run bedboss insert from within Python, instead of using the command line in the step #4, you can use the following code:</p> <pre><code>from bedboss import bedboss\n\nbedboss.insert_pep(\n    bedbase_config=\"bedconf.yaml\",\n    pep=\"path/to/pep.yaml\",\n    output_folder=\"path/to/output/dir\",\n    upload_pephub=True, # optional\n    upload_qdrant=True, # optional\n    upload_s3=True, # optional\n    create_bedset=True, # optional\n    no_fail=True, # optional\n)\n</code></pre>"},{"location":"bedhost/","title":"BEDhost overview","text":"bedhost <p><code>bedhost</code> is a Python FastAPI module for the API that powers BEDbase. It needs a path to the bedbase configuration file, which can be provided either via <code>-c</code>/<code>--config</code> argument or read from <code>$BEDBASE_CONFIG</code> environment variable. </p>"},{"location":"bedhost/#introduction","title":"Introduction","text":"<p>You can find the formal OpenAPI documentation and interactive interface at http://api.bedbase.org/docs. This document provides more conceptual introduction and explanations to how to use the API effectively.</p>"},{"location":"bedhost/#general-api-organization","title":"General API organization","text":""},{"location":"bedhost/#data-types","title":"Data types","text":"<p>BEDbase stores two types of data, which we call records. They are 1. BEDs, and 2. BEDsets. BEDsets are simply collections of BEDs. Each record in the database is either a BED or a BEDset.</p>"},{"location":"bedhost/#endpoint-organization","title":"Endpoint organization","text":"<p>The endpoints are divided into 3 groups:</p> <ol> <li><code>/bed</code> endpoints are used to interact with metadata for BED records.</li> <li><code>/bedset</code> endpoints are used to interact with metadata for BEDset records.</li> <li><code>/objects</code> endpoints are used to download metadata and get URLs to retrieve the underlying data itself. These endpoints implement the GA4GH DRS standard.</li> </ol> <p>Therefore, to get information and statistics about BED or BEDset records, or what is contained in the database, look through the <code>/bed</code> and <code>/bedset</code> endpoints. But if you need to write a tool that gets the actual underlying files, then you'll need to use the <code>/objects</code> endpoints. The type of identifiers used in each case differ.</p>"},{"location":"bedhost/#record-identifiers-vs-object-identifiers","title":"Record identifiers vs. object identifiers","text":"<p>Each record has an identifier. For example, <code>eaf9ee97241f300f1c7e76e1f945141f</code> is a BED identifier. You can use this identifier for the metadata endpoints. To download files, you'll need something slightly different -- you need an object identifier. This is because each BED record includes multiple files, such as the original BED file, the BigBed file, analysis plots, and so on. To download a file, you will construct what we call the <code>object_id</code>, which identifies the specific file.</p>"},{"location":"bedhost/#how-to-construct-object-identifiers","title":"How to construct object identifiers","text":"<p>Object IDs take the form <code>&lt;record_type&gt;.&lt;record_identifier&gt;.&lt;result_id&gt;</code>. An example of an object_id for a BED file is <code>bed.eaf9ee97241f300f1c7e76e1f945141f.bedfile</code></p> <p>So, you can get information about this object like this:</p> <p><code>GET</code> https://api.bedbase.org/objects/bed.eaf9ee97241f300f1c7e76e1f945141f.bedfile</p> <p>Or, you can get a URL to download the actual file with:</p> <p><code>GET</code> https://api.bedbase.org/objects/bed.eaf9ee97241f300f1c7e76e1f945141f.bedfile/access/http</p>"},{"location":"bedhost/build_image/","title":"Build the container locally","text":"<p>Running with <code>uvicorn</code> provides auto-reload. To configure, this assumes you have previously set up <code>databio/secrets</code>. </p> <ol> <li>Source <code>.env</code> file to populate the environment variables referenced in the configuration file.</li> <li>Start <code>bedhost</code> using <code>uvicorn</code> and pass the configuration file via the <code>BEDBASE_CONFIG</code> env var.</li> </ol> <pre><code>source ../bedbase.org/environment/production.env\nBEDBASE_CONFIG=../bedbase.org/config/api.bedbase.org.yaml uvicorn bedhost.main:app --reload\n</code></pre> <p>You can change the database you're connecting to by using a different config file: - Using a local config: <code>BEDBASE_CONFIG=../bbconf/tests/data/config.yaml uvicorn bedhost.main:app --reload</code> - With new database: <code>BEDBASE_CONFIG=../bedbase.org/config/bedbase2.yaml uvicorn bedhost.main:app --reload</code></p> <p>Now, you can access the service at http://127.0.0.1:8000. Example endpoints: - http://127.0.0.1:8000/v1/bed/bbad85f21962bb8d972444f7f9a3a932/metadata?full=true - http://127.0.0.1:8000/v1/bed/bbad85f21962bb8d972444f7f9a3a932/metadata/plots?full=true - http://127.0.0.1:8000/v1/objects/bed.bbad85f21962bb8d972444f7f9a3a932.chrombins - http://127.0.0.1:8000/v1/bed/list?limit=10&amp;offset=0</p>"},{"location":"bedhost/build_image/#running-the-server-in-docker","title":"Running the server in Docker","text":""},{"location":"bedhost/build_image/#building-image","title":"Building image","text":"<ul> <li>Primary image: <code>docker build -t databio/bedhost -f .Dockerfile .</code></li> <li>Dev image <code>docker build -t databio/bedhost:dev -f dev.Dockerfile .</code></li> <li>Test image: <code>docker build -t databio/bedhost:dev -f test.Dockerfile .</code></li> </ul> <p>Existing images can be found at dockerhub.</p>"},{"location":"bedhost/build_image/#deploying-updates-automatically","title":"Deploying updates automatically","text":"<ul> <li>Deploying bedbase.</li> </ul>"},{"location":"bedhost/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format. </p>"},{"location":"bedhost/changelog/#040","title":"[0.4.0]","text":"<ul> <li>Support of new bbconf.</li> <li>Updated endpoints.</li> </ul>"},{"location":"bedhost/changelog/#030-2023-03-01","title":"[0.3.0] -- 2023-03-01","text":""},{"location":"bedhost/changelog/#change","title":"change","text":"<ul> <li>switch to pydantic2</li> <li>updated requirements</li> <li>updated docs</li> </ul>"},{"location":"bedhost/changelog/#020-2023-10-17","title":"[0.2.0] -- 2023-10-17","text":"<ul> <li>remove all graphql</li> <li>remove local static hosting of UI</li> <li>update to new pipestat-based bbconf (pending)</li> <li>major refactor of API that introduces backwards-incompatible changes</li> </ul>"},{"location":"bedhost/changelog/#013-2023-09-01","title":"[0.1.3] -- 2023-09-01","text":"<ul> <li>allow all origins</li> </ul>"},{"location":"bedhost/changelog/#012-2023-02-06","title":"[0.1.2] -- 2023-02-06","text":""},{"location":"bedhost/changelog/#change_1","title":"change","text":"<ul> <li>change <code>/bedset/my_bedset/file_paths</code>endpoint from GET to POST</li> </ul>"},{"location":"bedhost/changelog/#011-2021-10-30","title":"[0.1.1] -- 2021-10-30","text":""},{"location":"bedhost/changelog/#change_2","title":"change","text":"<ul> <li><code>/bed/genomes</code> and <code>bedset/genomes</code>: impove speed</li> </ul>"},{"location":"bedhost/changelog/#010-2021-10-25","title":"[0.1.0] -- 2021-10-25","text":""},{"location":"bedhost/changelog/#add","title":"add","text":"<ul> <li>GraphQL endpoints</li> </ul>"},{"location":"bedhost/changelog/#change_3","title":"change","text":"<ul> <li>endpoints update due to <code>bbconf</code> and <code>pipestat</code> changes</li> </ul>"},{"location":"bedhost/changelog/#006-2021-05-17","title":"[0.0.6] -- 2021-05-17","text":""},{"location":"bedhost/changelog/#add_1","title":"add","text":"<ul> <li>Add endpoints that serve:</li> <li>a list of genome assemblies in bedsets and bedfiles table</li> <li>bed files by search term(s)</li> <li>remote file path (http / s3)</li> </ul>"},{"location":"bedhost/changelog/#005-2021-04-15","title":"[0.0.5] -- 2021-04-15","text":""},{"location":"bedhost/changelog/#add_2","title":"add","text":"<ul> <li>Add examples of API endpoints</li> </ul>"},{"location":"bedhost/changelog/#fix","title":"fix","text":"<ul> <li>resolve <code>/about</code> page not found when typing/editing url in the address bar. </li> </ul>"},{"location":"bedhost/changelog/#004-2021-04-01","title":"[0.0.4] -- 2021-04-01","text":""},{"location":"bedhost/changelog/#add_3","title":"add","text":"<ul> <li>add endpoint for region-based query </li> </ul>"},{"location":"bedhost/changelog/#fix_1","title":"fix","text":"<ul> <li>constrauction of local file/img path</li> </ul>"},{"location":"bedhost/changelog/#003-2021-02-22","title":"[0.0.3] -- 2021-02-22","text":"<ul> <li>Initial project release</li> </ul>"},{"location":"bedhost/deployment/","title":"Deploying bedbase.org","text":"<p>This repository deploys the API for bedbase. It will run these services:</p> <ol> <li>production API: https://api.bedbase.org/</li> <li>dev API: https://api-dev.bedbase.org/</li> </ol> <p>This repo will deploy a new service by following these steps:</p> <ol> <li>Build an image by packaging the bedhost image (from dockerhub) with the bbconf file in this repository.</li> <li>Push that image to AWS.</li> <li>Deploy it to yeti cluster with aws task def.</li> </ol>"},{"location":"bedhost/deployment/#build-the-container","title":"Build the container","text":"<p>Here we use the <code>databio/bedhost</code> container on dockerhub, and just add the configuration file in this repo to it, so build is super fast.</p> <pre><code>docker build -t databio/bedhost-configured -f Dockerfiles/primary.Dockerfile .\n</code></pre> <p>Or for dev:</p> <pre><code>docker build -t databio/bedhost-configured-dev -f Dockerfiles/dev1.Dockerfile .\n</code></pre>"},{"location":"bedhost/deployment/#run-it-locally-to-test","title":"Run it locally to test","text":"<p>First, source the .env file to set env vars in the calling environment. Then, use <code>--env-file</code> to pass those env vars through to the container</p> <pre><code>source environment/production.env\ndocker run --rm --network=\"host\" \\\n  --env-file environment/docker.env \\\n  databio/bedhost-configured-dev\n</code></pre> <p>Here's another example for running the container:</p> <pre><code>docker run --rm --init -p 8000:8000 --name bedstat-rest-server \\\n  --network=\"host\" \\\n  --volume ~/code/bedbase.org/config/api.bedbase.org.yaml:/bedbase.yaml \\\n  --env-file ../bedbase.org/environment/docker.env \\\n  --env BEDBASE_CONFIG=/bedbase.yaml \\\n  databio/bedhost  uvicorn bedhost.main:app --reload\n</code></pre>"},{"location":"bedhost/deployment/#building-the-amazon-tagged-version","title":"Building the Amazon-tagged version","text":"<p>You could build and push to ECR like this if you need it... but the github action will do this for you.</p> <p>Authenticate with AWS ECR: <pre><code>aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 235728444054.dkr.ecr.us-east-1.amazonaws.com\n</code></pre></p> <p>Build/tag/push image: <pre><code>docker build -t 235728444054.dkr.ecr.us-east-1.amazonaws.com/bedhost -f Dockerfiles/primary.Dockerfile .\ndocker push 235728444054.dkr.ecr.us-east-1.amazonaws.com/bedhost\n</code></pre></p>"},{"location":"geniml/","title":"Geniml","text":""},{"location":"geniml/#introduction","title":"Introduction","text":"<p>Geniml is a genomic interval machine learning toolkit, a Python package for building machine learning models of genomic interval data (BED files). It also includes ancillary functions to support other types of analyses of genomic interval data.</p> <p>As of Feburary 2024, this package and its documentation are undergoing rapid development, leading to some tutorials getting outdated. Please raise github issues if you find outdated or unclear directions, so we know where to focus effort that will benefit users.</p>"},{"location":"geniml/#install","title":"Install","text":"<pre><code>pip install --user --upgrade geniml\n</code></pre>"},{"location":"geniml/#modules-and-resources","title":"Modules and resources","text":""},{"location":"geniml/#organization","title":"Organization","text":"<p><code>geniml</code> is organized into modules. The modules section gives an overview of each module.</p>"},{"location":"geniml/#browsing-by-publication","title":"Browsing by publication","text":"<p>If you're coming here from a manuscript, you might find it easier to identify the tutorials relevant for a particular manuscript by visiting the landing page for the publication of interest. You can find documentation organized by manuscript in the manuscripts section.</p>"},{"location":"geniml/changelog/","title":"Changelog","text":"<p>This project adheres to Semantic Versioning and Keep a Changelog format. </p>"},{"location":"geniml/changelog/#040-2024-06-04","title":"[0.4.0] -- 2024-06-04","text":"<ul> <li>Added bed tokens caching to bbclient [bbclient] Add tokenized file cache and download #153</li> <li>Added pyBiocFileCache for bedfiles to support R caching [bbclient] Integrate bedbase caching with R #151</li> <li>Added support of Python3.12</li> <li>Optimized encoding of regions for Region2Vec models</li> <li>Added updates to the new Atacformer</li> <li>Renamed tokenizers to TreeTokenizer and AnnDataTokenizer</li> <li>Bump genimtools which includes performance upgrades and stability updates to the tokenizers</li> </ul>"},{"location":"geniml/changelog/#030-2024-04-04","title":"[0.3.0] -- 2024-04-04","text":"<ul> <li>Added S3 uploading to bbclient</li> <li>Added and cleaned tests</li> <li>Added bed2bed search interface</li> </ul>"},{"location":"geniml/changelog/#020-2024-02-20","title":"[0.2.0] -- 2024-02-20","text":"<ul> <li>Fixed a bug with pydantic</li> <li>Integrate <code>lightning</code> for easier training of models with SLURM and the DDP framework</li> <li>New datasets for streaming <code>.gtok</code> files to models</li> <li>New tutorials for updated ScEmbed</li> <li>Start working on <code>Atacformer</code></li> </ul>"},{"location":"geniml/changelog/#010-2023-12-18","title":"[0.1.0] -- 2023-12-18","text":"<ul> <li>First official released version of geniml</li> <li>Integrated bedshift 1.1.1 into geniml (excluding intersection support)</li> </ul>"},{"location":"geniml/changelog/#bedshift-111-2021-04-15","title":"[bedshift 1.1.1] - 2021-04-15","text":"<ul> <li>Updated documentation</li> <li>Fixed dependencies packaging for building documentation</li> </ul>"},{"location":"geniml/changelog/#bedshift-110-2021-04-02","title":"[bedshift 1.1.0] - 2021-04-02","text":"<ul> <li>Added ability to specify chrom sizes file, or refgenie genome.</li> <li>Add perturbation will create regions proportional to chromosome size</li> <li>Improve performance of perturbations</li> <li>Add --dropfile, --addfile, and --shiftfile options</li> <li>Add --add_valid option</li> <li>Add --yaml-config option</li> <li>Improved testing framework</li> <li>Improved logging messages</li> </ul>"},{"location":"geniml/changelog/#bedshift-100-2020-05-20","title":"[bedshift 1.0.0] - 2020-05-20","text":"<ul> <li>Add, shift, drop, cut, and merge perturbations.</li> <li>Basic documentation</li> <li>Basic tests</li> </ul>"},{"location":"geniml/contributing/","title":"Contributor guide","text":""},{"location":"geniml/contributing/#repository-organization","title":"Repository organization","text":"<p>This repo is divided into modules that provide related functions, each in a subfolder. The parent <code>geniml/</code> folder holds functionality that spans across modules, such as the general (top-level) argument parser (in <code>cli.py</code>), constants (<code>const.py</code>), and any other utility or general-purpose code that is used across modules.</p>"},{"location":"geniml/contributing/#using-modules-from-python","title":"Using modules from Python","text":"<p>This repo is divided into modules. Each module should be written in a way that it provides utility as a Python library. For example, you can call functions in the <code>hmm</code> module like this:</p> <pre><code>import geniml\n\ngeniml.hmm.function()\n</code></pre>"},{"location":"geniml/contributing/#command-line-interfaces","title":"Command-line interfaces","text":"<p>In addition to being importable from Python, some modules also provide a CLI. For these, developers provide a subcommand for CLI use. The root <code>geniml</code> package provides a generalized command-line interface with the command <code>geniml</code>. The modules that provide CLIs then correspond to CLI commands, e.g <code>geniml hmm</code> or <code>geniml likelihood</code>, with the corresponding code contained within a sub-folder named after the model:</p> <pre><code>geniml &lt;module&gt; ...\n</code></pre> <p>This is implemented within each module folder with:</p> <ul> <li><code>geniml/&lt;module&gt;/cli.py</code> - defines the command-line interface and provides a subparser for this module's CLI command.</li> </ul>"},{"location":"geniml/contributing/#adding-a-new-module","title":"Adding a new module","text":"<p>To add functionality to geniml, you could add it to an existing module. Or, if no existing module fit, you could add a new module.</p>"},{"location":"geniml/contributing/#creating-your-module","title":"Creating your module","text":"<p>Each module should be written in a way that it provides utility as a Python library. Organize your module with these files:</p> <ul> <li><code>/docs/tutorials/&lt;module&gt;.md</code> - describes how to use the code</li> <li><code>/geniml/&lt;module&gt;/main.py</code>, and other <code>.py</code> files - functions that provide utility for this module.</li> </ul> <p>All the functions should be written to be useful via import, calling with <code>geniml.&lt;module&gt;.&lt;function&gt;</code>. For example:</p> <pre><code>import geniml\n\ngeniml.hmm.function()\n</code></pre>"},{"location":"geniml/contributing/#adding-your-module-to-geniml","title":"Adding your module to geniml","text":"<ol> <li>Put your module in a subfolder.</li> <li>Make sure to include a <code>__init__.py</code> so it's importable.</li> <li>Add it to list of packages in <code>setup.py</code>.</li> <li>If it makes sense to have a CLI for this module, implement it in <code>geniml/&lt;module_name&gt;/cli.py</code>. Link this into the main cli by putting it under an appropriate command name following the pattern for other modules in <code>geniml/cli.py</code>.</li> </ol>"},{"location":"geniml/contributing/#shared-code","title":"Shared code","text":"<p>Any variables, functions, or other code that is shared across modules should be placed in the parent module, which is held in the geniml folder.</p>"},{"location":"geniml/datasets/","title":"Genomic datasets with <code>geniml</code>","text":""},{"location":"geniml/datasets/#overview","title":"Overview","text":"<p>Genomic datasets are well known for their incredible size. Therefore, using these for machine learning pipelines requires clever strategies and considerations to effectively handle the large volumes of data. This is particularly problematic when training new models. To that end, <code>geniml</code> has two separate workflows for handling genomic data: one for model training and a second for model inference (using a pre-trained model). These workflows have big differences on when and where genomic datasets are tokenized and stored.</p>"},{"location":"geniml/datasets/#model-training","title":"Model training","text":"<p>Model training, especially pre-training, usually requires large datasets with billions of genomic tokens. These datasets are way too large to fit in memory and therefore must be streamed from disk during training. Because of this, the data must be tokenized \"on the fly\" for each epoch. This is wildly inefficient and for particularly large datasets, results in the majority of training time being dedicated to tokenization alone. Therefore, the data need be pre-tokenized into an intermediate form which can then be streamed in for each epoch. This removes tokenization entirely from the training procedure and therefore increase efficiency (Fig. 1A).</p>"},{"location":"geniml/datasets/#model-inference","title":"Model inference","text":"<p>Model inference is the process of utilizing a pre-trained model to analyze some new, unseen data. While the output of the model might vary (embeddings, label, new data), the input is always the same: genomic tokens. Except under rare circumstances, it's not typical that model inference involves large volumes of data. Therefore, pre-tokenization is not necessary. Because of this, data is to be tokenized in-memory and directly to the format required to pass through the model (Fig. 1B).</p>"},{"location":"geniml/datasets/#tokenization-forms","title":"Tokenization forms","text":"<p>Given the above requirements, tokenizers need to be able to output tokenized genomic data into different forms and locations. For model training: tokenizers should take either bed files or <code>.h5ad</code> single-cell datasets and convert them into an intermediary <code>.gtok</code> file format. These <code>.gtok</code> files will be directly consumed during model training. For model inference: tokenizers should take either bed files or <code>.h5ad</code> single-cell datasets and output an in-memory representation of these tokens; typically in the form of a <code>torch.Tensor</code> or python list. The following table summarizes the format, location, and scenario in which data is tokenized:</p>   |                 | Where  | What        | When              | | --------------- | --------- | ------------- | ----------------- | | Model training  | On disk   | `.gtok` files | Prior to training | | Model inference | In memory | `torch.Tensors`              | On the fly                  |"},{"location":"geniml/datasets/#datasets-in-geniml","title":"Datasets in <code>geniml</code>","text":"<p><code>geniml</code> uses <code>pytorch</code> + <code>lightning</code> to train models. This ecosystem encourages the use of <code>torch</code>s built-in <code>Dataset</code> class to parallelize and batch the loading of data. Because training and fine-tuning models requires pre-tokenized data (<code>.gtok</code> files), <code>geniml</code> needs datasets to handle this. It most likely will look like: <pre><code>from typing import List\n\nfrom torch.data.utils import IterableDataset\n\nclass PretokenizedDataset(IterableDataset):\n  def __init__(self, data: Union[str, List[str]):\n    self.data = data\n      if isinstance(data, str):\n        self._is_folder = True\n      elif isinstance(data, list) and isinstance(data[0], str):\n        self._is_folder = False\n      else:\n        raise ValueError(\"`data` must be a path to a folder or a list of `.gtok` files\")\n\n  def __iter__(self, indx: int):\n    if self._is_folder:\n      for file in os.listdir(self.data):\n        with open(file, 'r') as f:\n          for line in file.readlines():\n            yield line.split(\",\")\n    else:\n      for file in self.data:\n        with open(file, 'r') as f:\n          for line in file.readlines():\n            yield line.split(\",\")\n</code></pre> Here, we are no longer tokenizing each epoch, rather just streaming in data that has already been pre-tokenized. I still need to think about this in the context of fine-tuning and datasets that require targets and labels.</p>"},{"location":"geniml/datasets/#gtok-file-format","title":"<code>.gtok</code> file format","text":"<p>The <code>.gtok</code> file format is a binary file where each token is stored as a 32-bit integer. This allows tokens to be stored in a very compact format with the ability to represent up to 4 billion unique tokens. Using our companion package <code>genimtools</code>, we can convert <code>.bed</code> files into <code>.gtok</code> files after tokenization.</p>"},{"location":"geniml/manuscripts/","title":"Published manuscripts describing geniml components","text":"<p>If you find <code>geniml</code> useful for your research, please cite us! It helps us convince others that our work is useful. Here is a list of published papers that describe different modules or components in the <code>geniml</code> package.</p> <p>N. J. LeRoy et al., \u201cFast clustering and cell-type annotation of scATACdata with pre-trained embeddings,\u201d bioRxiv, 2023, doi: 10.1101/2023.08.01.551452.</p> <p>J. Rymuza et al., \u201cMethods for constructing and evaluating consensus genomic interval sets,\u201d bioRxiv, 2023, doi: 10.1101/2023.08.03.551899.</p> <p>E. Gharavi, N. J. LeRoy, G. Zheng, A. Zhang, D. E. Brown, and N. C. Sheffield, \u201cJoint representation learning for retrieval and annotation of genomic interval sets,\u201d bioRxiv, 2023, doi: 10.1101/2023.08.21.554131.</p> <p>G. Zheng et al., \u201cMethods for evaluating unsupervised vector representations of genomic regions,\u201d bioRxiv, 2023, doi: 10.1101/2023.08.03.551899.</p> <p>B. Xue, O. Khoroshevskyi, R. A. Gomez, and N. C. Sheffield, \u201cOpportunities and challenges in sharing and reusing genomic interval data,\u201d Frontiers in Genetics, vol. 14, 2023-03, doi: 10.3389/fgene.2023.1155809.</p> <p>E. Gharavi et al., \u201cEmbeddings of genomic region sets capture rich biological associations in low dimensions,\u201d Bioinformatics, 2021-03, doi: 10.1093/bioinformatics/btab439.</p>"},{"location":"geniml/modules/","title":"Module overviews","text":"<p><code>geniml</code> is organized into modules. Each module groups together related tasks. This document provides an overview of each module.</p>"},{"location":"geniml/modules/#module-assess-universe","title":"Module <code>assess-universe</code>","text":"<p>Many genomic interval analysis methods, particularly those used by <code>geniml</code> require that regions be re-defined in terms of a consensus region set, or universe. However, a universe may not be a good fit to a collection of files. This module assesses that fit. Given a collection of genomic interval sets, and a proposed universe, we can assess how well the universe fits the genomic interval sets. This module provides several complementary methods to assess fit.</p>"},{"location":"geniml/modules/#module-bbclient","title":"Module <code>bbclient</code>","text":"<p>The <code>bbclient</code> module can download BED files or BED sets from BEDbase and cache them into local folders.</p>"},{"location":"geniml/modules/#module-bedspace","title":"Module <code>bedspace</code>","text":"<p>The <code>bedspace</code> module uses the StarSpace method (Wu et al., 2018) to jointly embed genomic interval regions sets with associated metadata into a shared latent embedding space. This facilitates fast search and retrieval of similar region sets and their associated metadata. </p>"},{"location":"geniml/modules/#module-build-universe","title":"Module <code>build-universe</code>","text":"<p>This module provides multiple ways to build a genomic region universe. These include: 1. HMM: uses an HMM to create a flexible segment universe, given an input of several bed files.</p>"},{"location":"geniml/modules/#module-evaluation","title":"Module <code>evaluation</code>","text":"<p>Once a <code>geniml</code> region embedding model is trained, we may want to evaluate the embeddings. The <code>evaluation</code> module provides several functions for that. These include statistical tests, like the Cluster Tendency Test (CTT) and the Reconstruction Test (RCT), and biological tests, the Genome Distance Scaling Test (GDST) and the Neighborhood Preserving Test (NPT). These evaluation metrics can be helpful to determine if your models are working well, optimize training parameters, etc.</p>"},{"location":"geniml/modules/#module-region2vec","title":"Module <code>region2vec</code>","text":"<p><code>Region2Vec</code> is an unsupervised method for creating embeddings for genomic regions and region sets from a set of raw BED files. The program uses a variation of the word2vec algorithm by building shuffled context windows from BED files. The co-occurence statistics of genomic regions in a collection of BED files allow the model to learn region embeddings.</p>"},{"location":"geniml/modules/#module-scembed","title":"Module <code>scembed</code>","text":"<p><code>scEmbed</code> is a single-cell implementation of <code>region2Vec</code>: a method to represent genomic region sets as vectors, or embeddings, using an adapted word2vec approach. <code>scEmbed</code> allows for dimensionality reduction and feature selection of single-cell ATAC-seq data; a notoriously sparse and high-dimensional data type. We intend for <code>scEmbed</code> to be used with the <code>scanpy</code> package. As such, it natively accepts <code>AnnData</code> objects as input and returns <code>AnnData</code> objects as output.</p>"},{"location":"geniml/modules/#module-search","title":"Module <code>search</code>","text":"<p>The search module provides a generic interface for vector search. Several geniml modules (such as <code>region2vec</code>), will create embeddings for different entities. The search module provides interfaces that store vectors and perform fast k-nearest neighbors (KNN) search with a given query vector.  Back-end options include a database backend (using <code>qdrant-client</code>) and  local file backend (using <code>hnswlib</code>.</p>"},{"location":"geniml/modules/#module-text2bednn","title":"Module <code>text2bednn</code>","text":"<p><code>Vec2Vec</code> is a feedforward neural network that maps vectors from the embedding space of natural language (such as embeddings created by <code>fastembed</code>) to the embedding space of BED (such as embeddings created by <code>Region2Vec</code>). By mapping the embedding of natural language query strings to the space of BED files, <code>Vec2Vec</code> can perform natural language search of BED files. </p>"},{"location":"geniml/modules/#module-tokenization","title":"Module <code>tokenization</code>","text":"<p>In NLP, training word embeddings requires first tokenizing words such that words in different forms are represented by one word. For example, \"orange\", \"oranges\" and \"Orange\" are all mapped to \"orange\" since they essentially convey the same meaning. This reduces the vocabulary size and improves the quality of learned embeddings. Similary, many <code>geniml</code> modules (such as <code>region2vec</code>) require first tokenizating regions.</p> <p>To tokenize reigons, we need to provide a universe, which specifies the \"vocabulary\" of genomic regions. The universe is a BED file, containing representative regions. With the given universe, we represent (tokenize) raw regions into the regions in the universe.</p> <p>Different strategies can be used to tokenize. The simplest case we call hard tokenization, which means if the overlap between a raw region in a BED file and a region in the universe exceeds a certain amount, then we use the region in the universe to represent this raw region; otherwise, we ignore this raw region. This is a \"zero or one\" process. After hard tokenization, each BED file will contain only regions from the universe, and the number of regions will be smaller or equal to the original number.</p>"},{"location":"geniml/support/","title":"Support","text":"<p>Please raise any issues or questions using the GitHub issue tracker.</p>"},{"location":"geniml/autodoc_build/bedshift/","title":"Bedshift","text":""},{"location":"geniml/autodoc_build/bedshift/#package-bedshift-documentation","title":"Package <code>bedshift</code> Documentation","text":""},{"location":"geniml/autodoc_build/bedshift/#class-bedshift","title":"Class <code>Bedshift</code>","text":"<p>The bedshift object with methods to perturb regions</p> <pre><code>def __init__(self, bedfile_path, chrom_sizes, delimiter='\\t')\n</code></pre> <p>Read in a .bed file to pandas DataFrame format</p>"},{"location":"geniml/autodoc_build/bedshift/#parameters","title":"Parameters:","text":"<ul> <li><code>bedfile_path</code> (<code>str</code>):  the path to the BED file</li> <li><code>chrom_sizes</code> (<code>str</code>):  the path to the chrom.sizes file</li> <li><code>delimiter</code> (<code>str</code>):  the delimiter used in the BED file</li> </ul> <pre><code>def add(self, addrate, addmean, addstdev)\n</code></pre> <p>Add regions</p>"},{"location":"geniml/autodoc_build/bedshift/#parameters_1","title":"Parameters:","text":"<ul> <li><code>addrate</code> (<code>float</code>):  the rate to add regions</li> <li><code>addmean</code> (<code>float</code>):  the mean length of added regions</li> <li><code>addstdev</code> (<code>float</code>):  the standard deviation of the length of added regions</li> </ul>"},{"location":"geniml/autodoc_build/bedshift/#returns","title":"Returns:","text":"<ul> <li><code>int</code>:  the number of regions added</li> </ul> <pre><code>def add_from_file(self, fp, addrate, delimiter='\\t')\n</code></pre> <p>Add regions from another bedfile to this perturbed bedfile</p>"},{"location":"geniml/autodoc_build/bedshift/#parameters_2","title":"Parameters:","text":"<ul> <li><code>addrate</code> (<code>float</code>):  the rate to add regions</li> <li><code>fp</code> (<code>str</code>):  the filepath to the other bedfile</li> </ul>"},{"location":"geniml/autodoc_build/bedshift/#returns_1","title":"Returns:","text":"<ul> <li><code>int</code>:  the number of regions added</li> </ul> <pre><code>def all_perturbations(self, addrate=0.0, addmean=320.0, addstdev=30.0, addfile=None, shiftrate=0.0, shiftmean=0.0, shiftstdev=150.0, cutrate=0.0, mergerate=0.0, droprate=0.0)\n</code></pre> <p>Perform all five perturbations in the order of shift, add, cut, merge, drop.</p>"},{"location":"geniml/autodoc_build/bedshift/#parameters_3","title":"Parameters:","text":"<ul> <li><code>addrate</code> (<code>float</code>):  the rate (as a proportion of the total number of regions) to add regions</li> <li><code>addmean</code> (<code>float</code>):  the mean length of added regions</li> <li><code>addstdev</code> (<code>float</code>):  the standard deviation of the length of added regions</li> <li><code>shiftrate</code> (<code>float</code>):  the rate to shift regions (both the start and end are shifted by the same amount)</li> <li><code>shiftmean</code> (<code>float</code>):  the mean shift distance</li> <li><code>shiftstdev</code> (<code>float</code>):  the standard deviation of the shift distance</li> <li><code>cutrate</code> (<code>float</code>):  the rate to cut regions into two separate regions</li> <li><code>mergerate</code> (<code>float</code>):  the rate to merge two regions into one</li> <li><code>droprate</code> (<code>float</code>):  the rate to drop/remove regions</li> </ul>"},{"location":"geniml/autodoc_build/bedshift/#returns_2","title":"Returns:","text":"<ul> <li><code>int</code>:  the number of total regions perturbed</li> </ul> <pre><code>def cut(self, cutrate)\n</code></pre> <p>Cut regions to create two new regions</p>"},{"location":"geniml/autodoc_build/bedshift/#parameters_4","title":"Parameters:","text":"<ul> <li><code>cutrate</code> (<code>float</code>):  the rate to cut regions into two separate regions</li> </ul>"},{"location":"geniml/autodoc_build/bedshift/#returns_3","title":"Returns:","text":"<ul> <li><code>int</code>:  the number of regions cut</li> </ul> <pre><code>def drop(self, droprate)\n</code></pre> <p>Drop regions</p>"},{"location":"geniml/autodoc_build/bedshift/#parameters_5","title":"Parameters:","text":"<ul> <li><code>droprate</code> (<code>float</code>):  the rate to drop/remove regions</li> </ul>"},{"location":"geniml/autodoc_build/bedshift/#returns_4","title":"Returns:","text":"<ul> <li><code>int</code>:  the number of rows dropped</li> </ul> <pre><code>def merge(self, mergerate)\n</code></pre> <p>Merge two regions into one new region</p>"},{"location":"geniml/autodoc_build/bedshift/#parameters_6","title":"Parameters:","text":"<ul> <li><code>mergerate</code> (<code>float</code>):  the rate to merge two regions into one</li> </ul>"},{"location":"geniml/autodoc_build/bedshift/#returns_5","title":"Returns:","text":"<ul> <li><code>int</code>:  number of regions merged</li> </ul> <pre><code>def pick_random_chrom(self)\n</code></pre> <p>Utility function to pick a random chromosome</p>"},{"location":"geniml/autodoc_build/bedshift/#returns_6","title":"Returns:","text":"<ul> <li><code>str, float chrom_str, chrom_len</code>:  chromosome number and length</li> </ul> <pre><code>def read_bed(self, bedfile_path, delimiter='\\t')\n</code></pre> <p>Read a BED file into pandas dataframe</p>"},{"location":"geniml/autodoc_build/bedshift/#parameters_7","title":"Parameters:","text":"<ul> <li><code>bedfile_path</code> (<code>str</code>):  The path to the BED file</li> </ul> <pre><code>def reset_bed(self)\n</code></pre> <p>Reset the stored bedfile to the state before perturbations</p> <pre><code>def shift(self, shiftrate, shiftmean, shiftstdev)\n</code></pre> <p>Shift regions</p>"},{"location":"geniml/autodoc_build/bedshift/#parameters_8","title":"Parameters:","text":"<ul> <li><code>shiftrate</code> (<code>float</code>):  the rate to shift regions (both the start and end are shifted by the same amount)</li> <li><code>shiftmean</code> (<code>float</code>):  the mean shift distance</li> <li><code>shiftstdev</code> (<code>float</code>):  the standard deviation of the shift distance</li> </ul>"},{"location":"geniml/autodoc_build/bedshift/#returns_7","title":"Returns:","text":"<ul> <li><code>int</code>:  the number of regions shifted</li> </ul> <pre><code>def to_bed(self, outfile_name)\n</code></pre> <p>Write a pandas dataframe back into BED file format</p>"},{"location":"geniml/autodoc_build/bedshift/#parameters_9","title":"Parameters:","text":"<ul> <li><code>outfile_name</code> (<code>str</code>):  The name of the output BED file</li> </ul> <p>Version Information: <code>bedshift</code> v1.1.0-dev, generated by <code>lucidoc</code> v0.4.2</p>"},{"location":"geniml/autodoc_build/geniml/","title":"API documentation","text":""},{"location":"geniml/autodoc_build/geniml/#package-geniml-documentation","title":"Package <code>geniml</code> Documentation","text":"<p>Version Information: <code>geniml</code> v0.0.1-dev, generated by <code>lucidoc</code> v0.4.3</p>"},{"location":"geniml/code/assess-universe/","title":"Assess universe fit","text":"assess-universe In\u00a0[1]: <pre>from geniml.assess.assess import get_rbs_from_assessment_file, get_f_10_score_from_assessment_file\nimport pandas as pd\n\nassessment_file_path = \"test_assess_data.csv\"\ndf = pd.read_csv(assessment_file_path)\ndf.head()\n</pre> Out[1]: file univers/file file/universe universe&amp;file median_dist_file_to_universe median_dist_file_to_universe_flex median_dist_universe_to_file median_dist_universe_to_file_flex 0 test_1.bed 2506 403 3630 27.0 0.0 76.5 0.0 1 test_2.bed 1803 146 4333 27.0 0.0 70.0 7.5 2 test_3.bed 2949 0 3187 28.0 0.0 225.0 224.5 3 test_4.bed 2071 546 4065 27.0 0.0 116.5 105.5 In\u00a0[2]: <pre>rbs = get_rbs_from_assessment_file(assessment_file_path)\nf_10 = get_f_10_score_from_assessment_file(assessment_file_path)\nrbs_flex = get_rbs_from_assessment_file(assessment_file_path, flexible=True)\nprint(f\"Universe \\nF10: {f_10:.2f}\\nRBS: {rbs:.2f}\\nflexible RBS: {rbs_flex:.2f}\")\n</pre> <pre>Universe \nF10: 0.93\nRBS: 0.77\nflexible RBS: 0.98\n</pre> <p>Or all of this metrics can be directly calculated from the universe and raw files including a likelihood score (LH):</p> In\u00a0[\u00a0]: <pre>from geniml.assess.assess import get_f_10_score\n\nf10 = get_f_10_score(\n    \"raw/\",\n    'file_list.txt',\n    \"universe_hmm.bed\",\n    1)\n\nf\"Universe F10: {f10:.2f}\"\n</pre> Out[\u00a0]: <pre>'Universe F10: 0.93'</pre> In\u00a0[\u00a0]: <pre>from geniml.assess.assess import get_mean_rbs\nrbs = get_mean_rbs(\"raw/\",\n    'file_list.txt',\n    \"universe_hmm.bed\", 1)\nf\"Universe RBS: {rbs:.2f}\"\n</pre> Out[\u00a0]: <pre>'Universe RBS: 0.77'</pre> In\u00a0[\u00a0]: <pre>from geniml.assess.assess import get_likelihood\nlh = get_likelihood(\n    \"model.tar\",\n    \"universe_hmm.bed\",\n    \"coverage/\"\n)\nf\"Universe LH: {lh:.2f}\" \n</pre> Out[\u00a0]: <pre>'Universe LH: -127156.87'</pre> <p>Both region boundary score and likelihood can be also calculated taking into account universe flexibility:</p> In\u00a0[\u00a0]: <pre>from geniml.assess.assess import get_mean_rbs\nrbs_flex = get_mean_rbs(\n    \"raw/\",\n    'file_list.txt',\n    \"universe_hmm.bed\",\n    1,\n    flexible=True)\nf\"Universe flexible RBS: {rbs_flex:.2f}\"\n</pre> Out[\u00a0]: <pre>'Universe flexible RBS: 0.98'</pre> In\u00a0[\u00a0]: <pre>lh_flex = get_likelihood(\n    \"model.tar\",\n    \"universe_hmm.bed\",\n    \"coverage/\"\n)\nf\"Universe flexible LH: {lh_flex:.2f}\" \n</pre> Out[\u00a0]: <pre>'Universe flexible LH: -127156.87'</pre>"},{"location":"geniml/code/assess-universe/#How-to-assess-universe-fit-to-collection-of-BED-files","title":"How to assess universe fit to collection of BED files\u00b6","text":""},{"location":"geniml/code/assess-universe/#Introduction","title":"Introduction\u00b6","text":"<p>In this tutorial, you will see how to assess a fit of a given universe to a collection of files. (Tutorial on creating different universes from files can be found here and here.) Choosing, which universe represents data the best can be challenging. To help with this decision we created three different metrics for assessing universe fit to the region collections: a base-level overlap score, a region boundary score, and a likelihood score. Fit of a universe can be assessed both using CLI and python functions depending on use case. With CLI you can create a file with values of universe assessment methods for each file within the collection, while with python functions you can get measures of universe fit to the whole collection.</p>"},{"location":"geniml/code/assess-universe/#CLI","title":"CLI\u00b6","text":"<p>Using CLI you can calculate both base-level overlap score and region boundary score separately for each file in the collections and than summarized. To calculate them you need raw files as well as the analyzed universe. It is also necessary to choose at least one assessment metric to be calculated:</p> <ul> <li><code>--overlap</code> - to calculate base pair overlap between universe and regions in the file, number of base pair only in the universe, number of base pair only in the file, which can be used to calculate F10 score; </li> <li><code>--distance</code> - to calculate median of distance form regions in the raw file to the universe;</li> <li><code>--distance-universe-to-file</code> - to calculate median of distance form the universe to regions in the raw file;</li> <li><code>--distance-flexible</code> - to calculate median of distance form regions in the raw file to the universe taking into account universe flexibility;</li> <li><code>--distance-flexible-universe-to-file</code> - - to calculate median of distance form the universe to regions in the raw file taking into account universe flexibility.  </li> </ul> <p>Here we present an example, which calculates all possible metrics for HMM universe:</p> <pre><code> geniml assess-universe --raw-data-folder raw/ \\\n --file-list file_list.txt \\\n --universe universe_hmm.bed \\\n --folder-out . \\\n --pref test_assess \\\n --overlap \\\n --distance \\\n --distance-universe-to-file \\\n --distance-flexible \\\n --distance-flexible-universe-to-file</code></pre> <p>The resulting file is called test_assess_data.csv, and contains columns with the raw calculated metrics for each file: file, univers/file, file/universe, universe&amp;file, median_dist_file_to_universe, median_dist_file_to_universe_flex, median_dist_universe_to_file, median_dist_universe_to_file_flex.</p>"},{"location":"geniml/code/assess-universe/#Python-functions","title":"Python functions\u00b6","text":"<p>The file created with CLI can be further summarized into specific metrics assessing the fit of a universe to a whole collection such as: a base-level overlap score (F10), a region boundary distance score (RBD).</p>"},{"location":"geniml/code/bedspace-analysis/","title":"BEDspace results notebook","text":"bedspace-analysis In\u00a0[1]: <pre>import pandas as pd\nimport glob\nimport numpy as np\nimport re\n</pre> In\u00a0[2]: <pre>import matplotlib\nmatplotlib.rcParams[\"svg.fonttype\"] = \"none\"\nmatplotlib.rcParams[\"text.usetex\"] = False\nimport matplotlib.pyplot as plt\n</pre> In\u00a0[3]: <pre># Path to the pre-calcualted distance file between label embedding and region set embeddings\npath_simfile = './distance_l2r.csv'\ndistance = pd.read_csv(path_simfile)\ndistance.file_label = distance.file_label.str.lower()\ndistance.search_term = distance.search_term.str.lower()\ndistance = distance.drop_duplicates()\n</pre> In\u00a0[4]: <pre># Print the search terms (labels)\nprint(distance.search_term.unique())\n</pre> <pre>['h3k4me3' 'h3k27me3' 'h3k27ac' 'h3k4me1' 'h3k9me3' 'h3k4me2' 'h3k9ac'\n 'h3k79me2' 'h4k20me1' 'h3k9me2' 'h3k9me1']\n</pre> In\u00a0[5]: <pre>def S1(searchterm, distance):\n    nof = len(distance[distance.file_label.str.contains(searchterm)])\n    df = distance[distance.search_term == searchterm].sort_values(by=['score'], ascending = False)[0:10]\n    df = df.sort_values(by=['score'], ascending=True)\n    df['color']='gray'\n    df.loc[df.file_label.str.contains(searchterm), 'color'] = 'green'\n    if(len(df[df.color == 'green']) == nof):\n        df.loc[(df.color!='green'), 'color'] = 'gray'\n\n    plt= df.plot.barh(x='filename', y='score', figsize=(10,7), fontsize=16, color=list(df['color']))\n    plt.set_xlabel('Search term:' + searchterm, fontsize=15)\n\n    plt.axis(xmin=0.5, xmax=1.01)\n</pre> In\u00a0[6]: <pre>S1('h3k4me2', distance)\n</pre> In\u00a0[7]: <pre># Print a sample of filenames\n\nprint(list(set(distance.filename))[0:4])\n</pre> <pre>['gse124683/gsm3540312_k562_h3k4me2_rep1_20181202_icell8_21.bed.gz', 'gse175750/gsm5345532_kdm4a-oe.h3k9ac.ls.rep2.bed.gz', 'gse161624/gsm4911338_etv6_ncoa2_tm_hcd34_h3k27ac_vs_total_peaks.narrowpeak.gz', 'gse124690/gsm3540920_k562_h3k4me2_rep2_20181202_icell8_215.bed.gz']\n</pre> In\u00a0[8]: <pre>def S2(file, distance):\n    df = distance[distance.filename == file].sort_values(by=['score'], ascending = False)[0:10]\n    df= df.sort_values(by=['score'], ascending = True)\n    df['color']='green'\n    plt= df.plot.barh(x='search_term', y='score', figsize=(8,5), fontsize=16, color=list(df['color']))\n    plt.set_xticks(np.arange(0.5,1.1, 0.1))\n    plt.set_ylabel('Similarity', fontsize=15)\n    plt.set_xlabel(file, fontsize=15)\n\n</pre> In\u00a0[9]: <pre>S2('gse156613/gsm4743940_t52-h3k27ac_peaks.bed.gz', distance)\n</pre> In\u00a0[11]: <pre>file_name = './distance_r2r.csv'\ndistance_s3 = pd.read_csv(file_name)\ndistance_s3.score = 1 - distance_s3.score\n</pre> In\u00a0[12]: <pre># print sample query region set\nlist(set(distance_s3.test_file))[0:10]\n</pre> Out[12]: <pre>['ENCFF538OAF.bed.gz,h3k4me1',\n 'ENCFF494ASP.bed.gz,h3k27me3',\n 'ENCFF292YSJ.bed.gz,h3k4me3',\n 'ENCFF001WUM.bed.gz,h3k4me3',\n 'ENCFF701ENX.bed.gz,h3k27ac',\n 'ENCFF727GSV.bed.gz,h3k27ac',\n 'ENCFF787BOJ.bed.gz,h3k27me3',\n 'ENCFF526BJR.bed.gz,h3k27ac',\n 'ENCFF908TKC.bed.gz,h3k9me3',\n 'ENCFF099WAJ.bed.gz,h4k20me1']</pre> In\u00a0[13]: <pre>def S3(query_file, distance_s3):\n\n    df = distance_s3[distance_s3.test_file==query_file].sort_values(by ='score', ascending = False)[['test_file', 'train_file', 'score']]\n    df['label_test'] = df.test_file.str.split(',', expand = True)[1]\n    df['label_train'] = df.train_file.str.split(',', expand = True)[1]\n\n    nof = len(df[df.label_test==df.label_train])\n    df = df[0:10]\n\n\n    df=df.sort_values(by=['score'])\n    df['color']='gray'\n    df.loc[df.label_test==df.label_train, 'color'] = 'green'\n\n    if(len(df[df.color=='green']) ==nof):\n\n        df.loc[(df.color!='green'), 'color'] = 'gray'\n\n    plt= df.plot.barh(x='train_file', y='score', figsize=(10,7), fontsize=16, color=list(df['color']))\n    plt.axis(xmin=0.7,xmax=1.01)\n\n    plt.set_ylabel('Similarity', fontsize=15)\n    plt.set_xlabel(query_file, fontsize=15)\n</pre> In\u00a0[16]: <pre>S3('ENCFF168GCU.bed.gz,h3k4me1', distance_s3)\n</pre>"},{"location":"geniml/code/bedspace-analysis/#Scenario-1:-return-a-list-most-similar-region-sets-and-thier-similarity-score-to-a-query-metadata-label-(l2r)","title":"Scenario 1: return a list most similar region sets and thier similarity score to a query metadata label (l2r)\u00b6","text":""},{"location":"geniml/code/bedspace-analysis/#Scenario-2:-return-a-list-of-most-similar-labels-and-thier-similarity-score-to-a-query-region-set-(r2l)","title":"Scenario 2: return a list of most similar labels and thier similarity score to a query region set (r2l)\u00b6","text":""},{"location":"geniml/code/bedspace-analysis/#Scenario-3:-return-a-list-of-most-similar-region-sets-and-thier-similarity-scores-to-a-query-region-set-(r2r)","title":"Scenario 3: return a list of most similar region sets and thier similarity scores to a query region set (r2r)\u00b6","text":""},{"location":"geniml/code/create-consensus-peaks-python/","title":"Create consensus peaks with Python","text":"create-consensus-peaks-python In\u00a0[1]: <pre>from geniml.universe.cc_universe import cc_universe\ncc_universe(\"coverage/\", file_out=\"universe_cc.bed\")\n</pre> <p>Depending on the task the universe can be smooth by setting <code>merge</code> option with the distance below witch peaks should be merged together and  <code>filter_size</code> with minimum size of peak that should be part of the universe. Instead of using maximum likelihood cutoff one can also defined cutoff with <code>cutoff</code> option. If it is set to 1 the result is union universe, and when to number of files it wil produce intersection universe:</p> In\u00a0[2]: <pre>cc_universe(\"coverage/\", file_out=\"universe_union.bed\", cutoff=1)\ncc_universe(\"coverage/\", file_out=\"universe_intersection.bed\", cutoff=4)\n</pre> In\u00a0[3]: <pre>from geniml.universe.ccf_universe import ccf_universe\n\nccf_universe(\"coverage/\", file_out=\"universe_ccf.bed\")\n</pre> In\u00a0[4]: <pre>from geniml.likelihood.build_model import main\n\nmain(\"model.tar\", \"coverage/\",\n     \"all\",\n     file_no=4)\n</pre> <pre>Function 'main' executed in 0.0001min\n</pre> <p>The resulting tar archiver contains LH model. This model can be used as a scoring function that assigns to each position probability of it being a start, core or end of a region. It can be both used for universe assessment and universe building. Combination of LH model and optimization algorithm for building flexible universes results in maximum likelihood universe (ML):</p> In\u00a0[5]: <pre>from geniml.universe.ml_universe import ml_universe\n\nml_universe(\"model.tar\",\n     \"coverage\",\n     \"all\",\n     \"universe_ml.bed\")\n</pre> In\u00a0[6]: <pre>from geniml.universe.hmm_universe import hmm_universe\n\nhmm_universe(\"coverage/\",\n             \"universe_hmm.bed\")\n</pre>"},{"location":"geniml/code/create-consensus-peaks-python/#How-to-build-a-new-universe?","title":"How to build a new universe?\u00b6","text":""},{"location":"geniml/code/create-consensus-peaks-python/#Data-preprocessing","title":"Data preprocessing\u00b6","text":"<p>This is a jupyter version of CLI tutorial that can be found here. You will use here python functions instead of CLI to build and assess different universes. Files that you will use here can be downloaded from XXX. In there you will find a compressed folder:</p> <pre><code>consensus:\n    - raw\n        test_1.bed\n        test_2.bed\n        test_3.bed\n        test_4.bed\n    file_list.txt\n    chrom.sizes</code></pre> <p>In the raw folder there are example BED files used in this tutorial and in file_list.txt are names of files you will analyze. Additionally there is a file with chromosome sizes, which you will use to preprocess the data.</p> <p>Here we assume that you already have files of the genome coverage by the analyzed collection. The example of how to create them can be found here.</p>"},{"location":"geniml/code/create-consensus-peaks-python/#Coverage-cutoff-universe","title":"Coverage cutoff universe\u00b6","text":"<p>First, you will create a coverage cutoff universe (CC). This is the simplest type of a universe that only includes genomic positions with coverage greater or equal to cutoff x. This cutoff by default is calculated using simple likelihood model that calculates the probability of appearing in a collection. The universe can be build just based on genome coverage:</p>"},{"location":"geniml/code/create-consensus-peaks-python/#Coverage-cutoff-flexible-universe","title":"Coverage cutoff flexible universe\u00b6","text":"<p>A more complex version of coverage cutoff universe is coverage cutoff flexible universe (CCF). In contrast to its' fixed version it produces flexible universes. It uses two cutoffs calculated based on maximum likelihood cutoff, making a confidence interval around the optimal cutoff value. Despite the fact that the CFF universe is more complex it is build using the same input as the CC universe:</p>"},{"location":"geniml/code/create-consensus-peaks-python/#Maximum-likelihood-universe","title":"Maximum likelihood universe\u00b6","text":"<p>In the previous examples both CC anf CCF universes used simple likelihood model to calculate the cutoff. However, we also developed more complex likelihood model that takes into account the positions of starts and ends of the regions in the collection. This LH model can build based on coverage files:</p>"},{"location":"geniml/code/create-consensus-peaks-python/#HMM","title":"HMM\u00b6","text":"<p>The forth presented method of creating universes utilizes Hidden Markov Models. In this approach the parts of flexible regions are hidden states of the model, while genome coverage by the collections are emissions. The resulting universe is called Hidden Markov Model universe. It can be build only based on the genome coverage by the collection:</p>"},{"location":"geniml/code/create-consensus-peaks-python/#How-to-assess-new-universe?","title":"How to assess new universe?\u00b6","text":"<p>So far you used many different methods for creating new universes. But choosing, which universe represents data the best can be challenging. To help with this we created a tutorial that can be found here, which presents different  methods that assess universe fit to the collection of files.</p>"},{"location":"geniml/tutorials/assess-universe/","title":"How to assess universe fit to collection of BED files","text":""},{"location":"geniml/tutorials/assess-universe/#introduction","title":"Introduction","text":"<p>If you have a potential universe file, and a collection of BED files, this module will help you assess the fit of the proposed universe to your collection of BED files. We can assess fit either from CLI, or from within Python.</p>"},{"location":"geniml/tutorials/assess-universe/#command-line-usage","title":"Command-line usage","text":"<p>Both overlap and distance based assessments can be run using: <code>geniml assess ...</code> with appropriate flags.</p> <p><pre><code> geniml assess --assessment-method1 \\\n             --assessment-method2 \\\n             --...\n             --raw-data-folder tests/consesnus/raw/ \\\n             --file-list tests/consesnus/file_list.txt \\\n             --universe tests/consenus/universe/universe.bed \\\n             --save-to-file \\\n             --folder-out tests/consesnus/results/intersection/ \\\n             --pref test \\\n             --no-workers 1\n</code></pre> Where:</p> <ul> <li><code>--raw-data-folder</code>, takes the path to folder with files from the collection</li> <li><code>--file-list</code>, takes the path to file with list of files</li> <li><code>--universe</code>, takes the path to file with the assessed universe</li> <li><code>--save-to-file</code>,  is a flag that specifies whether to out put table with each row  containing file name and results of chosen metrics</li> <li><code>--folder-out</code>, takes the path to folder in which put the output file</li> <li><code>--pref</code>, takes a prefix of output file name</li> <li><code>--no-workers</code>, takes the number of workers that should be used</li> <li><code>--save-each</code>, is a flag that specifies whether to save between the closest peaks to file</li> </ul>"},{"location":"geniml/tutorials/assess-universe/#base-level-overlap-measure","title":"Base-level overlap measure","text":"<p>First test checks how much of our file is present in the universe and how much additional information is present in the universe. We can check that by adding <code>--overlap</code> to <code>geniml assess ...</code>. In the result files it will output columns with: number of bp in universe but not in file, number of bp in file but not the universe, and number of bp both in universe and file.</p> <p>We can also use it directly from Python like this:</p> <pre><code>from geniml.assess.intersection import run_intersection\n\nrun_intersection(\"test/consensus/raw/\",\n                        \"tests/consensus/file_list.txt\",\n                        \"tests/consensus/universe/universe.bed\",\n                        no_workers=1)\n</code></pre> <p>Or, we can calculate F10 score of the universe using:</p> <pre><code>from geniml.assess.intersection import get_f_10_score\n\nget_f_10_score(\"test/consensus/raw/\",\n               \"tests/consensus/file_list.txt\",\n               \"tests/consensus/universe/universe.bed\",\n               no_workers=1)\n</code></pre>"},{"location":"geniml/tutorials/assess-universe/#region-boundary-distance-measure","title":"Region boundary distance measure","text":"<p>Next, we can calculate the distance between query and universe. To do that we can choose from :  - <code>distance</code> - calculates distance from region in query to the nearest region in the universe  - <code>distance-universe-to-file</code>- calculates distance from region in query to the nearest region in the universe accounting for universe flexibility  - <code>distance-flexible</code> - calculates distance from region in universe to the nearest region in the query  - <code>distance-flexible-universe-to-file</code> - calculates distance from region in universe to the nearest region in the query accounting for universe flexibility</p> <p>All presented distance measures can be done using python, which will result in matrix where first column is file names and the second one is median of distances. </p> <p><pre><code>from geniml.assess.distance import run_distance\n\nd_median = run_distance(\"tests/consensus/raw\",\n                  \"tests/consensus/file_list.txt\",\n                  \"tests/consensus/universe/universe.bed\",\n                  npool=2)\n</code></pre> Additionally, we can directly calculate the closeness score using:</p> <pre><code>from geniml.assess.distance import get_closeness_score\n\ncloseness_score = get_closeness_score(\"tests/consensus/raw\",\n                                      \"tests/consensus/file_list.txt\",\n                                      \"tests/consensus/universe/universe.bed\",\n                                      no_workers=2)\n</code></pre>"},{"location":"geniml/tutorials/assess-universe/#universe-likelihood","title":"Universe likelihood","text":"<p>We can also calculate the likelihood of universe given collection of file. For that we will need likelihood model. We can do it either for hard universe:</p> <pre><code>from geniml.assess.likelihood import hard_universe_likelihood\n\nlh_hard = hard_universe_likelihood(\"tests/consensus/lh_model.tar\",\n                         \"tests/consensus/universe/universe.bed\",\n                         \"tests/consensus/coverage\", \"all\")\n</code></pre> <p>or with taking into account universe flexibility:</p> <pre><code>from geniml.assess.likelihood import likelihood_flexible_universe\n\nlh_flexible = likelihood_flexible_universe(\"tests/consensus/lh_model.tar\",\n                         \"tests/consensus/universe/universe.bed\",\n                         \"tests/consensus/coverage\", \"all\")\n</code></pre>"},{"location":"geniml/tutorials/bbclient/","title":"BED file caching and loading from BEDbase","text":"<p>The BEDbase client command <code>bbclient</code> downloads, processes, and caches BED files and BED sets from the BEDbase API and converts them into GenomicRanges or GenomicRangesList objects. It provides various commands to interact with BED files, including downloading individual files, downloading BEDsets, processing local BED files, and processing BED file identifiers.</p> <p>This document provides tutorials for using <code>bbclient</code> via either:</p> <ol> <li>the Python interface, or</li> <li>the command-line interface.</li> </ol>"},{"location":"geniml/tutorials/bbclient/#getting-started-python-interface","title":"Getting started: Python interface","text":""},{"location":"geniml/tutorials/bbclient/#create-an-instance-of-the-bbclient-class","title":"Create an instance of the BBClient Class:","text":"<pre><code>from geniml.bbclient import BBClient\n\nbbclient = BBClient(cache_folder=\"cache\", bedbase_api=\"https://api.bedbase.org\")\n</code></pre>"},{"location":"geniml/tutorials/bbclient/#download-and-cache-a-remote-bed-file-from-bedbase","title":"Download and cache a remote BED file from BEDbase","text":"<pre><code>bedfile_id = \"....\"  # find interesting bedfile on bedbase\nbedfile = bbclient.load_bed(bedfile_id)  # download, cache and return a RegionSet object\ngr = bedfile.to_granges()  # return a GenomicRanges object\n</code></pre>"},{"location":"geniml/tutorials/bbclient/#cache-a-local-bed-file","title":"Cache a local BED file","text":"<pre><code>from geniml.io import RegionSet\n\nbedfile = RegionSet(\"path/to/bedfile\")\ngr = bedfile.to_granges()  # should return a GenomicRanges object\nbedfile_id = bbclient.add_bed_to_cache(bedfile) # compute its ID and add it to the cache\n</code></pre>"},{"location":"geniml/tutorials/bbclient/#cache-a-bed-file-from-within-python-memory","title":"Cache a BED file from within Python memory","text":"<p>You can just provide a URL and it will add to cache for you:</p> <pre><code>bbclient.add_bed_to_cache(url)\n</code></pre>"},{"location":"geniml/tutorials/bbclient/#load-a-bed-file-from-cache-into-python-session","title":"Load a BED file from cache into Python session","text":"<pre><code>bedfile_id = \"....\"  # get the identifier\nbedfile = bbclient.load_bed(bedfile_id)  # the same function can also load BED files that have already been cached\n</code></pre>"},{"location":"geniml/tutorials/bbclient/#download-and-cache-a-bedset-from-bedbase","title":"Download and cache a BEDset from BEDbase","text":"<pre><code>bedset_identifier = \"xyz\" # find some interesting bedset on bedbase.org\nbedset = bbclient.load_bedset(bedset_identifier)  # download, cache and return a BedSet object\ngrl = bedset.to_granges_list()  # return a GenomicRangesList object\n</code></pre>"},{"location":"geniml/tutorials/bbclient/#cache-a-local-bedset","title":"Cache a local BEDset","text":"<pre><code>from geniml.io import BedSet\n\nbedset_folder = \"path/to/bed/files/folder\"\nbedset = BedSet(\n    [os.path.join(bedset-folder, file_name) for file_name in os.listdir(bedset_folder)]\n)\nbedset_id = bbclient.add_bedset_to_cache(bedset)\n</code></pre>"},{"location":"geniml/tutorials/bbclient/#command-line-interface","title":"Command line interface","text":""},{"location":"geniml/tutorials/bbclient/#cache-bed-file","title":"Cache BED file","text":"<pre><code>geniml bbclient cache-bed &lt;BED_file_or_identifier_or_url&gt;\n</code></pre> <p>The <code>&lt;BED_file_or_identifier_or_url&gt;</code> variable can be one of 3 things:</p> <ol> <li>a path to a local BED file;</li> <li>a BED record identifier from BEDbase; or,</li> <li>a URL to a BED file hosted anywhere.</li> </ol>"},{"location":"geniml/tutorials/bbclient/#cache-bedset","title":"Cache BEDset","text":"<pre><code>geniml bbclient cache-bedset &lt;BED_files_folder_or_identifier&gt;\n</code></pre> <p>The <code>&lt;BED_files_folder_or_identifier&gt;</code> variable may be:</p> <ol> <li>local path to a folder containing BED files; or,</li> <li>a BEDbase BEDset identifier</li> </ol>"},{"location":"geniml/tutorials/bbclient/#seek-the-path-of-a-bed-file-or-bedset-in-cache-folder","title":"Seek the path of a BED file or BEDset in cache folder","text":"<p>To retrieve the local file path to a BED file stored locally,</p> <pre><code>geniml bbclient seek &lt;identifier&gt;\n</code></pre> <p>Replace  with the identifier of the BED file or BEDset you want to seek."},{"location":"geniml/tutorials/bbclient/#count-the-subdirectories-and-files-in-bedfiles-bedsets-folder","title":"Count the subdirectories and files in <code>bedfiles</code> &amp; <code>bedsets</code> folder","text":"<pre><code>geniml bbclient inspect\n</code></pre> <p><code>inspect</code> command may need installing <code>tree</code></p>"},{"location":"geniml/tutorials/bbclient/#remove-a-bed-file-or-bedset-from-the-cache-folder","title":"Remove a BED file or BEDset from the cache folder","text":"<pre><code>geniml bbclient rm &lt;identifier&gt;\n</code></pre> <p>Replace  with the identifier of the BED file or BEDset you want to remove."},{"location":"geniml/tutorials/bbclient/#cache-folder","title":"Cache Folder","text":"<p>By default, the downloaded and processed BED files are cached in the bed_cache folder. You can specify a different cache folder using the --cache-folder argument, or set the environment variable <code>BBCLIENT_CACHE</code>. The cache folder has this structure: <pre><code>cache_folder\n  bedfiles\n    a/b/ab1234xyz.bed.gz\n    ..\n  bedsets\n    c/d/cd123hij.txt\n</code></pre></p>"},{"location":"geniml/tutorials/bbclient/#dependencies","title":"Dependencies","text":"<pre><code>requests: For making HTTP requests.\npandas: For data manipulation.\ngenomicranges: For processing BED files and creating GenomicRanges objects.\n</code></pre>"},{"location":"geniml/tutorials/bedshift-evaluation-guide/","title":"Using bedshift to create an evaluation dataset for similarity scores","text":""},{"location":"geniml/tutorials/bedshift-evaluation-guide/#generate-different-files","title":"Generate different files","text":"<p>Bedshift perturbations include add, drop, shift, cut, and merge. Using any of these perturbations, or combinations of them, you can generate a set of files that are slightly perturbed from the original file. Assuming that the original file is called <code>original.bed</code>, and you want 100 files of added regions and 100 files of dropped regions:</p> <pre><code>bedshift -l hg38.chrom.sizes -b original.bed -a 0.1 -r 100\nbedshift -l hg38.chrom.sizes -b original.bed -d 0.3 -r 100\n</code></pre> <p>Don't forget the add and shift operations require a chrom.sizes file. The output file will be in <code>bedshifted_original.bed</code>.</p>"},{"location":"geniml/tutorials/bedshift-evaluation-guide/#evaluating-a-similarity-score","title":"Evaluating a similarity score","text":"<p>This is when the bedshifted file will be put to use. The 100 repetitions of add and drop will be compared against the original file using the similarity score of your choice. The output of the similarity score should reflect the degree of change specified to bedshift. In very general terms, the pseudocode should be like this:</p> <pre><code>for each bedshift_file in folder:\n    score = SimilarityScore(bedshift_file, original_file, ...)\n    add score to score_list\navg_similarity_score = mean(score_list)\n</code></pre> <p>You can repeat this for each of the similarity scores and each of the perturbation combinations, and then compare the results. This way, you can get an accurate understanding of whether your similarity score reflects added regions, dropped regions, and more.</p>"},{"location":"geniml/tutorials/bedshift-evaluation-guide/#using-a-pep-to-quickly-submit-multiple-bedshift-jobs","title":"Using a PEP to quickly submit multiple bedshift jobs","text":"<p>Using a Portable Encapsulated Project (PEP), creating multiple combinations of bedshift files becomes faster and more organized. The PEP consists of a sample table containing the perturbation parameters and a config file. Here is what the <code>sample_table.csv</code> may look like. Each row specifies the arguments for a bedshift command.</p> sample_name add drop shift cut merge add1 0.1 0.0 0.0 0.0 0.0 add2 0.2 0.0 0.0 0.0 0.0 add3 0.3 0.0 0.0 0.0 0.0 drop-shift1 0.0 0.1 0.2 0.0 0.0 drop-shift2 0.0 0.2 0.2 0.0 0.0 drop-cut 0.0 0.3 0.0 0.4 0.0 shift-merge 0.0 0.0 0.4 0.0 0.4 <p>And here is what the <code>project_config.yaml</code> file looks like:</p> <pre><code>pep_version: 2.0.0\nsample_table: \"sample_table.csv\"\nsample_modifiers:\n  append:\n    file: \"original.bed\"\n    repeat: 100\n</code></pre> <p>Now the project is described neatly in two files. The <code>sample_modifiers</code> in the config file just adds extra columns to the sample table in post-processing and makes the project more configurable, instead of having to repeat the same parameter in the <code>sample_table.csv</code>. In this example, the <code>sample_modifiers</code> append two columns with the file which bedshift is to be performed on, and the number of repetitions that bedshift should create.</p> <p>The PEP describes the project, but the tool that submits the project jobs is called looper. In one line of code, it will interpret the PEP and form commands to be submitted to your processor or computing cluster. To use looper, you will need to add a few lines to your <code>project_config.yaml</code>:</p> <pre><code>pep_version: 2.0.0\nsample_table: \"sample_table.csv\"\nlooper:\n  output_dir: \"looper_output/\"\nsample_modifiers:\n  append:\n    pipeline_interfaces: \"pipeline_interface.yaml\"\n    file: \"original.bed\"\n    repeat: 100\n</code></pre> <p>You will also need to create a <code>pipeline_interface.yaml</code> that describes how to form commands:</p> <pre><code>pipeline_name: bedshift_run\npipeline_type: sample\ncommand_template: &gt;\n    bedshift -b {sample.file} -l hg38.chrom.sizes -a {sample.add} -d {sample.drop} -s {sample.shift} -c {sample.cut} -m {sample.merge} -r {sample.repeat} -o {sample.sample_name}.bed\ncompute:\n  mem: 4000\n  cores: 1\n  time: \"00:10:00\"\n</code></pre> <p>After all of this, the command to run looper and submit the jobs is:</p> <pre><code>looper run project_config.yaml\n</code></pre> <p>Soon, you should see bedshift files appear in the <code>looper_output</code> folder. The BED file names will correspond to the sample names from <code>sample_table.csv</code>.</p>"},{"location":"geniml/tutorials/bedshift/","title":"Randomizing BED files with bedshift","text":"<p>Bedshift is a tool for randomly perturbing BED file regions. The perturbations supported on regions are shift, drop, add, cut, and merge. This tool is particularly useful for creating test datasets for various tasks, since there often is no ground truth dataset to compare to. By perturbing a file, a pipeline or analysis can be run on both the perturbed file and the original file, then be compared.</p>"},{"location":"geniml/tutorials/bedshift/#installing","title":"Installing","text":"<p>Bedshift is part of the <code>geniml</code> package distributed on PyPI.</p> <pre><code>pip install geniml\n</code></pre>"},{"location":"geniml/tutorials/bedshift/#quickstart","title":"Quickstart","text":"<p>The package is available for use both as a command line interface and a python package. To get started, type on the command line:</p> <pre><code>bedshift -h\n</code></pre> <p>The following examples will shift 10% of the regions and add 10% new regions in <code>examples/test.bed</code>. The -l argument is the file in which chromosome sizes are located, and is only required for adding and/or shifting regions. The output is located at <code>bedshifted_test.bed</code>.</p> <p>CLI:</p> <pre><code>bedshift -l hg38.chrom.sizes -b tests/test.bed -s 0.1 -a 0.1\n</code></pre> <p>Python:</p> <pre><code>import bedshift\n\nbedshifter = bedshift.Bedshift('tests/test.bed', 'hg38.chrom.sizes')\nbedshifter.shift(shiftrate=0.1, shiftmean=0.0, shiftstdev=120.0)\nbedshifter.add(addrate=0.1, addmean=320.0, addstdev=20.0)\nbedshifter.to_bed('tests/test_output.bed')\n</code></pre>"},{"location":"geniml/tutorials/bedshift/#example-repository","title":"Example Repository","text":"<p>If you're looking to use Bedshift in your own experiment, we created an example repository containing working code to:</p> <ol> <li>Produce a large dataset of Bedshift files</li> <li>Run a pipeline on the dataset and obtain results</li> <li>Aggregate and visualize the results</li> </ol> <p>It integrates the PEP and looper workflow allowing you to easily run the project out of the box.</p>"},{"location":"geniml/tutorials/bedshift/#generate-a-random-bed-file","title":"Generate a random BED file","text":"<p>To generate a random BED file, you need to start with is a BED file with one region, which you will delete later. (It will appear at the top of the BED files, so it will be easier to delete later.) On the command line:</p> <pre><code>echo \"chr1\\t1\\t1000\" &gt; random.bed\n</code></pre> <p>The next step is to construct a bedshift command to add regions to this file. We will need to specify the rate of add, which in this case is going to be the number of the new regions we want. Let's try generating 1,000 new regions. Don't forget to specify a chromosome sizes file, which is required for adding regions.</p> <pre><code>bedshift -b random.bed -l hg38.chrom.sizes -a 1000\n</code></pre> <p>The printed output should say that 1000 regions changed. Finally, go into the output at bedshifted_random.bed and delete the original region. If you specified repeats and have many output files that need to have the original region deleted, here is a handy command to delete the first line of every BED file. (Warning: make sure there are no other BED files in the folder before using this command.)</p> <pre><code>find . -name \"*.bed\" -exec sed -i '.bak' '1d' {} \\;\n</code></pre> <p>This <code>find</code> command will find all BED files and execute a <code>sed</code> command to remove the first line. The <code>sed</code> command will operate in place and create <code>.bak</code> backup files, which can be removed later.</p>"},{"location":"geniml/tutorials/bedshift/#shift-add-and-drop-from-file","title":"Shift, Add, and Drop From File","text":"<p>\"From file\" means that the regions selected to shift, add, or drop are specified from a provided file. These features provide the ability to finely control what regions are perturbed. For example, if you have a BED file specifying exon regions and you want to add only exons, you can use <code>--addfile</code>.</p>"},{"location":"geniml/tutorials/bedshift/#add-from-file-example","title":"Add from file example","text":"<pre><code>bedshift -b mydata.bed -a 0.07 --addfile exons.bed\n</code></pre> <p>Specifying <code>--addfile</code> with <code>-a</code> add rate will increase the size of <code>mydata.bed</code> by 7% with new regions selected from <code>exons.bed</code>.</p>"},{"location":"geniml/tutorials/bedshift/#shift-from-file-example","title":"Shift from file example","text":"<p>Shift from file first calculates which regions overlap between the specified <code>--shiftfile</code> and <code>--bedfile</code>, then selects which regions to shift among those overlaps.</p> <pre><code>bedshift -b mydata.bed -s 0.42 --shiftmean 5 --shiftstdev 5 --shiftfile snp.bed\n</code></pre> <p>In this example, we only want to shift regions that are SNPs. The number of shifted regions is 42% of the total regions in <code>mydata.bed</code>. Notice here that unlike <code>--addfile</code>, we still have to specify the shift mean and standard deviation. This is because <code>--shiftfile</code> tells which regions to shift, but not by how much.</p>"},{"location":"geniml/tutorials/bedshift/#drop-from-file-example","title":"Drop from file example","text":"<p>Drop from file, like shift from file, calculates overlaps between the specified <code>--dropfile</code> and <code>--bedfile</code>, then selects regions from those overlaps to drop.</p> <pre><code>bedshift -b mydata.bed -d 0.4 -dropfile snp.bed\n</code></pre> <p>This command will drop regions that overlap with SNPs. The number of dropped regions is 40% of the total regions in <code>mydata.bed</code>.</p>"},{"location":"geniml/tutorials/bedshift/#use-a-yaml-file-to-specify-perturbations","title":"Use a YAML File to Specify Perturbations","text":"<p>Sometimes the default settings of bedshift does not allow enough control over perturbations. For example, the order of perturbations is fixed as shift, add, cut, merge, drop, so if you wanted to change the order you would have to specify multiple commands. The same problem arises when you want to run multiple \"add from file\" commands - there is just no way to do it using a single command.</p> <p>This is why we created the YAML config file perturbation option. In the YAML file, users can specify as many perturbations as they want, along with the parameters specific to each perturbation. An example of a YAML config file follows:</p> <pre><code>bedshift_operations:\n  - add_from_file:\n    file: exons.bed\n    rate: 0.2\n  - add_from_file:\n    file: snp.bed\n    rate: 0.05\n  - shift_from_file:\n    file: exons.bed\n    rate: 0.4\n    mean: 100\n    stdev: 85\n  - shift_from_file:\n    file: snp.bed\n    rate: 0.4\n    mean: 2\n    stdev: 1\n  - merge:\n    rate: 0.15\n</code></pre> <p>The order of perturbations is run in the same order they are specified. So in this example, we add from two different files, then also shift those regions that were just added. Finally we perform a merge at 15% rate.</p>"},{"location":"geniml/tutorials/bedshift/#how-to-bedshift-all-files-in-a-directory","title":"How to bedshift all files in a directory","text":""},{"location":"geniml/tutorials/bedshift/#using-shell","title":"Using shell","text":"<p>Assuming you are bedshifting all files in the current working directory using the same parameter, use the following shell script (changing parameters as needed), which iterates over files in the directory and applies bedshift:</p> <pre><code>#!/bin/bash\nfor filename in *.bed; do\n    CHROM_LENGTHS=hg38.chrom.sizes\n    BEDFILE=$filename\n    DROP_RATE=0.3\n\n    ADD_RATE=0.2\n    ADD_MEAN=320.0\n    ADD_STDEV=30.0\n\n    SHIFT_RATE=0.2\n    SHIFT_MEAN=0.0\n    SHIFT_STDEV=150.0\n\n    CUT_RATE=0.0\n    MERGE_RATE=0.0\n\n    bedshift --bedfile $BEDFILE --chrom-lengths $CHROM_LENGTHS --droprate $DROP_RATE --addrate $ADD_RATE --addmean $ADD_MEAN --addstdev $ADD_STDEV --shiftrate $SHIFT_RATE --shiftmean $SHIFT_MEAN --shiftstdev $SHIFT_STDEV --cutrate $CUT_RATE --mergerate $MERGE_RATE\ndone\n</code></pre>"},{"location":"geniml/tutorials/bedshift/#using-python","title":"Using Python","text":"<p>In Python, you need the <code>os</code> library to get the filenames in a directory. Then you loop through the filenames and apply bedshift.</p> <pre><code>import bedshift\nimport os\n\nfiles = os.listdir('/path/to/data/')\nfor file in files:\n    if file.endswith('.bed'):\n        # you may also pass in a chrom.sizes file as the \n        # second argument if you are adding or shifting regions\n        b = bedshift.Bedshift(file)\n        b.all_perturbations(cutrate=0.3, droprate=0.2)\n        b.to_bed('bedshifted_' + file)\n</code></pre>"},{"location":"geniml/tutorials/bedshift/#add-random-regions-only-in-valid-regions","title":"Add Random Regions Only in Valid Regions","text":"<p>Using the basic <code>--add</code> option, regions are added randomly onto any chromosome at any location, without any regard for non-coding regions. For use cases of Bedshift more rooted in biology, this effect is not desirable. The <code>--add-valid</code> option gives the user the ability to specify a BED file indicating areas where it is valid to add regions. Thus, if an <code>--add-valid</code> file has only coding regions, then regions will be randomly added only in those areas. Here is an example:</p> <pre><code>bedshift -b mydata.bed -a 0.5 --add-valid coding.bed --addmean 500 --addstdev 200\n</code></pre> <p><code>coding.bed</code> contains large regions of the genome which are coding. Added regions can be anywhere inside of those regions. In addition, the method considers the size of the valid regions in deciding where the new regions will be added, so the smaller valid regions will contain proportionally less new regions than the larger valid regions.</p>"},{"location":"geniml/tutorials/bedspace/","title":"How to use BEDSpace to jointly embed regions and metadata","text":""},{"location":"geniml/tutorials/bedspace/#introduction","title":"Introduction","text":"<p>BEDspace is an application of the StarSpace model to genomic interval data, described in Gharavi et al. 2023. It allows us to train numerical embeddings for a collection of region sets simultaneously with their metadata labels, capturing similarity between region sets and their metadata in a low-dimensional space. Using these learned co-embeddings, BEDspace solves three related information retrieval tasks using embedding distance computations: retrieving region sets related to a user query string; suggesting new labels for database region sets; and retrieving database region sets similar to a query region set.</p>"},{"location":"geniml/tutorials/bedspace/#installation","title":"Installation","text":"<p>The <code>bedspace</code> module is installed with <code>geniml</code>. To ensure that everything is working correctly, run: <code>python -c \"from geniml import bedspace\"</code>. </p>"},{"location":"geniml/tutorials/bedspace/#bedspace-operations","title":"BEDspace operations","text":"<p>There are four main commands in <code>bedspace</code>:</p> <ol> <li><code>bedspace preprocess</code>: preprocesses a set of genomic interval regions and their associated metadata into a format that can be used by <code>bedspace train</code>.</li> <li><code>bedspace train</code>: trains a StarSpace model on the preprocessed data.</li> <li><code>bedspace distances</code>: computes distances between region sets in the trained model and metadata labels.</li> <li><code>bedspace search</code>: searches for the most similar region sets and metadata labels to a given query. Three scenarios for this command are described in the details.</li> </ol> <p>These commands are accessed via the command line with <code>genimtools bedspace &lt;command&gt;</code>.</p>"},{"location":"geniml/tutorials/bedspace/#bedspace-preprocess","title":"<code>bedspace preprocess</code>","text":"<p>The <code>preprocess</code> command will prepare a set of region sets and metadata labels for training. This includes things like adding the <code>__label__</code> prefix to metadata labels, and converting the region sets into a format that can be used by StarSpace. The command takes in a set of region sets and metadata labels, and outputs a set of preprocessed region sets and metadata labels. The command can be run as follows:</p> <pre><code>geniml bedspace preprocess \\\n    --input &lt;path to input region sets&gt; \\\n    --metadata &lt;path to input metadata labels&gt; \\\n    --universe &lt;path to universe file&gt; \\\n    --labels &lt;path to the labels file&gt; \\\n    --output &lt;path to output preprocessed region sets&gt;\n</code></pre> <p>Input Description:</p> <p><code>--input</code>: Specifies the path to the folder containing the region sets. <code>--metadata</code>: Specifies the path to the metadata file in CSV format. The CSV file should include a column for file_name and separate columns for each label. The file_name column contains the names of the region set files, and the label columns contain the corresponding labels for each region set. <code>--universe</code>: Specifies the path to the universe file. The universe file contains the chromosome, start position, and end position for each region for region set tokenization. <code>--labels</code>: Specifies the target labels as a single string containing labels separated by commas.  </p>"},{"location":"geniml/tutorials/bedspace/#bedspace-train","title":"<code>bedspace train</code>","text":"<p>The <code>train</code> command will train a StarSpace model on the preprocessed region sets and metadata labels. It requires that you have run the <code>preprocess</code> command first. The <code>train</code> command takes in a set of preprocessed region sets and metadata labels, and outputs a trained StarSpace model. The command can be run as follows:</p> <pre><code>geniml bedspace train \\\n    --path-to-starspace &lt;path to StarSpace executable&gt; \\\n    --input &lt;path to preprocessed region sets&gt; \\\n    --output &lt;path to output trained model&gt; \\\n    --dim &lt;dimension of embedding space&gt; \\\n    --epochs &lt;number of epochs to train for&gt; \\\n    --lr &lt;learning rate&gt;\n</code></pre> <p>Input Description:</p> <p><code>--path-to-starspace</code>: Specifies the path to the StarSpace executable. <code>--input</code>: Specifies the path to the preprocessed region sets file generated from the preprocess function. The file should be in TXT format. <code>--output</code>: Specifies the path where the trained model will be saved.  <code>--dim</code>: Sets the dimension of the vector for the region set and label embedding from the StarSpace model. <code>--epochs</code>: Specifies the number of epochs to train the StartSpace model. <code>--lr</code>: Sets the learning rate for the training process.  </p>"},{"location":"geniml/tutorials/bedspace/#bedspace-distances","title":"<code>bedspace distances</code>","text":"<p>The <code>distances</code> command will compute the distances between all of the region sets and metadata labels in the trained model. It requires that you have ran the <code>train</code> command first. The <code>distances</code> command takes in a trained StarSpace model, and outputs a set of distances between all of the region sets and metadata labels in the model. The command can be run as follows:</p> <pre><code>geniml bedspace distances \\\n    --input &lt;path to trained model&gt; \\\n    --metadata &lt;path to input metadata labels&gt; \\\n    --universe &lt;path to universe file&gt; \\\n    --labels &lt;path to labels file&gt; \\\n    --files &lt;path to region sets&gt; \\\n    --output &lt;path to output distances&gt;\n</code></pre> <p>Input Description:</p> <p><code>--input</code>: Specifies the path to the trained model generated by the bedspace train command. <code>--metadata</code>: Specifies the path to the input metadata labels. <code>--universe</code>: Specifies the path to the universe file used for test file tokenization. <code>--labels</code>: Specifies the target labels as a single string containing labels separated by commas. <code>--files</code>: Specifies the path to the new region sets. <code>--output</code>: Specifies the path where the distances file between labels and files, as well as database files and new files, will be saved.  </p>"},{"location":"geniml/tutorials/bedspace/#bedspace-search","title":"<code>bedspace search</code>","text":"<p>The <code>search</code> command requires that you have previously run the <code>distances</code> command.  It also requires a query. To search, you must specify one of 3 scenarios when using the <code>search</code> command:</p> <ol> <li><code>r2l</code> (region-to-label): You have a query region set and want to find the most similar metadata labels,</li> <li><code>l2r</code> (label-to-region): You have a query metadata label and want to find the most similar region sets, and</li> <li><code>r2f</code> (region-to-region): You have a query region set and want to find the most similar region sets.</li> </ol> <p>Example usage for each type are given below:</p>"},{"location":"geniml/tutorials/bedspace/#r2l","title":"<code>r2l</code>","text":"<pre><code>geniml bedspace search \\\n    -t lr2\n    -d &lt;path to distances&gt; \\\n    -n &lt;number of results to return&gt; \\\n    path/to/regions.bed\n</code></pre>"},{"location":"geniml/tutorials/bedspace/#l2r","title":"<code>l2r</code>","text":"<pre><code>geniml bedspace search \\\n    -t rl2\n    -d &lt;path to distances&gt; \\\n    -n &lt;number of results to return&gt; \\\n    K562\n</code></pre>"},{"location":"geniml/tutorials/bedspace/#r2r","title":"<code>r2r</code>","text":"<pre><code>geniml bedspace search \\\n    -t r2r\n    -d &lt;path to distances&gt; \\\n    -n &lt;number of results to return&gt; \\\n    path/to/regions.bed\n</code></pre> <p>Input Description:</p> <p><code>-t</code>: Specifies the search type. <code>-d</code>: Specifies the path to the distances file generated by the bedsapce distances command. <code>-n</code>: Specifies the number of top results to return.  </p>"},{"location":"geniml/tutorials/cell-type-annotation-with-knn/","title":"How to annotate cell-types with KNN","text":"<p>In the previous tutorial, we loaded a vector database with cell embeddings. In this tutorial, we will show how to use this vector database to query cell embeddings and annotate cells with cell-type labels using a KNN classification algorithm.</p> <p>If you have not completed the previous tutorial, you should ensure you have a vector database with cell embeddings.</p>"},{"location":"geniml/tutorials/cell-type-annotation-with-knn/#what-is-k-nearest-neighbors-knn-classification","title":"What is K-nearest-neighbors (KNN) classification?","text":"<p>According to IBM, K-nearest-neighbors classification is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. Point more simply: KNN is a classification algorithm that uses the distance between an unlabeled data point and its labed neighbors to classify the new data point.</p> <p>Assuming we have a vector-space of well-annotated cell embeddings, we can use KNN to classify new cell embeddings based on their proximity to the labeled cell embeddings.</p>"},{"location":"geniml/tutorials/cell-type-annotation-with-knn/#querying-the-vector-database","title":"Querying the vector database","text":"<p>First, we need to generate new cell embeddings for the cells we want to annotate. Note: it is imperative that the new cell embeddings are generated using the same model as the cell embeddings in the vector database. The previous tutorial used <code>databio/r2v-luecken2021-hg38-v2</code> to generate cell embeddings. We will use the same model to generate new cell embeddings.</p> <pre><code>import scanpy as sc\n\nfrom geniml.scembed import ScEmbed\n\nadata = sc.read_h5ad(\"path/to/adata_unlabeled.h5ad\")\n\nmodel = ScEmbed(\"databio/r2v-luecken2021-hg38-v2\")\n</code></pre> <p>We can get embeddings of the dataset using the pre-trained model:</p> <pre><code>embeddings = model.encode(adata)\n\nadata.obsm['scembed_X'] = np.array(embeddings)\n</code></pre> <p>Now that we have the new cell embeddings, we can query the vector database to find the K-nearest-neighbors of each cell embedding.</p> <pre><code>from collections import Counter\nfrom qdrant_client import QdrantClient\n\nclient = QdrantClient(\"localhost\", port=6333)\n\n# Query the vector database\nk = 5 # set to whatever value you want, this is a hyperparameter\n\nfor i, embedding in enumerate(embeddings):\n    neighbors = client.search(\n        collection_name=\"luecken2021\", \n        query_vector=embedding.tolist(), \n        limit=k, \n        with_payload=True\n    )\n    cell_types = [neighbor.payload[\"cell_type\"] for neighbor in neighbors]\n\n    # get majority\n    cell_type = Counter(cell_types).most_common(1)[0][0]\n    adata.obs['cell_type'][i] = cell_type\n</code></pre> <p>And just like that, we've annotated our cells with cell-type labels using KNN classification. We can improve this methodology by first clustering the unlabeled cells and then using the cluster centroids to query the vector database. This will reduce the number of queries and improve the speed of the annotation process. Another approach would be to do a secondary consensus vote on each cluster and assign one label per cluster.</p>"},{"location":"geniml/tutorials/cli-tokenization/","title":"How to tokenize a BED file on the command line","text":"<p>For hard tokenization, run</p> <pre><code>from geniml.tokenization import hard_tokenization\n\nsrc_folder = '/path/to/raw/bed/files/'\ndst_folder = '/path/to/tokenized_files/'\nuniverse_file = '/path/to/universe_file.bed'\nhard_tokenization(src_folder, dst_folder, universe_file, 1e-9)\n</code></pre> <p>We use the <code>intersect</code> function of <code>bedtools</code> to do tokenization. If you want to switch to different tools, you can override the <code>bedtools_tokenization</code> function in <code>hard_tokenization_batch.py</code> and provide the path to your tool by specifying the input argument <code>bedtools_path</code>. The <code>fraction</code> argument specifies the minimum overlap required as a fraction of some region in the universe (default: 1E-9,i.e. 1bp; maximum 1.0). A raw region will be mapped into a universe region when an overlap is above the threshold.</p> <p>By default, the code assumes the binary <code>bedtools</code> exists and can be called via command line. If <code>bedtools</code> does not exists, the code will raise an exception. To solve this, please specify <code>bedtools_path</code> which points to a bedtools binary.</p> <p>Command line usage <pre><code>geniml tokenize --data-folder /folder/with/raw/BED/files --token-folder ./tokens --universe /universe/file --bedtools-path bedtools\n</code></pre></p> <p>For more details, type <code>geniml tokenize --help</code>.</p>"},{"location":"geniml/tutorials/create-consensus-peaks/","title":"How to build a new universe?","text":""},{"location":"geniml/tutorials/create-consensus-peaks/#data-preprocessing","title":"Data preprocessing","text":"<p>In this tutorial, you will use CLI of geniml package to build different types of universes from example files, which can be downloaded from XXX. In there you will find a compressed folder:</p> <pre><code>consensus:\n    - raw\n        test_1.bed\n        test_2.bed\n        test_3.bed\n        test_4.bed\n    file_list.txt\n    chrom.sizes\n</code></pre> <p>In the raw folder there are example BED files used in this tutorial and in file_list.txt are names of files you will analyze. Additionally there is a file with chromosome sizes, which you will use to preprocess the data. </p> <p>To build any kind of a universe you need bigWig files with genome coverage by the analyzed collection, which can be made it using uniwig. First we have to combine all the analyzed files into one BED file:</p> <pre><code>cat raw/* &gt; raw/combined_files.bed\n</code></pre> <p>This combined file can next be used to prepare the genome coverage tracks, with window size for smoothing of breakpoints set to 25:</p> <pre><code>$UNIWIG_PATH/bin/uniwig -m 25 raw/combined_files.bed chrom.sizes coverage/all\n</code></pre> <p>This will create three files: <code>coverage/all_start.bw</code>, <code>coverage/all_core.bw</code>, <code>coverage/all_end.bw</code>, with coverage of the genome by regions' starts, regions and regions' ends respectively. Those files can be loaded into Genomic Viewer for visualization.  </p>"},{"location":"geniml/tutorials/create-consensus-peaks/#coverage-cutoff-universe","title":"Coverage cutoff universe","text":"<p>First, you will create a coverage cutoff universe (CC). This is the simplest type of a universe that only includes genomic positions with coverage greater or equal to cutoff x. This cutoff by default is calculated using simple likelihood model that calculates the probability of appearing in a collection. The universe can be build just based on genome coverage:</p> <pre><code>geniml build-universe cc --coverage-folder coverage/ \\\n                          --output-file universe_cc.bed\n</code></pre> <p>Depending on the task the universe can be smooth by setting <code>--merge</code>  flag with the distance beloved witch peaks should be merged together and  <code>--filter-size</code> with minimum size of peak that should be part of the universe. Instead of using maximum likelihood cutoff one can also defined cutoff with <code>--cutoff</code> flag. If it is set to 1 the result is union universe, and when to number of analyzed files it wil produce intersection universe.</p>"},{"location":"geniml/tutorials/create-consensus-peaks/#coverage-cutoff-flexible-universe","title":"Coverage cutoff flexible universe","text":"<p>A more complex version of coverage cutoff universe is coverage cutoff flexible universe (CCF). In contrast to its' fixed version it produces flexible universe. It builds confidence interval around the maximum likelihood cutoff. This results in two values one for the cutoff for boundaries, and the other one for the region core. Despite the fact that the CFF universe is more complex it is build using the same input as the CC universe: </p> <pre><code>geniml build-universe ccf --coverage-folder coverage/ \\\n                           --output-file universe_ccf.bed\n</code></pre>"},{"location":"geniml/tutorials/create-consensus-peaks/#maximum-likelihood-universe","title":"Maximum likelihood universe","text":"<p>In the previous examples both CC anf CCF universes used simple likelihood model to calculate the cutoff. However, we also developed more complex likelihood model that takes into account the positions of starts and ends of the regions in the collection. This LH model can build based on coverage files and number of analyzed files:</p> <pre><code>geniml lh build_model --model-file model.tar \\\n                      --coverage-folder coverage/ \\\n                      --file-no `wc -l file_list.txt`\n</code></pre> <p>The resulting tar archiver contains LH model. This model can be used as a scoring function that assigns to each position probability of it being a start, core or end of a region. It can be both used for universe assessment and universe building. Combination of LH model and optimization algorithm for building flexible universes results in maximum likelihood universe (ML):</p> <pre><code>geniml build-universe ml --model-file model.tar \\\n                         --coverage-folder coverage/ \\\n                         --output-file universe_ml.bed \n</code></pre>"},{"location":"geniml/tutorials/create-consensus-peaks/#hmm","title":"HMM","text":"<p>The forth presented method of creating universes utilizes Hidden Markov Models (HMM). In this approach the parts of flexible regions are hidden states of the model, while genome coverage by the collections are emissions. The resulting universe is called Hidden Markov Model universe. It can be build only based on the genome coverage by the collection:</p> <pre><code>geniml build-universe hmm --coverage-folder coverage/ \\\n                          --output-file universe_hmm.bed\n</code></pre>"},{"location":"geniml/tutorials/create-consensus-peaks/#how-to-assess-new-universe","title":"How to assess new universe?","text":"<p>So far you used many different methods for creating new universes. But choosing, which universe represents data the best can be challenging. To help with this we created a tutorial that can be found here, which presents different  methods that assess universe fit to the collection of files.</p>"},{"location":"geniml/tutorials/evaluation/","title":"How to evaluate genomic region embeddings","text":""},{"location":"geniml/tutorials/evaluation/#preparation","title":"Preparation","text":""},{"location":"geniml/tutorials/evaluation/#create-a-base-embedding-object","title":"Create a Base Embedding Object","text":"<p>Given a set of genomic region embeddings <code>embeddings</code> and the corresponding regions <code>vocab</code>, use <code>BaseEmbeddings</code> to create an <code>base</code> embedding object.</p> <pre><code>from geniml.eval.utils import BaseEmbeddings\nimport pickle\n\nbase_obj = BaseEmbeddings(embeddings, vocab)\nwith open(\"base_embed.pt\", \"wb\") as f:\n    pickle.dump(base_obj, f)\n</code></pre>"},{"location":"geniml/tutorials/evaluation/#generate-binary-embeddings","title":"Generate Binary Embeddings","text":"<pre><code>from geniml.eval.utils import get_bin_embeddings\n\nuniverse_file = \"/path/to/universe.bed\"\ntoken_files = [\"file1.bed\", \"file2.bed\"]\nbin_embed = get_bin_embeddings(universe_file, token_files)\n</code></pre> <p>Or use command line:</p> <pre><code>geniml eval bin-gen --universe /path/to/universe.bed --token-folder /path/to/tokenized/folder --file-name bin_embed.pickle\n</code></pre>"},{"location":"geniml/tutorials/evaluation/#statistical-tests","title":"Statistical Tests","text":""},{"location":"geniml/tutorials/evaluation/#cluster-tendency-test-ctt","title":"Cluster Tendency Test (CTT)","text":"<p>CTT analyzes how well a set of region embeddings can be clustered.  CTT score lies between 0 and 1. A larger CTT score indicates a greater tendency for the embeddings being evaluated to have clusters. When the embeddings are uniformly distributed, the score is 0.5. For evenly spaced embeddings, the score approaches 0.</p> <pre><code>from geniml.eval.ctt import get_ctt_score, ctt_eval\n\npath = \"/path/to/a/region2vec/model/\"\nembed_type = \"region2vec\"\nctt_score = get_ctt_score(path, embed_type, seed=42, num_data=10000, num_workers=10)\nprint(ctt_score)\n\n# evaluate a batch of models and run CTT for 5 times with different random seeds\nbatch = [(path, embed_type)]\nctt_score_arr = ctt_eval(batch, num_runs=5, num_data=10000, num_workers=10)\nprint(f\"Model: {ctt_score_arr[0][0]}\\n CTT scores:{ctt_score_arr[0][1]}\")  # CTT scores for the 1st model in the batch\n</code></pre> <p>Or use the command line <pre><code>geniml eval ctt --model-path /path/to/a/region2vec/model/ --embed-type region2vec\n</code></pre></p>"},{"location":"geniml/tutorials/evaluation/#reconstruction-test-rct","title":"Reconstruction Test (RCT)","text":"<p>RCT evaluates how well an embedding of a region preserves the region\u2019s occurrence information in the training data. The best RCT score is 1.</p> <pre><code>from geniml.eval.rct import get_rct_score, rct_eval\n\npath = \"/path/to/a/region2vec/model/\"\nembed_type = \"region2vec\"\nbin_path = \"/path/to/a/binary/embedding/for/the/same/tokenized/files/\"\n# set out_dim to -1 use all the dimensions of the binary embeddings. Set out_dim to a small positive number to reduce computational complexity.\nrct_score = get_rct_score(path, embed_type, bin_path, out_dim=-1, cv_num=5, seed=42, num_workers=10)\nprint(rct_score)\n\n# evaluate a batch of models and run RCT for 5 times with different random seeds\nbatch = [(path, embed_type, bin_path)]\nrct_score_arr = rct_eval(batch, num_runs=5, cv_num=5, out_dim=-1, num_workers=10)\nprint(f\"Model: {rct_score_arr[0][0]}\\n RCT scores:{rct_score_arr[0][1]}\")  # RCT scores for the 1st model in the batch\n</code></pre> <p>Or use the command line  <pre><code>geniml eval rct --model-path /path/to/a/region2vec/model/ --embed-type region2vec\n</code></pre> To change the learning setting, go to the definition of <code>get_rct_score</code> in <code>geniml/eval/rct.py</code> and change the constructor of <code>MLPRegressor</code>.</p>"},{"location":"geniml/tutorials/evaluation/#biological-tests","title":"Biological Tests","text":""},{"location":"geniml/tutorials/evaluation/#genome-distance-scaling-test-gdst","title":"Genome Distance Scaling Test (GDST)","text":"<p>GDST calculates a score measuring how much the embedding distance between two regions scales the corresponding genome distance.</p> <pre><code>from geniml.eval.gdst import get_gdst_score, gdst_eval\n\npath = \"/path/to/a/region2vec/model/\"\nembed_type = \"region2vec\"\ngdst_score = get_gdst_score(path, embed_type, num_samples=10000, seed=42)\nprint(gdst_score)\n\n# evaluate a batch of models and run GDST for 5 times with different random seeds\nbatch = [(path, embed_type)]\ngdst_score_arr = gdst_eval(batch, num_runs=5, num_samples=10000)\n</code></pre> <p>Or use the command line  <pre><code>geniml eval gdst --model-path /path/to/a/region2vec/model/ --embed-type region2vec\n</code></pre></p>"},{"location":"geniml/tutorials/evaluation/#neighborhood-preserving-test-npt","title":"Neighborhood Preserving Test (NPT)","text":"<p>NPT evaluates how significant genomic region embeddings preserve their neighboring regions on the genome against random embeddings. The code output the NPT score for a set of region embeddings.</p> <pre><code>from geniml.eval.npt import get_npt_score, npt_eval\n\npath = \"/path/to/a/region2vec/model/\"\nembed_type = \"region2vec\"\nK = 10\n# If resolution = K gives NPT for K neighbors\n# If resolution &lt; K, gives NPT for [resolution, resolution*2, ...] neighbors\nresolution = K\nnpt_score = get_npt_score(path, embed_type, K, num_samples=100, seed=0, resolution=resolution, num_workers=10)\nprint(npt_score['SNPR'])\n\n# evaluate a batch of models and run NPT for 5 times with different random seeds\nbatch = [(path, embed_type)]\nnpt_score_arr = npt_eval(batch, K, num_samples=100, num_workers=10, num_runs=5, resolution=resolution)\nprint(f\"Model: {npt_score_arr[0][0]}\\n NPT scores: {npt_score_arr[0][1]}\")  # NPT scores for the 1st model in the batch\n</code></pre> <p>Or use the command line (the output will be the result when resolution=K) <pre><code>geniml eval npt --model-path /path/to/a/region2vec/model/ --embed-type region2vec --K 50 --num-samples 1000\n</code></pre></p>"},{"location":"geniml/tutorials/fine-tune-region2vec-model/","title":"How to fine-tune a Region2Vec model (Very experimental)","text":""},{"location":"geniml/tutorials/fine-tune-region2vec-model/#overview","title":"Overview","text":"<p>Fine-tuning a model is a way to adapt a pre-trained model to a new task. For example, we may want to fine-tune a model trained using unsupervised learning and ChIP-seq data to predict enhancers. This tutorial discusses how to fine-tune a pre-trained model. To learn how to train a new model see the region2vec training documentation.</p>"},{"location":"geniml/tutorials/fine-tune-region2vec-model/#get-a-pretrained-model","title":"Get a pretrained model","text":"<p>To begin, we need to get a pretrained model. We can get one from huggingface: <pre><code>from geniml.region2vec import Region2VecExModel\n\nmodel = Region2VecExModel(\"databio/r2v-ChIP-atlas-hg38-v2\")\n</code></pre> This will download the model from huggingface and load it into memory. The model is now ready to be fine-tuned. First we need to create a new classifier using the pretrained model: <pre><code>import torch\nimport torch.nn as nn\nimport torch.functional as F\n\n# enhancer classifier\nclass EnhancerClassifier(nn.Module):\n    def __init__(self, region2vec_model: torch.nn.Module):\n        super().__init__()\n        self.region2vec = region2vec_model\n        self.classification = nn.Sequential(\n            nn.Linear(region2vec_model.embedding_dim, 1),\n        )\n\n    def forward(self, x: torch.Tensor):\n        x = self.region2vec(x)  # Get the embeddings from Region2Vec\n        x = x.mean(dim=0)  # Average the embeddings (if multiple regions are passed in, this can occur due to tokenization)\n        x = nn.ReLU()(x)  # Pass through a non-linearity\n        x = self.classification(x)  # Pass through additional layers\n        return x\n</code></pre> After instantiating the tokenizer, we can can use the model like so: <pre><code>from geniml.io import Region\nfrom geniml.tokenization import ITTokenizer\n\nr = Region(\"chr1\", 1_000_000, 1_000_500) # some enhancer region (maybe)\n\ntokenizer = ITTokenizer.from_pretrained(\"databio/r2v-ChIP-atlas-hg38-v2\")\nclassifier = EnhancerClassifier(model.model) # get the inner core of the model\n\nx = tokenizer.tokenize(r)\nx = torch.tensor([t.id for t in x], dtype=torch.long)\nout = classifier(x)\n\nout.shape # torch.Size([1])\n\n# apply sigmoid\nout = torch.sigmoid(out)\n\nprint(\"Enhancer probability:\", round(out.item(), 3))\n</code></pre></p>"},{"location":"geniml/tutorials/fine-tune-region2vec-model/#saving-the-fine-tuned-embeddings","title":"Saving the fine-tuned embeddings","text":"<p><code>torch</code>'s computational graph links the original region2vec model back to the <code>Region2VecExModel</code>. Therefore, if we want to save the fine-tuned embeddings, we simply ned to call <code>export</code> on the original model: <pre><code>model.export(\"my-fine-tuned-model\")\n</code></pre></p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/","title":"Using our models with SnapATAC2","text":""},{"location":"geniml/tutorials/integrate-with-snapatac2/#overview","title":"Overview","text":"<p>SnapATAC2 is a flexible, versatile, and scalable single-cell omics analysis framework. It is designed to process and analyze single-cell ATAC-seq data. SnapATAC2 is written in Rust with Python bindings. It seemlessly integrates with <code>scanpy</code> and <code>anndata</code> objects. Therefore, it is extremely easy to use <code>geniml</code> models with SnapATAC2. Here's how you can do it:</p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/#install-tools","title":"Install tools","text":"<p>Ensure that you have <code>geniml</code> and <code>SnapATAC2</code> installed. You can install both using <code>pip</code>: <pre><code>pip install geniml snapatac2\n</code></pre></p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/#download-some-data","title":"Download some data","text":"<p>To get started, let's download some single-cell ATAC-seq data. We will use the 10x Genomics PBMC 10k dataset. The dataset contains 10,000 peripheral blood mononuclear cells (PBMCs) from a healthy donor.</p> <p>You can easily grab the fragment files like so: <pre><code>wget \"https://cf.10xgenomics.com/samples/cell-atac/2.1.0/10k_pbmc_ATACv2_nextgem_Chromium_Controller/10k_pbmc_ATACv2_nextgem_Chromium_Controller_fragments.tsv.gz\" -O pbmc_fragments.tsv.gz\n</code></pre></p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/#pre-process-with-snapatac2","title":"Pre-process with SnapATAC2","text":"<p>Lets start by pre-processing the data with SnapATAC2. We will closely follow the SnapATAC2 tutorial to get the data into an <code>anndata</code> object.</p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/#import-the-data","title":"Import the data","text":"<p>Lets import the data into <code>snapatac2</code>: <pre><code>from pathlib import Path\nimport snapatac2 as snap\n\nfragment_file = Path(\"pbmc_fragments.tsv.gz\")\ndata = snap.pp.import_data(\n    fragment_file,\n    chrom_sizes=snap.genome.hg38,\n    file=\"pbmc.h5ad\",  # Optional\n    sorted_by_barcode=False,\n)\n</code></pre></p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/#run-some-basic-quality-control","title":"Run some basic quality control","text":"<p>Using the <code>snapatac2</code> quality control functions, we can quickly assess the quality of the data:</p> <pre><code>snap.pl.frag_size_distr(data, interactive=False)\nfig = snap.pl.frag_size_distr(data, show=False)\nfig.update_yaxes(type=\"log\")\nfig.show()\n\nsnap.metrics.tsse(data, snap.genome.hg38)\nsnap.pl.tsse(data, interactive=False)\n\nsnap.pp.filter_cells(data, min_counts=5000, min_tsse=10, max_counts=100000)\n</code></pre> <p>Next, we can add a tile matrix to the data, select features, and run <code>scrublet</code> which is a doublet detection algorithm: <pre><code>snap.pp.add_tile_matrix(data)\nsnap.pp.select_features(data, n_features=250000)\nsnap.pp.scrublet(data)\n\n# actually filter the cells\nsnap.pp.filter_doublets(data)\n</code></pre></p> <p>With this, we have a clean <code>anndata</code> object that we can use with <code>geniml</code>.</p>"},{"location":"geniml/tutorials/integrate-with-snapatac2/#analyze-with-geniml","title":"Analyze with geniml","text":"<p>We will use a Region2Vec model to cluster the cells by generating embeddings. This PBMC data comes from peripheral blood mononuclear cells (PBMCs) from a healthy donor. As such. we will use the <code>databio/r2v-luecken2021-hg38-v2</code> model to generate embeddings because it contains embeddings for the Luecken2021 dataset, a first-of-its-kind multimodal benchmark dataset of 120,000 single cells from the human bone marrow of 10 diverse donors measured with two commercially-available multi-modal technologies: nuclear GEX with joint ATAC, and cellular GEX with joint ADT profiles.</p> <pre><code>import numpy as np\nimport scanpy as sc\nfrom geniml.scembed import ScEmbed\n\nadata = sc.read_h5ad(\"pbmc.h5ad\")\nmodel = ScEmbed(\"databio/r2v-luecken2021-hg38-v2\")\n\nadata.obsm['scembed_X'] = np.array(model.encode(adata))\n</code></pre> <p>With the embeddings, we can run a usual workflow like UMAP, clustering, and visualization: <pre><code>sc.pp.neighbors(adata, use_rep=\"scembed_X\")\nsc.tl.umap(adata)\n\nsc.tl.leiden(adata)\nsc.pl.umap(adata, color=\"leiden\")\n</code></pre></p> <p>And that's it! You've now used <code>geniml</code> with SnapATAC2. You can use the embeddings to annotate cell types, or perform other analyses. If you want to learn more about this, check out the cell-type annotation tutorial.</p>"},{"location":"geniml/tutorials/load-qdrant-with-cell-embeddings/","title":"How to load a vector database with cell embeddings","text":""},{"location":"geniml/tutorials/load-qdrant-with-cell-embeddings/#overview","title":"Overview","text":"<p>In this tutorial, we will show how to load a vector database with cell embeddings. There are many benefits to storing cell-embeddings in a vector database: 1. Speed: Loading a vector database is much faster than re-encoding cells. 2. Reproducibility: You can share your cell embeddings with others. 3. Flexibility: You can use the same cell embeddings for many different analyses. 4. Interoperability: You can use the same cell embeddings with many different tools.</p> <p>In a subsequent tutorial, we will show how to use a vector database to query cell embeddings and annotate cells with cell-type labels using a KNN classification algorithm.</p>"},{"location":"geniml/tutorials/load-qdrant-with-cell-embeddings/#preqrequisites","title":"Preqrequisites","text":"<p>There are two core components to this tutorial: 1) the pre-trained model, and 2) the vector database.</p> <p>Pre-trained model: I will be using the <code>databio/luecken2021</code> model. It was trained on the Luecken2021 dataset, a first-of-its-kind multimodal benchmark dataset of 120,000 single cells from the human bone marrow of 10 diverse donors measured with two commercially-available multi-modal technologies: nuclear GEX with joint ATAC, and cellular GEX with joint ADT profiles.</p> <p>Vector database: Vector databases are a new and exciting technology that allow you to store and query high-dimensional vectors very quickly. This tutorial will use the <code>qdrant</code> vector database. As a lab, we really like <code>qdrant</code> because it is fast, easy to use, and has a great API. You can learn more about <code>qdrant</code> here. For <code>qdrant</code> setup, please refer to the qdrant documentation. In the end, you should have a running <code>qdrant</code> instance at <code>http://localhost:6333</code>.</p>"},{"location":"geniml/tutorials/load-qdrant-with-cell-embeddings/#data-preparation","title":"Data preparation","text":"<p>Grab a fresh copy of the Luecken2021 data from the geo accession. We want the <code>multiome</code> data. This dataset contains the binary accessibility matrix, the peaks, and the barcodes. It also conveniently contains the cell-type labels. Pre-trained models also requires that the data be in a <code>scanpy.AnnData</code> format and the <code>.var</code> attribute contain <code>chr</code>, <code>start</code>, and <code>end</code> values.</p> <pre><code>import scanpy as sc\n\nadata = sc.read_h5ad(\"path/to/adata.h5ad\")\nadata = adata[:, adata.var['feature_types'] == 'ATAC']\n</code></pre>"},{"location":"geniml/tutorials/load-qdrant-with-cell-embeddings/#getting-embeddings","title":"Getting embeddings","text":"<p>We can easily get embeddings of the dataset using the pre-trained model:</p> <pre><code>import scanpy as sc\n\nfrom geniml.scembed import ScEmbed\n\nadata = sc.read_h5ad(\"path/to/adata.h5ad\")\n\nmodel = ScEmbed(\"databio/r2v-luecken2021-hg38-v2\")\nembeddings = model.encode(adata)\n\nadata.obsm['scembed_X'] = np.array(embeddings)\n</code></pre>"},{"location":"geniml/tutorials/load-qdrant-with-cell-embeddings/#loading-the-vector-database","title":"Loading the vector database","text":"<p>With the embeddings, we can now upsert them to <code>qdrant</code>. Ensure you have <code>qdrant_client</code> installed:</p> <pre><code>pip install qdrant-client\n</code></pre> <pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.http.models import Distance, VectorParams, PointStruct\n\nclient = QdrantClient(\"localhost\", port=6333)\n\nclient.create_collection(\n    collection_name=\"luecken2021\",\n    vectors_config=VectorParams(size=embeddings.shape[1], distance=Distance.DOT),\n)\n\nembeddings, cell_types = adata.obsm['scembed_X'], adata.obs['cell_type']\n\npoints = []\nfor embedding, cell_type, i in zip(embeddings, cell_types, range(len(embeddings)):\n    points.append(\n        PointStruct(\n            id=adata.obs.index[i],\n            vector=embedding.tolist(),\n            payload={\"cell_type\": cell_type}\n\n    ))\n\n\nclient.upsert(collection_name=\"luecken2021\", points=points, wait=True)\n</code></pre> <p>You should now have a vector database with cell embeddings. In the next tutorial, we will show how to use this vector database to query cell embeddings and annotate cells with cell-type labels using a KNN classification algorithm.</p>"},{"location":"geniml/tutorials/pre-tokenization/","title":"Pre-tokening training data","text":""},{"location":"geniml/tutorials/pre-tokenization/#overview","title":"Overview","text":"<p>Before we train a model, we must do what is called pre-tokenization. Pre-tokenziation is the process of converting raw genomic region data into lists of tokens and saved into a special file format we call <code>.gtok</code> (genomic token) files. These are binary files that contain the tokenized data in the form of integers. The <code>.gtok</code> files are used directly to train the model. There are several benefits to this, including: 1. Speed: The tokenization process can be slow, especially when you have a lot of data. By pre-tokenizing the data, you only have to do this once. Then, you can use the <code>.gtok</code> files to train the model as many times as you want. 2. Memory: The <code>.gtok</code> files are much smaller than the original data. This means that you can store more data in memory and train larger models. Moreover, this enables streaming the data from disk, which is useful when you have a lot of data. 3. Reproducibility: By saving the tokenized data, you can ensure that the same data is used to train the model every time. This is important for reproducibility.</p>"},{"location":"geniml/tutorials/pre-tokenization/#how-to-pretokenize-data","title":"How to pretokenize data","text":"<p>Pretokenizing data is easy. You can use the built-in tokenizers and utilities in <code>geniml</code> to do this. Here is an example of how to pretokenize a bed file:</p> <pre><code>from genimtools.utils import write_tokens_to_gtok\nfrom geniml.tokenization import ITTokenizer\n\n# instantiate a tokenizer\ntokenizer = ITTokenizer(\"path/to/universe.bed\")\n\n# get tokens\ntokens = tokenizer.tokenize(\"path/to/bedfile.bed\")\nwrite_tokens_to_gtok(tokens.ids, \"path/to/bedfile.gtok\")\n</code></pre> <p>Thats it! Now you can use the <code>.gtok</code> file to train a model.</p>"},{"location":"geniml/tutorials/pre-tokenization/#how-to-use-the-gtok-files","title":"How to use the <code>.gtok</code> files","text":"<p>To facilitate working with <code>.gtok</code> files, we have some helper-classes that can be used to train a model directly from <code>.gtok</code> files. For example, you can use teh <code>Region2VecDataset</code> class to load the <code>.gtok</code> files and train a model. See the training documentation for more information.</p> <pre><code>from geniml.region2vec.utils import Region2VecDataset\n\ntokens_dir = \"path/to/tokens\"\ndataset = Region2VecDataset(tokens_dir)\n\nfor tokens in dataset:\n    # train the model\n    print(tokens) # [42, 101, 99, ...]\n</code></pre>"},{"location":"geniml/tutorials/pre-tokenization/#caveats-and-considerations","title":"Caveats and considerations","text":"<p>When pretokenizing data, you should consider the following: 1. Tokens are specific to the universe that the tokenizer was trained on. If you use a different universe, you will get different tokens. This means that you should use the same universe to pretokenize the data as you will use to train the model. 2. The <code>.gtok</code> files are binary files. This means that they are not human-readable. You should keep the original bed files as well as the <code>.gtok</code> files. This is important for reproducibility and for debugging.</p>"},{"location":"geniml/tutorials/pyBiocFileCache/","title":"How to use bbclient cache in R","text":"<p>The bbclient caching system is supported in both Python and R.  Both of these caching systems are compatible with <code>BiocFileCache</code> for BED files and <code>Zarr</code> with BED tokens.</p>"},{"location":"geniml/tutorials/pyBiocFileCache/#bed-caching","title":"Bed caching","text":""},{"location":"geniml/tutorials/pyBiocFileCache/#step-1-cache-bed-file-using-bbclient-cli","title":"Step 1: Cache BED File Using bbclient CLI","text":"<pre><code>geniml bbclient cache-bed bbad85f21962bb8d972444f7f9a3a932\n</code></pre> <p>The cached file is saved in the default bbclient cache folder, or a user-provided cache folder.  Since we didn't provide a caching folder, the default folder will be in our home directory.</p>"},{"location":"geniml/tutorials/pyBiocFileCache/#step-2-get-path-to-the-bed-file-using-python","title":"Step 2: Get Path to the BED File Using Python","text":""},{"location":"geniml/tutorials/pyBiocFileCache/#a-using-bbclient-in-python","title":"a. Using bbclient in Python","text":"<p>Here's an example of how you can retrieve the path to the BED file in Python: <pre><code>from geniml.bbclient import BBClient\n\nbbc = BBClient()\nprint(bbc.seek(\"bbad85f21962bb8d972444f7f9a3a932\"))\n</code></pre></p> <p>And it will print path to our file: <pre><code>'/home/bnt4me/.bbcache/bedfiles/b/b/bbad85f21962bb8d972444f7f9a3a932.bed.gz'\n</code></pre></p> <p>b. Using pyBiocFileCache</p> <p><pre><code>import os\n\nfrom pybiocfilecache import BiocFileCache\n\n# get cache folder\nbbcache_folder = os.path.join(os.path.expanduser(\"~\"), \".bbcache\")\n\n# get bedfile cache folder \nbedfile_cache_folder = os.path.join(bbcache_folder, \"bedfiles\")\n\nbio_cache = BiocFileCache(bedfile_cache_folder)\n\nbed_cache_obj = bio_cache.get(\"bbad85f21962bb8d972444f7f9a3a932\")\n\nprint(bed_cache_obj.fpath)\n</code></pre> And it will print path to our file: <pre><code>'/home/bnt4me/.bbcache/bedfiles/b/b/bbad85f21962bb8d972444f7f9a3a932.bed.gz'\n</code></pre></p> <ol> <li>Get path using R caching system - BiocFileCache</li> </ol> <p><pre><code># Install necessary package\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n  install.packages(\"BiocManager\")\n}\nBiocManager::install(\"BiocFileCache\")\n\n# Load necessary libraries\nlibrary(BiocFileCache)\n\n# Get cache folder\nbbcache_folder &lt;- file.path(Sys.getenv(\"HOME\"), \".bbcache\")\n\n# Get bedfile cache folder\nbedfile_cache_folder &lt;- file.path(bbcache_folder, \"bedfiles\")\n\n# Create a BiocFileCache object\nbio_cache &lt;- BiocFileCache(bedfile_cache_folder)\n\n# Get the cached file using the identifier\nbed_cache_obj &lt;- bfcrpath(bio_cache, \"bbad85f21962bb8d972444f7f9a3a932\")\n\n# Print the file path\nprint(bed_cache_obj)\n</code></pre> And we will get this message printed: <pre><code>[1] \"/home/bnt4me/.bbcache/bedfiles/b/b/bbad85f21962bb8d972444f7f9a3a932.bed.gz\"\n</code></pre></p> <p>This R script will perform the same tasks as the Python script: handling the cache and retrieving the specified BED file from the cache.</p> <p>By following these steps, you can efficiently manage and retrieve cached BED files using the bbclient caching system in both Python and R.</p> <p>P.S. All links will be different on your machine, as they are generated based on your local home directory.</p>"},{"location":"geniml/tutorials/pyBiocFileCache/#caching-of-tokenized-bed-files","title":"Caching of tokenized bed files","text":"<p>To store tokenized BED files, we use the Zarr format.  BBClient saves tokenized files in the zarr folder within the bbcache folder (which is located in our home directory if the bbcache folder is not specified).</p> <p>Here's an example of how you can download and cache tokenized bed file using bbclient: <pre><code>from geniml.bbclient import BBClient\n\nbbc = BBClient()\nbbc.add_bed_tokens_to_cache( bed_id= '0dcdf8986a72a3d85805bbc9493a1302', universe_id= '58dee1672b7e581c8e1312bd4ca6b3c7')\n</code></pre> If user didn't get any error, the tokenized file is saved in the default bbclient cache folder, or a user-provided cache folder.</p>"},{"location":"geniml/tutorials/pyBiocFileCache/#step-1-get-zarr-tokenized-bed-file-using-bbclient-in-python","title":"Step 1: Get zarr tokenized bed file using bbclient in Python","text":"<p><pre><code>from geniml.bbclient import BBClient\n\nbbc = BBClient()\ntokens_arr = bbc.load_bed_tokens(bed_id= '0dcdf8986a72a3d85805bbc9493a1302', universe_id= '58dee1672b7e581c8e1312bd4ca6b3c7')\n\nprint(tokens_arr)\n</code></pre> Result is a zarr array object: <pre><code>&lt;zarr.core.Array '/58dee1672b7e581c8e1312bd4ca6b3c7/0dcdf8986a72a3d85805bbc9493a1302' (29438,) int64&gt;\n</code></pre></p>"},{"location":"geniml/tutorials/pyBiocFileCache/#step-2-get-zarr-tokenized-bed-file-using-python-zarr-library","title":"Step 2: Get zarr tokenized bed file using Python zarr library","text":"<p><pre><code>import os \nimport zarr\n\n\nbbcache_folder = os.path.join(os.path.expanduser(\"~\"), \".bbcache\")\n\n# get zarr cache folder \nbedfile_cache_folder = os.path.join(bbcache_folder, \"tokens.zarr\")\n\nzarr_cache = zarr.group(bedfile_cache_folder)\n\nuniverse_id = \"58dee1672b7e581c8e1312bd4ca6b3c7\"\nbed_id = \"0dcdf8986a72a3d85805bbc9493a1302\"\n\ntokens_arr = zarr_cache[universe_id][bed_id]\nprint(tokens_arr)\n</code></pre> Result is a zarr array object: <pre><code>&lt;zarr.core.Array '/58dee1672b7e581c8e1312bd4ca6b3c7/0dcdf8986a72a3d85805bbc9493a1302' (29438,) int64&gt;\n</code></pre></p>"},{"location":"geniml/tutorials/pyBiocFileCache/#step-3-get-zarr-tokenized-bed-file-using-r-zarr-library","title":"Step 3: Get zarr tokenized bed file using R zarr library","text":"<p><pre><code>## we need BiocManager to perform the installation\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n## install Rarr\nBiocManager::install(\"Rarr\")\n\nlibrary(Rarr)\n</code></pre> \ud83d\udea7 Use <code>Rarr</code> library to open zarr file: https://github.com/grimbough/Rarr?tab=readme-ov-file</p>"},{"location":"geniml/tutorials/region2vec/","title":"How to train Region2Vec interval embeddings","text":"<p><code>Region2Vec</code> is an unsupervised method for creating embeddings for genomic regions and region sets from a set of raw BED files. The program will first map all raw regions to a given universe (vocabulary) set. Then, it will construct sentences by concatenating regions from a BED file in random order. The generated sentences will be used for Region2Vec training using word2vec.</p>"},{"location":"geniml/tutorials/region2vec/#usage","title":"Usage","text":"<ol> <li>Prepare a set of bed files in <code>src_folder</code>. [Optional] If only a subset of files will be used, specify a list of those files as <code>file_list</code>. By default, the program will use all the files in the folder to train a Region2Vec model.</li> <li>Prepare a universe file <code>universe_file</code>.</li> <li>Create a token folder which will be used to store tokenized files <code>dst_folder</code>.</li> <li>Run the following command <pre><code>from geniml.tokenization import hard_tokenization\nfrom geniml.region2vec import region2vec\n\nsrc_folder = '/path/to/raw/bed/files'\ndst_folder = '/path/to/tokenized_files'\nuniverse_file = '/path/to/universe_file'\n\n# must run tokenization first\nstatus = hard_tokenization(src_folder, dst_folder, universe_file, 1e-9)\n\nif status: # if hard_tokenization is successful, then run Region2Vec training\n    save_dir = '/path/to/training/results'\n    region2vec(dst_folder, save_dir, num_shufflings=1000)\n</code></pre> For customized settings, please go and check the parameters used in <code>main.py</code>.  For training a Region2Vec model, the parameters, <code>init_lr</code>, <code>window_size</code>, <code>num_shufflings</code>, <code>embedding_dim</code>, are frequently tuned in experiments.</li> </ol> <p>For command line usage, type <code>geniml region2vec --help</code> for details. We give a simple usage below</p> <pre><code>geniml region2vec \n  --token-folder /path/to/token/folder \\\n  --save-dir ./region2vec_model \\\n  --num-shuffle 10 \\\n  --embed-dim 100 \\\n  --context-len 50 \n</code></pre>"},{"location":"geniml/tutorials/text2bednn-search-interface/","title":"How to create a natural language search backend for BED files","text":"<p>The metadata of each BED file is needed to build a natural language search backend. BED files embedding vectors are created by <code>Region2Vec</code> model, and metadata embedding vectors are created by <code>FastEmbed</code>, <code>SentenceTransformers</code>, or other text embedding models.</p> <p><code>Vec2VecFNN</code>, a feedforward neural network (FNN), is trained to maps vectors from the embedding space of natural language to the embedding space of BED files. When a natural language query string is given, it will first be encoded to a vector by the text embedding model, and then created  vector will be encoded to a query vector by the FNN. <code>search</code> backend can perform k-nearest neighbors (KNN) search among the stored BED file embedding vectors, and the BED files whose embedding vectors are closest to that query vector are the search results.</p>"},{"location":"geniml/tutorials/text2bednn-search-interface/#search-distance-metrics","title":"Search distance metrics","text":"<p>The default distance metrics for KNN search in <code>geniml</code> is cosine similarity. Which is bounded in [0,1]. The smaller the value is, the higher similarity between the query vector and returend search results. HNSWBackend and QdrantBackend also have other options of distance metrics.</p>"},{"location":"geniml/tutorials/text2bednn-search-interface/#store-embedding-vectors","title":"Store embedding vectors","text":"<p>It is recommended to use <code>geniml.search.backend.HNSWBackend</code> to store embedding vectors. In the <code>HNSWBackend</code> that stores each BED file embedding vector, the <code>payload</code> should contain the name or identifier of BED file. In the <code>HNSWBackend</code> that stores the embedding vectors of each  metadata string, the <code>payload</code> should contain the original string text and the names of BED files that have that string in metadata.</p>"},{"location":"geniml/tutorials/text2bednn-search-interface/#train-the-model","title":"Train the model","text":"<p>Training a <code>Vec2VecFNN</code> needs x-y pairs of vectors (x: metadata embedding vector; y: BED embedding vector). A pair of a metadata embedding vector with the embedding vectors of BED files in its payload is a target pair, otherwise a non-target pair. Non-target pairs are sampled for contrastive loss. Here is sample code to generate pairs from storage backend and train the model:</p> <pre><code># target is an array of 1 (target) and -1 (non-target) \nX, Y, target = vec_pairs(\n    nl_backend,  # HNSWBackend that store metadata embedding vectors\n    bed_backend,  # HNSWBackend that store BED embedding vectors\n    \"name\",  # key to file name in BED backend payloads\n    \"files\",  # key to matching files in metadata backend payloads\n    True,  # sample non-target pairs\n    1.0  # number of non-target pairs /number of target pairs = 1\n)\n\n# train without validate data\nv2v_torch_contrast.train(\n    X,\n    Y,\n    folder_path=\"path/to/folder/for/checkpoint\",\n    loss_func=\"cosine_embedding_loss\",  # right now \"cosine_embedding_loss\" is the only contrastive loss function available\n    training_target=target,\n)\n</code></pre>"},{"location":"geniml/tutorials/text2bednn-search-interface/#search-interface","title":"Search interface","text":"<p>A search interface consists of a storage backend where vectors are stored, and a module (<code>geniml.search.query2vec</code>) that embed the query. <code>geniml.search</code> supports two types of queries: region set query and text query. </p>"},{"location":"geniml/tutorials/text2bednn-search-interface/#region-set-query","title":"Region set query","text":"<p><code>BED2Vec</code> embed the query region set with a <code>Region2VecExModel</code>, and the embedding vector is used to perform KNN search within the backend.</p> <pre><code>from geniml.search import BED2BEDSearchInterface, BED2Vec\n\n# init BED2Vec with a hugging face repo of a Region2VecExModel\nbed2vec = BED2Vec(\"databio/r2v-ChIP-atlas-hg38-v2\")\n\n# the search_backend can be QdrantBackend or HNSWBackend\nsearch_interface = BED2BEDSearchInterface(search_backend, bed2vec)\n\n# the query cam be a RegionSet object (see geniml.io) or path to a BED file in disk\nfile_search_result = search_interface.query_search(\"path/to/a/bed/file.bed\", 5)\n</code></pre>"},{"location":"geniml/tutorials/text2bednn-search-interface/#text-query","title":"Text query","text":"<p><code>Text2Vec</code> embed the query string with a with a natural language embedding model first (default: <code>FlagEmbedding</code>), and then maps the text embedding vector into the embedding space of region sets through a trained <code>Vec2VecFNN</code>.</p> <pre><code>from geniml.search import Text2BEDSearchInterface, Text2Vec\n\ntext2vec = Text2Vec(\n    \"sentence-transformers/all-MiniLM-L6-v2\",  # either a hugging face repo or an object from geniml.text2bednn.embedder\n    \"databio/v2v-geo-hg38\"  # either a hugging face repo or a Vec2VecFNN\n)\n\nsearch_interface = Text2BEDSearchInterface(search_backend, text2vec)\ntext_search_result = search_interface.query_search(\"cancer cells\", 5)\n</code></pre> <p>With a dictionary that contains query strings and id of relevant query results in search backend in this format:</p> <pre><code>{\n    &lt;query string&gt;: [\n        &lt;id of relevant result in backend&gt;,\n        ...    \n    ],\n    ...\n}\n</code></pre> <p><code>Text2BEDSearchInterface</code> can return mean average precision, average AUC-ROC, and average R-Precision, here is example code:</p> <pre><code>query_dict = {\n    \"metadata string 1\": [2, 3],\n    \"metadata string 12\": [1],\n    \"metadata string 3\": [2, 4, 5],\n    \"metadata string 1\": [0]\n}\n\nMAP, AUC, RP = search_interface.eval(query_dict)\n</code></pre>"},{"location":"geniml/tutorials/tokenization/","title":"How to use the tokenizers","text":""},{"location":"geniml/tutorials/tokenization/#overview","title":"Overview","text":"<p>The <code>geniml</code> tokenizers are used to prepare data for training, evaluation, and inference of genomic machine learning models. Like tokenizers for natural langauge processing, the <code>geniml</code> tokenizers convert raw data into a format that can be used by our models. <code>geniml</code> has a few tokenizers, but they all follow the same principles.</p> <p>All tokenizers require a universe file (or, vocab file). This is a bedfile that contains all possible regions that can be tokenized. It may also include special tokens like the start, end, unknown, and padding token.</p> <p>Our tokenizers are implemented in Rust for speed and efficiency. They exist in the <code>geniml</code> companion library called <code>genimtools</code>. Currently, there are two tokenizers available: the TreeTokenizer, and the AnnDataTokenizer. The TreeTokenizer is a simple and flexible tokenizer that can be used for any type of data. The AnnDataTokenizer is specifically designed for use with single-cell AnnData objects from the <code>anndata</code> library.</p> <p>The API is loosely based on the <code>transformers</code> library, so it should be familiar to users of that library.</p>"},{"location":"geniml/tutorials/tokenization/#using-the-tokenizers","title":"Using the tokenizers","text":"<p>To start using a tokenizer, simply pass it an appropriate universe file:</p> <pre><code>from geniml.tokenization import TreeTokenizer # or any other tokenizer\nfrom geniml.io import RegionSet\n\nrs = RegionSet(\"/path/to/file.bed\")\nt = TreeTokenizer(\"/path/to/universe.bed\")\n\ntokens = t(rs)\nfor token in tokens:\n    print(f\"{t.chr}:{t.start}-{t.end}\")\n</code></pre> <p>You can also get token ids for the tokens:</p> <pre><code>from geniml.tokenization import ITTokenizer # or any other tokenizer\nfrom geniml.io import RegionSet\n\nrs = RegionSet(\"/path/to/file.bed\")\nt = ITTokenizer(\"/path/to/universe.bed\")\n\nmodel = Region2Vec(len(t), 100) # 100 dimensional embedding\ntokens = t(rs))\n\nids = tokens.to_ids()\n</code></pre>"},{"location":"geniml/tutorials/tokenization/#future-work","title":"Future work","text":"<p>Genomic region tokenization is an active area of research. We will implement new tokenizers as they are developed. If you have a tokenizer you'd like to see implemented, please open an issue or submit a pull request.</p> <p>For core development of our tokenizers, see the gtokenizers repository.</p>"},{"location":"geniml/tutorials/train-region2vec/","title":"How to train a new Region2Vec model","text":"<p>Region2Vec is an unsupervised method for creating embeddings of genomic regions and genomic region sets. This tutorial discusses how to train a new model. To learn how to use a pre-trained model see the region2vec usage documentation.</p>"},{"location":"geniml/tutorials/train-region2vec/#training-data-and-universe","title":"Training data and universe","text":"<p>Training a model requires two things: 1) a set of pre-tokenized data and 2) a universe. The universe is a set of regions that the model will be trained on. The universe is used to create the tokenizer, which is used to convert the raw data into tokens. The universe should be representative of the data that you will be training the model on. For example, if you are training a model on human data, you should use a universe that contains human regions. If you dont have a universe, a safe bet is to use the 1000 tiles hg38 genome.</p> <p>You can download the 1000 tiles hg38 genome here.</p> <p>The pre-tokenized data is a set of <code>.gtok</code> files. These are binary files that contain the tokenized data in the form of integers. The <code>.gtok</code> files are used directly to train the model. If you have not pre-tokenized your data, see the pre-tokenization documentation.</p>"},{"location":"geniml/tutorials/train-region2vec/#training-a-model","title":"Training a model","text":""},{"location":"geniml/tutorials/train-region2vec/#instantiate-a-new-model","title":"Instantiate a new model","text":"<p>To begin, create a new model from <code>Region2VecExModel</code>.</p> <p>Note: We use the <code>Region2VecExModel</code> because it is an extension of the <code>Region2Vec</code> class that comes with its own tokenizer. This ensures that the model and tokenizer are compatible.</p> <pre><code>import logging\nimport os\nfrom multiprocessing import cpu_count\n\nfrom geniml.io import RegionSet\nfrom geniml.tokenization import ITTokenizer\nfrom geniml.region2vec import Region2VecExModel\nfrom rich.progress import track\n\n\nlogging.basicConfig(level=logging.INFO)\n\n# get the paths to data\nuniverse_path = os.path.expandvars(\"$RESOURCES/regions/genome_tiles/tiles1000.hg38.bed\")\ndata_path = os.path.expandvars(\"$DATA/ChIP-Atlas/hg38/ATAC_seq/tokens\")\n\nmodel = Region2VecExModel(\n    tokenizer=ITTokenizer(universe_path),\n)\n</code></pre>"},{"location":"geniml/tutorials/train-region2vec/#training-data","title":"Training data","text":"<p>The training data is a set of <code>.gtok</code> files. You can use the <code>Region2VecDataset</code> class to load the <code>.gtok</code> files and train the model. <pre><code>from geniml.region2vec.utils import Region2VecDataset\n\ndataset = Region2VecDataset(data_path)\n</code></pre></p>"},{"location":"geniml/tutorials/train-region2vec/#training","title":"Training","text":"<p>Now, simply give the model the list of <code>RegionSet</code>s and run the training: <pre><code># train the model\nmodel.train(dataset, epochs=100)\n</code></pre></p> <p>You can export your model using the <code>export</code> function:</p> <pre><code>model.export(\"out\")\n</code></pre> <p>These files are intended to be directly uploaded to huggingface. You can upload them using the <code>huggingface-cli</code> or the huggingface website.</p>"},{"location":"geniml/tutorials/train-scembed-model/","title":"How to train a single-cell model with scEmbed","text":"<p>This example walks you through training an <code>scembed</code> region2vec model on a single-cell dataset. We start with data preparation, then train the model, and finally use the model to cluster the cells.</p> <p>For this example we are using the 10x Genomics PBMC 10k dataset. The dataset contains 10,000 peripheral blood mononuclear cells (PBMCs) from a healthy donor.</p>"},{"location":"geniml/tutorials/train-scembed-model/#installation","title":"Installation","text":"<p>Simply install the parent package <code>geniml</code> from PyPi:</p> <pre><code>pip install geniml\n</code></pre> <p>Then import <code>scEmbed</code> from <code>geniml</code>:</p> <pre><code>from geniml.scembed import ScEmbed\n</code></pre>"},{"location":"geniml/tutorials/train-scembed-model/#data-preparation","title":"Data preparation","text":"<p><code>scembed</code> requires that the input data is in the AnnData format. Moreover, the <code>.var</code> attribute of this object must have <code>chr</code>, <code>start</code>, and <code>end</code> values. The reason is two fold: 1) we can track which vectors belong to which genmomic regions, and 2) region vectors are now reusable. We ned three files: 1) The <code>barcodes.txt</code> file, 2) the <code>peaks.bed</code> file, and 3) the <code>matrix.mtx</code> file. These will be used to create the <code>AnnData</code> object. To begin, download the data from the 10x Genomics website:</p> <pre><code>wget https://cf.10xgenomics.com/samples/cell-atac/2.1.0/10k_pbmc_ATACv2_nextgem_Chromium_Controller/10k_pbmc_ATACv2_nextgem_Chromium_Controller_raw_peak_bc_matrix.tar.gz\ntar -xzf 10k_pbmc_ATACv2_nextgem_Chromium_Controller_raw_peak_bc_matrix.tar.gz\n</code></pre> <p>Your files will be inside <code>filtered_peak_bc_matrix/</code>. Assuming you've installed the proper dependencies, you can now use python to build the <code>AnnData</code> object:</p> <pre><code>import pandas as pd\nimport scanpy as sc\n\nfrom scipy.io import mmread\nfrom scipy.sparse import csr_matrix\n\nbarcodes = pd.read_csv(\"barcodes.tsv\", sep=\"\\t\", header=None, names=[\"barcode\"])\npeaks = pd.read_csv(\"peaks.bed\", sep=\"\\t\", header=None, names=[\"chr\", \"start\", \"end\"])\nmtx = mmread(\"matrix.mtx\")\nmtx_sparse = csr_matrix(mtx)\nmtx_sparse = mtx_sparse.T\n\nadata = sc.AnnData(X=mtx_sparse, obs=barcodes, var=peaks)\nadata.write_h5ad(\"pbmc.h5ad\")\n</code></pre> <p>We will use the <code>pbmc.h5ad</code> file for downstream work.</p>"},{"location":"geniml/tutorials/train-scembed-model/#training","title":"Training","text":"<p>Training an <code>scEmbed</code> model requires two key steps: 1) pre-tokenizing the data, and 2) training the model.</p>"},{"location":"geniml/tutorials/train-scembed-model/#pre-tokenizing-the-data","title":"Pre-tokenizing the data","text":"<p>To learn more about pre-tokenizing the data, see the pre-tokenization tutorial. Pre-tokenization offers many benefits, the two most important being 1) speeding up training, and 2) lower resource requirements. The pre-tokenization process is simple and can be done with a combination of <code>geniml</code> and <code>genimtools</code> utilities. Here is an example of how to pre-tokenize the 10x Genomics PBMC 10k dataset:</p> <pre><code>from genimtools.utils import write_tokens_to_gtok\nfrom geniml.tokenization import ITTokenizer\n\nadata = sc.read_h5ad(\"path/to/adata.h5ad\")\ntokenizer = ITTokenizer(\"peaks.bed\")\n\ntokens = tokenizer(adata)\n\nfor i, t in enumerate(tokens):\n    file = f\"tokens{i}.gtok\"\n    write_tokens_to_gtok(t, file)\n</code></pre>"},{"location":"geniml/tutorials/train-scembed-model/#training-the-model","title":"Training the model","text":"<p>Now that the data is pre-tokenized, we can train the model. The <code>scEmbed</code> model is designed to be used with <code>scanpy</code>. Here is an example of how to train the model:</p> <pre><code>from geniml.region2vec.utils import Region2VecDataset\n\ndataset = Region2VecDataset(\"path/to/tokens\")\n\nmodel = ScEmbed(\n    tokenizer=tokenizer,\n)\nmodel.train(\n    dataset,\n    epochs=100,\n)\n</code></pre> <p>We can then export the model for upload to huggingface:</p> <pre><code>model.export(\"path/to/model\")\n</code></pre>"},{"location":"geniml/tutorials/train-scembed-model/#get-embeddings-of-single-cells","title":"Get embeddings of single-cells","text":"<p><code>scEmbed</code> is simple to use and designed to be used with <code>scanpy</code>. Here is a simple example of how to train a model and get cell embeddings:</p> <pre><code>model = ScEmbed.from_pretrained(\"path/to/model\")\nmodel = ScEmbed(\"databio/scembed-pbmc10k\")\n\nadata = sc.read_h5ad(\"path/to/adata.h5ad\")\nembeddings = model.encode(adata)\n\nadata.obsm[\"scembed_X\"] = embeddings\n</code></pre>"},{"location":"geniml/tutorials/train-scembed-model/#clustering-the-cells","title":"Clustering the cells","text":"<p>With the model now trained, and cell-embeddings obtained, we can get embeddings of our individual cells. You can use <code>scanpy</code> utilities to cluster the cells:</p> <pre><code>sc.pp.neighbors(adata, use_rep=\"scembed_X\")\nsc.tl.leiden(adata) # or louvain\n</code></pre> <p>And visualize with UMAP</p> <pre><code>sc.tl.umap(adata)\nsc.pl.umap(adata, color=\"leiden\")\n</code></pre>"},{"location":"geniml/tutorials/use-pretrained-region2vec-model/","title":"How to use a pre-trained Region2Vec model","text":"<p>Region2Vec is an unsupervised method for creating embeddings of genomic regions and genomic region sets. This tutorial discusses how to use pre-trained models. To learn how to train a new model see the region2vec training documentation We make available several pre-trained models available on our huggingface repo. These models can be used to create embeddings of genomic regions and region sets without having to train a new model.</p>"},{"location":"geniml/tutorials/use-pretrained-region2vec-model/#get-the-model","title":"Get the model","text":"<p>To use one of our pre-trained models, simply import the <code>Region2VecExModel</code> and download the model from huggingface:</p> <pre><code>from geniml.io import Region\nfrom geniml.region2vec import Region2VecExModel\n\nmodel = Region2VecExModel(\"databio/r2v-encode-hg38\")\n</code></pre> <p>Note: We use the <code>Region2VecExModel</code> class to load the model because it is an extension of the <code>Region2Vec</code> class that comes with its own tokenizer. This ensures that the model and tokenizer are compatible.</p>"},{"location":"geniml/tutorials/use-pretrained-region2vec-model/#encode-regions","title":"Encode regions","text":"<p>Now we can encode a genomic region or a list of regions:</p> <pre><code>region = Region(\"chr1\", 160100, 160200)\nregions = [region] * 10\n\nembedding = model.encode(region) # get one region embedding\nembeddings = model.encode(regions) # or, get many embeddings\n</code></pre> <p>We can also encode an entire bed file, which will return region embeddings for each region in the file:</p> <pre><code>bed = \"/path/to/bed/file.bed\"\n\nembeddings = model.encode(bed)\n</code></pre> <p>Note: It is possible that a region can not be tokenized by the tokenizer. This is because the tokenizer was instantiated with a specific set of regions. If this is the case, the model simply returns the unknown token (<code>chrUNK-0:0</code>). If you find that this is happening often, you may want to ensure that your regions are a good fit for the universe of regions that the model was trained on. The unknown token will indeed have an embedding, but it will not be a meaningful representation of the region.</p>"},{"location":"geniml/tutorials/use-pretrained-region2vec-model/#region-pooling","title":"Region pooling","text":"<p>It is often the case that we want a single embedding that represents a set of regions. For example, we may want to encode a patient by taking the average embedding of all the SNPs in the patient's genome. We can do this by simply averaging across the embeddings of the regions:</p> <pre><code>patient_snps = \"/path/to/bed/file.bed\"\n\nembeddings = model.encode(patient_snps) \npatient_embedding = np.mean(embeddings, axis=0)\n</code></pre>"},{"location":"geniml/tutorials/use-pretrained-scembed-model/","title":"How to use a pre-trained scEmbed model","text":"<p>One advantage of scEmbed is the ability to use pre-trained models. This is useful for quickly getting embeddings of new data without having to train a new model. In this tutorial, we will show how to use a pre-trained model to get embeddings of new data.</p> <p>I will be using the <code>databio/luecken2021</code> model. It was trained on the Luecken2021 dataset, a first-of-its-kind multimodal benchmark dataset of 120,000 single cells from the human bone marrow of 10 diverse donors measured with two commercially-available multi-modal technologies: nuclear GEX with joint ATAC, and cellular GEX with joint ADT profiles.</p> <p>This model will work best on PBMC-like data. It also requires your fragments be aligned to the GRCh38 genome.</p>"},{"location":"geniml/tutorials/use-pretrained-scembed-model/#example-data-preparation","title":"Example data preparation","text":"<p>Grab a fresh set of PBMC data from 10X genomics: https://www.10xgenomics.com/resources/datasets/10k-human-pbmcs-atac-v2-chromium-controller-2-standard</p> <p>You need the Peak by cell matrix (filtered). This  contains the binary accessibility matrix, the peaks, and the barcodes. Pre-trained models also requires that the data be in a <code>scanpy.AnnData</code> format and the <code>.var</code> attribute contain <code>chr</code>, <code>start</code>, and <code>end</code> values. For details on how to make this, see data preparation.</p> <p>Once your data is ready, you can load it into python and get embeddings.</p>"},{"location":"geniml/tutorials/use-pretrained-scembed-model/#encoding-cells","title":"Encoding cells","text":"<p>Encoding cells is as easy as:</p> <pre><code>import scanpy as sc\n\nfrom geniml.scembed import ScEmbed\n\n\nadata = sc.read_h5ad(\"path/to/adata.h5ad\")\nmodel = ScEmbed(\"databio/luecken2021\")\n\nembeddings = model.encode(adata)\nadata.obsm['scembed_X'] = embeddings\n</code></pre> <p>And, thats it! You can now cluster your cells using the <code>scembed_X</code> embeddings.</p>"},{"location":"gtars/","title":"Gtars","text":"<p>Gtars is a Rust package with Python bindings for genomic interval analysis.</p> <p>Coming soon!</p>"},{"location":"manuscripts/gharavi2021/","title":"Embeddings of genomic region sets capture rich biological associations in low dimensions","text":""},{"location":"manuscripts/gharavi2021/#relevant-tutorials","title":"Relevant tutorials","text":"<p>This paper was our first publication showing how to build and evaluate region set embeddings using region-set2vec, based on word2vec.</p> <p>See: train Region2Vec embeddings</p>"},{"location":"manuscripts/gharavi2024/","title":"Joint representation learning for retrieval and annotation of genomic interval sets","text":"<p>Paper: Manuscript at Bioengineering </p>"},{"location":"manuscripts/gharavi2024/#abstract","title":"Abstract","text":"<p>As available genomic interval data increase in scale, we require fast systems to search them. A common approach is simple string matching to compare a search term to metadata, but this is limited by incomplete or inaccurate annotations. An alternative is to compare data directly through genomic region overlap analysis, but this approach leads to challenges like sparsity, high dimensionality, and computational expense. We require novel methods to quickly and flexibly query large, messy genomic interval databases. Here, we develop a genomic interval search system using representation learning. We train numerical embeddings for a collection of region sets simultaneously with their metadata labels, capturing similarity between region sets and their metadata in a low-dimensional space. Using these learned co-embeddings, we develop a system that solves three related information retrieval tasks using embedding distance computations: retrieving region sets related to a user query string, suggesting new labels for database region sets, and retrieving database region sets similar to a query region set. We evaluate these use cases and show that jointly learned representations of region sets and metadata are a promising approach for fast, flexible, and accurate genomic region information retrieval.</p>"},{"location":"manuscripts/gharavi2024/#relevant-tutorials","title":"Relevant tutorials","text":"<p>This paper trained BEDspace models (using StarSpace with BED files). See these tutorials:</p> <ul> <li>How to use BEDSpace to jointly embed regions and metadata</li> </ul>"},{"location":"manuscripts/gu2021/","title":"Bedshift: perturbation of genomic interval sets","text":"<p>Paper: Manuscript at Genome Biology </p>"},{"location":"manuscripts/gu2021/#abstract","title":"Abstract","text":"<p>Functional genomics experiments, like ChIP-Seq or ATAC-Seq, produce results that are summarized as a region set. There is no way to objectively evaluate the effectiveness of region set similarity metrics. We present Bedshift, a tool for perturbing BED files by randomly shifting, adding, and dropping regions from a reference file. The perturbed files can be used to benchmark similarity metrics, as well as for other applications. We highlight differences in behavior between metrics, such as that the Jaccard score is most sensitive to added or dropped regions, while coverage score is most sensitive to shifted regions.</p>"},{"location":"manuscripts/gu2021/#relevant-tutorials","title":"Relevant tutorials","text":"<p>Analysis from the paper is described in these tutorials: </p> <ul> <li>Randomizing BED files with BEDshift</li> </ul>"},{"location":"manuscripts/leroy2024/","title":"Fast clustering and cell-type annotation of scATAC data using pre-trained embeddings","text":"<p>Paper: Manuscript at bioRxiv </p>"},{"location":"manuscripts/leroy2024/#abstract","title":"Abstract","text":"<p>Motivation Data from the single-cell assay for transposase-accessible chromatin using sequencing (scATAC-seq) is now widely available. One major computational challenge is dealing with high dimensionality and inherent sparsity, which is typically addressed by producing lower-dimensional representations of single cells for downstream clustering tasks. Current approaches produce such individual cell embeddings directly through a one-step learning process. Here, we propose an alternative approach by building embedding models pre-trained on reference data. We argue that this provides a more flexible analysis workflow that also has computational performance advantages through transfer learning.</p> <p>Results We implemented our approach in scEmbed, an unsupervised machine learning framework that learns low-dimensional embeddings of genomic regulatory regions to represent and analyze scATAC-seq data. scEmbed performs well in terms of clustering ability and has the key advantage of learning patterns of region co-occurrence that can be transferred to other, unseen datasets. Moreover, pre-trained models on reference data can be exploited to build fast and accurate cell-type annotation systems without the need for other data modalities. scEmbed is implemented in Python and it is available to download from GitHub. We also make our pre-trained models available on huggingface for public use.</p>"},{"location":"manuscripts/leroy2024/#relevant-tutorials","title":"Relevant tutorials","text":"<p>Analysis from the paper is described in these tutorials: </p> <ul> <li>Train single-cell embeddings</li> <li>Populate a vector store</li> <li>Predict cell-types using KNN</li> </ul>"},{"location":"manuscripts/rymuza2024/","title":"Methods for constructing and evaluating consensus genomic interval sets","text":"<p>Paper: Manuscript at bioRxiv </p>"},{"location":"manuscripts/rymuza2024/#abstract","title":"Abstract","text":"<p>The amount of genomic region data continues to increase. Integrating across diverse genomic region sets requires consensus regions, which enable comparing regions across experiments, but also by necessity lose precision in region definitions. We require methods to assess this loss of precision and build optimal consensus region sets.</p> <p>Here, we introduce the concept of flexible intervals and propose 3 novel methods for building consensus region sets, or universes: a coverage cutoff method, a likelihood method, and a Hidden Markov Model. We then propose 3 novel measures for evaluating how well a proposed universe fits a collection of region sets: a base-level overlap score, a region boundary distance score, and a likelihood score. We apply our methods and evaluation approaches to several collections of region sets and show how these methods can be used to evaluate fit of universes and build optimal universes. We describe scenarios where the common approach of merging regions to create consensus leads to undesirable outcomes and provide principled alternatives that provide interoperability of interval data while minimizing loss of resolution.</p>"},{"location":"manuscripts/rymuza2024/#relevant-tutorials","title":"Relevant tutorials","text":"<p>This paper published 2 types of method: 1. Methods to construct a universe, and 2. Methods to evaluate a universe.</p>"},{"location":"manuscripts/rymuza2024/#1-constructing-a-universe","title":"1. Constructing a universe","text":"<p>You can construct a universe either on the command line, or using geniml as a library:</p> <ul> <li>Create consensus peaks with CLI</li> <li>Create consensus peaks with Python</li> </ul>"},{"location":"manuscripts/rymuza2024/#2-evaluating-a-universe","title":"2. Evaluating a universe","text":"<p>The main methods are implemented in the <code>assess-universe</code> model with tutorial:</p> <ul> <li>Assess universe fit tutorial</li> </ul>"},{"location":"manuscripts/zheng2024/","title":"Methods for evaluating unsupervised vector representations of genomic regions","text":"<p>Paper: Manuscript at bioRxiv </p>"},{"location":"manuscripts/zheng2024/#abstract","title":"Abstract","text":"<p>Representation learning models have become a mainstay of modern genomics. These models are trained to yield vector representations, or embeddings, of various biological entities, such as cells, genes, individuals, or genomic regions. Recent applications of unsupervised embedding approaches have been shown to learn relationships among genomic regions that define functional elements in a genome. Unsupervised representation learning of genomic regions is free of the supervision from curated metadata and can condense rich biological knowledge from publicly available data to region embeddings. However, there exists no method for evaluating the quality of these embeddings in the absence of metadata, making it difficult to assess the reliability of analyses based on the embeddings, and to tune model training to yield optimal results. To bridge this gap, we propose four evaluation metrics: the cluster tendency score (CTS), the reconstruction score (RCS), the genome distance scaling score (GDSS), and the neighborhood preserving score (NPS). The CTS and RCS statistically quantify how well region embeddings can be clustered and how well the embeddings preserve information in training data. The GDSS and NPS exploit the biological tendency of regions close in genomic space to have similar biological functions; they measure how much such information is captured by individual region embeddings in a set. We demonstrate the utility of these statistical and biological scores for evaluating unsupervised genomic region embeddings and provide guidelines for learning reliable embeddings.</p>"},{"location":"manuscripts/zheng2024/#relevant-tutorials","title":"Relevant tutorials","text":"<p>Analysis from the paper is described in these tutorials: </p> <ul> <li>How to evalute embeddings</li> </ul>"}]}